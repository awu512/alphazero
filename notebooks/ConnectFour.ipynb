{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7620efee-0dba-4939-ad3d-880af53233b5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-24T03:24:22.152294Z",
     "start_time": "2023-11-24T03:24:20.805377Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.24.4\n",
      "2.1.1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f99c0b84290>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "import random\n",
    "from tqdm.notebook import trange\n",
    "\n",
    "import numpy as np\n",
    "print(np.__version__)\n",
    "\n",
    "import torch\n",
    "print(torch.__version__)\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2d894942-1528-4af0-8993-7d0d1e30b243",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConnectFour:\n",
    "    ''' Game definition for ConnectFour '''\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.row_count = 6\n",
    "        self.col_count = 7\n",
    "        self.action_size = self.col_count\n",
    "        self.win_condition = 4\n",
    "\n",
    "    def __repr__(self):\n",
    "        return 'ConnectFour'\n",
    "\n",
    "    def get_initial_state(self):\n",
    "        ''' Get board with all zeros '''\n",
    "        return np.zeros((self.row_count, self.col_count))\n",
    "\n",
    "    def get_next_state(self, state, action, player):\n",
    "        ''' Get the next state given the given action by the given player '''\n",
    "        row = np.max(np.where(state[:, action] == 0))\n",
    "        state[row, action] = player\n",
    "        return state\n",
    "\n",
    "    def get_valid_moves(self, state):\n",
    "        ''' Get all the legal moves in the position '''\n",
    "        return (state[0] == 0).astype(np.uint8)\n",
    "\n",
    "    def check_win(self, state, action):\n",
    "        ''' Check if the given action has led to a win '''\n",
    "        if action == None:\n",
    "            return False\n",
    "        \n",
    "        row = np.min(np.where(state[:, action] != 0))\n",
    "        column = action\n",
    "        player = state[row][column]\n",
    "\n",
    "        def count(offset_row, offset_column):\n",
    "            for i in range(1, self.win_condition):\n",
    "                r = row + offset_row * i\n",
    "                c = action + offset_column * i\n",
    "                if (\n",
    "                    r < 0 \n",
    "                    or r >= self.row_count\n",
    "                    or c < 0 \n",
    "                    or c >= self.col_count\n",
    "                    or state[r][c] != player\n",
    "                ):\n",
    "                    return i - 1\n",
    "            return self.win_condition - 1\n",
    "\n",
    "        return (\n",
    "            count(1, 0) >= self.win_condition - 1 # vertical\n",
    "            or (count(0, 1) + count(0, -1)) >= self.win_condition - 1 # horizontal\n",
    "            or (count(1, 1) + count(-1, -1)) >= self.win_condition - 1 # top left diagonal\n",
    "            or (count(1, -1) + count(-1, 1)) >= self.win_condition - 1 # top right diagonal\n",
    "        )\n",
    "\n",
    "    def get_value_and_terminated(self, state, action):\n",
    "        ''' Get the value (win/tie) and if the game has terminated '''\n",
    "        if self.check_win(state, action):\n",
    "            return 1, True\n",
    "        if np.sum(self.get_valid_moves(state)) == 0:\n",
    "            return 0, True\n",
    "        return 0, False\n",
    "\n",
    "    def get_opponent(self, player):\n",
    "        ''' Get the opponent of the player '''\n",
    "        return -player\n",
    "\n",
    "    def get_opponent_value(self, value):\n",
    "        ''' Get the value from the opponent's perspective '''\n",
    "        return -value\n",
    "\n",
    "    def change_perspective(self, state, player):\n",
    "        ''' Transform the state to be from the opponent's perspective '''\n",
    "        return state * player\n",
    "\n",
    "    def get_encoded_state(self, state):\n",
    "        ''' Get the network-ready encoded state of the game '''\n",
    "        encoded_state = np.stack((state == -1, state == 0, state == 1)).astype(np.float32)\n",
    "\n",
    "        # check for batched states\n",
    "        if len(state.shape) == 3:\n",
    "            encoded_state = np.swapaxes(encoded_state, 0, 1)\n",
    "        \n",
    "        return encoded_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3bd0cd66-2da5-4e7e-97eb-becc878292d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    "    ''' The residual neural network implementation for AlphaZero '''\n",
    "    \n",
    "    def __init__(self, game, num_resblocks, num_hidden, device):\n",
    "        super().__init__()\n",
    "\n",
    "        # store the specified device for computation\n",
    "        self.device = device\n",
    "\n",
    "        # initial NN block\n",
    "        self.start_block = nn.Sequential(\n",
    "            nn.Conv2d(3, num_hidden, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(num_hidden),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        # shared portion of network between policy and value heads\n",
    "        self.backbone = nn.ModuleList(\n",
    "            [ResBlock(num_hidden) for _ in range(num_resblocks)]\n",
    "        )\n",
    "\n",
    "        # the portion of the network responsible for outputting policies\n",
    "        self.policy_head = nn.Sequential(\n",
    "            nn.Conv2d(num_hidden, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(32 * game.row_count * game.col_count, game.action_size)\n",
    "        )\n",
    "\n",
    "        # the portion of the network responsible for outputting values\n",
    "        self.value_head = nn.Sequential(\n",
    "            nn.Conv2d(num_hidden, 3, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(3),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(3 * game.row_count * game.col_count, 1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "        # send computation to the device\n",
    "        self.to(device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        ''' Feed forward implementation for this network '''\n",
    "        x = self.start_block(x)\n",
    "        for resblock in self.backbone:\n",
    "            x = resblock(x)\n",
    "        policy = self.policy_head(x)\n",
    "        value = self.value_head(x)\n",
    "\n",
    "        return policy, value\n",
    "\n",
    "class ResBlock(nn.Module):\n",
    "    ''' ResNet block '''\n",
    "    \n",
    "    def __init__(self, num_hidden):\n",
    "        ''' Initialize the block '''\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(num_hidden, num_hidden, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(num_hidden)\n",
    "        self.conv2 = nn.Conv2d(num_hidden, num_hidden, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(num_hidden)\n",
    "\n",
    "    def forward(self, x):\n",
    "        ''' Feed forward implementation for this block '''\n",
    "        residual = x\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.bn2(self.conv2(x))\n",
    "        x += residual\n",
    "        x = F.relu(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e888f91a-00c3-46bf-a9f5-d0685368479e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  1.]\n",
      " [ 0.  0. -1.  0. -1.  0.  1.]]\n",
      "[[[0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 1. 0. 1. 0. 0.]]\n",
      "\n",
      " [[1. 1. 1. 1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1. 1. 1. 0.]\n",
      "  [1. 1. 0. 1. 0. 1. 0.]]\n",
      "\n",
      " [[0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 1.]\n",
      "  [0. 0. 0. 0. 0. 0. 1.]]]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for ResNet:\n\tMissing key(s) in state_dict: \"backbone.4.conv1.weight\", \"backbone.4.conv1.bias\", \"backbone.4.bn1.weight\", \"backbone.4.bn1.bias\", \"backbone.4.bn1.running_mean\", \"backbone.4.bn1.running_var\", \"backbone.4.conv2.weight\", \"backbone.4.conv2.bias\", \"backbone.4.bn2.weight\", \"backbone.4.bn2.bias\", \"backbone.4.bn2.running_mean\", \"backbone.4.bn2.running_var\", \"backbone.5.conv1.weight\", \"backbone.5.conv1.bias\", \"backbone.5.bn1.weight\", \"backbone.5.bn1.bias\", \"backbone.5.bn1.running_mean\", \"backbone.5.bn1.running_var\", \"backbone.5.conv2.weight\", \"backbone.5.conv2.bias\", \"backbone.5.bn2.weight\", \"backbone.5.bn2.bias\", \"backbone.5.bn2.running_mean\", \"backbone.5.bn2.running_var\", \"backbone.6.conv1.weight\", \"backbone.6.conv1.bias\", \"backbone.6.bn1.weight\", \"backbone.6.bn1.bias\", \"backbone.6.bn1.running_mean\", \"backbone.6.bn1.running_var\", \"backbone.6.conv2.weight\", \"backbone.6.conv2.bias\", \"backbone.6.bn2.weight\", \"backbone.6.bn2.bias\", \"backbone.6.bn2.running_mean\", \"backbone.6.bn2.running_var\", \"backbone.7.conv1.weight\", \"backbone.7.conv1.bias\", \"backbone.7.bn1.weight\", \"backbone.7.bn1.bias\", \"backbone.7.bn1.running_mean\", \"backbone.7.bn1.running_var\", \"backbone.7.conv2.weight\", \"backbone.7.conv2.bias\", \"backbone.7.bn2.weight\", \"backbone.7.bn2.bias\", \"backbone.7.bn2.running_mean\", \"backbone.7.bn2.running_var\", \"backbone.8.conv1.weight\", \"backbone.8.conv1.bias\", \"backbone.8.bn1.weight\", \"backbone.8.bn1.bias\", \"backbone.8.bn1.running_mean\", \"backbone.8.bn1.running_var\", \"backbone.8.conv2.weight\", \"backbone.8.conv2.bias\", \"backbone.8.bn2.weight\", \"backbone.8.bn2.bias\", \"backbone.8.bn2.running_mean\", \"backbone.8.bn2.running_var\". \n\tsize mismatch for start_block.0.weight: copying a param with shape torch.Size([64, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([128, 3, 3, 3]).\n\tsize mismatch for start_block.0.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for start_block.1.weight: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for start_block.1.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for start_block.1.running_mean: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for start_block.1.running_var: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for backbone.0.conv1.weight: copying a param with shape torch.Size([64, 64, 3, 3]) from checkpoint, the shape in current model is torch.Size([128, 128, 3, 3]).\n\tsize mismatch for backbone.0.conv1.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for backbone.0.bn1.weight: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for backbone.0.bn1.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for backbone.0.bn1.running_mean: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for backbone.0.bn1.running_var: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for backbone.0.conv2.weight: copying a param with shape torch.Size([64, 64, 3, 3]) from checkpoint, the shape in current model is torch.Size([128, 128, 3, 3]).\n\tsize mismatch for backbone.0.conv2.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for backbone.0.bn2.weight: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for backbone.0.bn2.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for backbone.0.bn2.running_mean: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for backbone.0.bn2.running_var: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for backbone.1.conv1.weight: copying a param with shape torch.Size([64, 64, 3, 3]) from checkpoint, the shape in current model is torch.Size([128, 128, 3, 3]).\n\tsize mismatch for backbone.1.conv1.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for backbone.1.bn1.weight: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for backbone.1.bn1.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for backbone.1.bn1.running_mean: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for backbone.1.bn1.running_var: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for backbone.1.conv2.weight: copying a param with shape torch.Size([64, 64, 3, 3]) from checkpoint, the shape in current model is torch.Size([128, 128, 3, 3]).\n\tsize mismatch for backbone.1.conv2.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for backbone.1.bn2.weight: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for backbone.1.bn2.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for backbone.1.bn2.running_mean: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for backbone.1.bn2.running_var: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for backbone.2.conv1.weight: copying a param with shape torch.Size([64, 64, 3, 3]) from checkpoint, the shape in current model is torch.Size([128, 128, 3, 3]).\n\tsize mismatch for backbone.2.conv1.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for backbone.2.bn1.weight: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for backbone.2.bn1.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for backbone.2.bn1.running_mean: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for backbone.2.bn1.running_var: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for backbone.2.conv2.weight: copying a param with shape torch.Size([64, 64, 3, 3]) from checkpoint, the shape in current model is torch.Size([128, 128, 3, 3]).\n\tsize mismatch for backbone.2.conv2.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for backbone.2.bn2.weight: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for backbone.2.bn2.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for backbone.2.bn2.running_mean: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for backbone.2.bn2.running_var: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for backbone.3.conv1.weight: copying a param with shape torch.Size([64, 64, 3, 3]) from checkpoint, the shape in current model is torch.Size([128, 128, 3, 3]).\n\tsize mismatch for backbone.3.conv1.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for backbone.3.bn1.weight: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for backbone.3.bn1.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for backbone.3.bn1.running_mean: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for backbone.3.bn1.running_var: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for backbone.3.conv2.weight: copying a param with shape torch.Size([64, 64, 3, 3]) from checkpoint, the shape in current model is torch.Size([128, 128, 3, 3]).\n\tsize mismatch for backbone.3.conv2.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for backbone.3.bn2.weight: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for backbone.3.bn2.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for backbone.3.bn2.running_mean: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for backbone.3.bn2.running_var: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for policy_head.0.weight: copying a param with shape torch.Size([32, 64, 3, 3]) from checkpoint, the shape in current model is torch.Size([32, 128, 3, 3]).\n\tsize mismatch for value_head.0.weight: copying a param with shape torch.Size([3, 64, 3, 3]) from checkpoint, the shape in current model is torch.Size([3, 128, 3, 3]).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[48], line 22\u001b[0m\n\u001b[1;32m     19\u001b[0m tensor_state \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(encoded_state, device\u001b[38;5;241m=\u001b[39mdevice)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     21\u001b[0m model \u001b[38;5;241m=\u001b[39m ResNet(game, \u001b[38;5;241m9\u001b[39m, \u001b[38;5;241m128\u001b[39m, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[0;32m---> 22\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmodels/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mgame\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/model_0.pt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m     25\u001b[0m policy, value \u001b[38;5;241m=\u001b[39m model(tensor_state)\n",
      "File \u001b[0;32m~/Desktop/Pomona/CS158/alphazero/venv/lib/python3.8/site-packages/torch/nn/modules/module.py:2152\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[1;32m   2147\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[1;32m   2148\u001b[0m             \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2149\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)))\n\u001b[1;32m   2151\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 2152\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2153\u001b[0m                        \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)))\n\u001b[1;32m   2154\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for ResNet:\n\tMissing key(s) in state_dict: \"backbone.4.conv1.weight\", \"backbone.4.conv1.bias\", \"backbone.4.bn1.weight\", \"backbone.4.bn1.bias\", \"backbone.4.bn1.running_mean\", \"backbone.4.bn1.running_var\", \"backbone.4.conv2.weight\", \"backbone.4.conv2.bias\", \"backbone.4.bn2.weight\", \"backbone.4.bn2.bias\", \"backbone.4.bn2.running_mean\", \"backbone.4.bn2.running_var\", \"backbone.5.conv1.weight\", \"backbone.5.conv1.bias\", \"backbone.5.bn1.weight\", \"backbone.5.bn1.bias\", \"backbone.5.bn1.running_mean\", \"backbone.5.bn1.running_var\", \"backbone.5.conv2.weight\", \"backbone.5.conv2.bias\", \"backbone.5.bn2.weight\", \"backbone.5.bn2.bias\", \"backbone.5.bn2.running_mean\", \"backbone.5.bn2.running_var\", \"backbone.6.conv1.weight\", \"backbone.6.conv1.bias\", \"backbone.6.bn1.weight\", \"backbone.6.bn1.bias\", \"backbone.6.bn1.running_mean\", \"backbone.6.bn1.running_var\", \"backbone.6.conv2.weight\", \"backbone.6.conv2.bias\", \"backbone.6.bn2.weight\", \"backbone.6.bn2.bias\", \"backbone.6.bn2.running_mean\", \"backbone.6.bn2.running_var\", \"backbone.7.conv1.weight\", \"backbone.7.conv1.bias\", \"backbone.7.bn1.weight\", \"backbone.7.bn1.bias\", \"backbone.7.bn1.running_mean\", \"backbone.7.bn1.running_var\", \"backbone.7.conv2.weight\", \"backbone.7.conv2.bias\", \"backbone.7.bn2.weight\", \"backbone.7.bn2.bias\", \"backbone.7.bn2.running_mean\", \"backbone.7.bn2.running_var\", \"backbone.8.conv1.weight\", \"backbone.8.conv1.bias\", \"backbone.8.bn1.weight\", \"backbone.8.bn1.bias\", \"backbone.8.bn1.running_mean\", \"backbone.8.bn1.running_var\", \"backbone.8.conv2.weight\", \"backbone.8.conv2.bias\", \"backbone.8.bn2.weight\", \"backbone.8.bn2.bias\", \"backbone.8.bn2.running_mean\", \"backbone.8.bn2.running_var\". \n\tsize mismatch for start_block.0.weight: copying a param with shape torch.Size([64, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([128, 3, 3, 3]).\n\tsize mismatch for start_block.0.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for start_block.1.weight: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for start_block.1.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for start_block.1.running_mean: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for start_block.1.running_var: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for backbone.0.conv1.weight: copying a param with shape torch.Size([64, 64, 3, 3]) from checkpoint, the shape in current model is torch.Size([128, 128, 3, 3]).\n\tsize mismatch for backbone.0.conv1.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for backbone.0.bn1.weight: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for backbone.0.bn1.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for backbone.0.bn1.running_mean: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for backbone.0.bn1.running_var: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for backbone.0.conv2.weight: copying a param with shape torch.Size([64, 64, 3, 3]) from checkpoint, the shape in current model is torch.Size([128, 128, 3, 3]).\n\tsize mismatch for backbone.0.conv2.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for backbone.0.bn2.weight: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for backbone.0.bn2.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for backbone.0.bn2.running_mean: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for backbone.0.bn2.running_var: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for backbone.1.conv1.weight: copying a param with shape torch.Size([64, 64, 3, 3]) from checkpoint, the shape in current model is torch.Size([128, 128, 3, 3]).\n\tsize mismatch for backbone.1.conv1.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for backbone.1.bn1.weight: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for backbone.1.bn1.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for backbone.1.bn1.running_mean: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for backbone.1.bn1.running_var: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for backbone.1.conv2.weight: copying a param with shape torch.Size([64, 64, 3, 3]) from checkpoint, the shape in current model is torch.Size([128, 128, 3, 3]).\n\tsize mismatch for backbone.1.conv2.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for backbone.1.bn2.weight: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for backbone.1.bn2.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for backbone.1.bn2.running_mean: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for backbone.1.bn2.running_var: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for backbone.2.conv1.weight: copying a param with shape torch.Size([64, 64, 3, 3]) from checkpoint, the shape in current model is torch.Size([128, 128, 3, 3]).\n\tsize mismatch for backbone.2.conv1.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for backbone.2.bn1.weight: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for backbone.2.bn1.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for backbone.2.bn1.running_mean: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for backbone.2.bn1.running_var: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for backbone.2.conv2.weight: copying a param with shape torch.Size([64, 64, 3, 3]) from checkpoint, the shape in current model is torch.Size([128, 128, 3, 3]).\n\tsize mismatch for backbone.2.conv2.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for backbone.2.bn2.weight: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for backbone.2.bn2.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for backbone.2.bn2.running_mean: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for backbone.2.bn2.running_var: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for backbone.3.conv1.weight: copying a param with shape torch.Size([64, 64, 3, 3]) from checkpoint, the shape in current model is torch.Size([128, 128, 3, 3]).\n\tsize mismatch for backbone.3.conv1.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for backbone.3.bn1.weight: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for backbone.3.bn1.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for backbone.3.bn1.running_mean: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for backbone.3.bn1.running_var: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for backbone.3.conv2.weight: copying a param with shape torch.Size([64, 64, 3, 3]) from checkpoint, the shape in current model is torch.Size([128, 128, 3, 3]).\n\tsize mismatch for backbone.3.conv2.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for backbone.3.bn2.weight: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for backbone.3.bn2.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for backbone.3.bn2.running_mean: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for backbone.3.bn2.running_var: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for policy_head.0.weight: copying a param with shape torch.Size([32, 64, 3, 3]) from checkpoint, the shape in current model is torch.Size([32, 128, 3, 3]).\n\tsize mismatch for value_head.0.weight: copying a param with shape torch.Size([3, 64, 3, 3]) from checkpoint, the shape in current model is torch.Size([3, 128, 3, 3])."
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "game = ConnectFour()\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "state = game.get_initial_state()\n",
    "state = game.get_next_state(state, 2, -1)\n",
    "state = game.get_next_state(state, 4, -1)\n",
    "state = game.get_next_state(state, 6, 1)\n",
    "state = game.get_next_state(state, 6, 1)\n",
    "\n",
    "print(state)\n",
    "\n",
    "encoded_state = game.get_encoded_state(state)\n",
    "\n",
    "print(encoded_state)\n",
    "\n",
    "tensor_state = torch.tensor(encoded_state, device=device).unsqueeze(0)\n",
    "\n",
    "model = ResNet(game, 9, 128, device=device)\n",
    "model.load_state_dict(torch.load(f'models/{game}/model_0.pt', map_location=device))\n",
    "model.eval()\n",
    "\n",
    "policy, value = model(tensor_state)\n",
    "value = value.item()\n",
    "policy = torch.softmax(policy, axis=1).squeeze(0).detach().cpu().numpy()\n",
    "\n",
    "print(value)\n",
    "\n",
    "plt.bar(range(game.action_size), policy)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "fc005069-4b4f-41cb-97fb-65bad1af58cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    ''' A Monte Carlo Tree Search node '''\n",
    "    \n",
    "    def __init__(self, game, args, state, parent=None, action_taken=None, prior=0, visit_count=0):\n",
    "        self.game = game\n",
    "        self.args = args\n",
    "        self.state = state\n",
    "        self.parent = parent\n",
    "        self.action_taken = action_taken\n",
    "        self.prior = prior\n",
    "\n",
    "        self.children = []\n",
    "\n",
    "        self.visit_count = visit_count\n",
    "        self.value_sum = 0\n",
    "\n",
    "    def is_expanded(self):\n",
    "        ''' Check if the node already has children. Since we expand all children of a node at once, we check for >0 children '''\n",
    "        return len(self.children) > 0\n",
    "\n",
    "    def select(self):\n",
    "        ''' Select a child to explore '''\n",
    "        best_child = None\n",
    "        best_ucb = -np.inf\n",
    "\n",
    "        for child in self.children:\n",
    "            ucb = self.get_ucb(child)\n",
    "            if ucb > best_ucb:\n",
    "                best_child = child\n",
    "                best_ucb = ucb\n",
    "\n",
    "        return best_child\n",
    "\n",
    "    def get_ucb(self, child):\n",
    "        ''' Get how promising a move is from the opponent's perspective, normalized on [0,1] '''\n",
    "        if child.visit_count == 0:\n",
    "            q = 0\n",
    "        else:\n",
    "            q = 1 - ((child.value_sum / child.visit_count) + 1) / 2 \n",
    "\n",
    "        return q + self.args['C'] * (math.sqrt(self.visit_count) / (child.visit_count + 1)) * child.prior\n",
    "\n",
    "    def expand(self, policy):\n",
    "        ''' Expand a node by adding all legal child moves '''\n",
    "        for action, prob in enumerate(policy):\n",
    "            if prob > 0:\n",
    "                child_state = self.state.copy()\n",
    "                child_state = self.game.get_next_state(child_state, action, 1)\n",
    "                child_state = self.game.change_perspective(child_state, player=-1)\n",
    "        \n",
    "                child = Node(self.game, self.args, child_state, self, action, prob)\n",
    "                self.children.append(child)\n",
    "\n",
    "    def backpropagate(self, value):\n",
    "        ''' Propagate value sums and visit counts from children to all parents '''\n",
    "        self.value_sum += value\n",
    "        self.visit_count += 1\n",
    "\n",
    "        # flip value for opponent (parent)\n",
    "        value = self.game.get_opponent_value(value)\n",
    "\n",
    "        if self.parent is not None:\n",
    "            self.parent.backpropagate(value)\n",
    "\n",
    "class MCTS:\n",
    "    ''' A Monte Carlo Tree Search '''\n",
    "    \n",
    "    def __init__(self, game, args, model):\n",
    "        self.game = game\n",
    "        self.args = args\n",
    "        self.model = model\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def search(self, state):\n",
    "        ''' Expand and explore the MCTS and update value sums and visit counts '''\n",
    "        # DEFINE ROOT\n",
    "        root = Node(self.game, self.args, state, visit_count=1)\n",
    "\n",
    "        # add some random noise to policy to increase exploration\n",
    "        policy, _ = self.model(\n",
    "            torch.tensor(self.game.get_encoded_state(state), device=self.model.device).unsqueeze(0)\n",
    "        )\n",
    "        policy = torch.softmax(policy, axis=1).squeeze(0).cpu().numpy()\n",
    "        policy = ((1 - self.args['dirichlet_epsilon']) * \n",
    "                    policy + \n",
    "                    self.args['dirichlet_epsilon'] * \n",
    "                    np.random.dirichlet([self.args['dirichlet_alpha']] * \n",
    "                    self.game.action_size\n",
    "                ))\n",
    "        valid_moves = self.game.get_valid_moves(state)\n",
    "        policy *= valid_moves\n",
    "        policy /= np.sum(policy)\n",
    "        root.expand(policy)\n",
    "        \n",
    "        for search in range(self.args['num_searches']):\n",
    "            node = root\n",
    "            \n",
    "            # SELECTION\n",
    "            while node.is_expanded():\n",
    "                node = node.select()\n",
    "                \n",
    "            # check for end of game\n",
    "            value, is_terminal = self.game.get_value_and_terminated(node.state, node.action_taken)\n",
    "\n",
    "            # flip parent value\n",
    "            value = self.game.get_opponent_value(value)\n",
    "\n",
    "            if not is_terminal:\n",
    "                # get output from model\n",
    "                policy, value = self.model(\n",
    "                    torch.tensor(self.game.get_encoded_state(node.state), device=self.model.device).unsqueeze(0)\n",
    "                )\n",
    "\n",
    "                # change policy to proabability distribution\n",
    "                policy = torch.softmax(policy, axis=1).squeeze(0).cpu().numpy()\n",
    "\n",
    "                # mask out illegal moves\n",
    "                valid_moves = self.game.get_valid_moves(node.state)\n",
    "                policy *= valid_moves\n",
    "\n",
    "                # readjust back to probability distribution\n",
    "                policy /= np.sum(policy)\n",
    "\n",
    "                # get the value as a number from singleton tensor\n",
    "                value = value.item()\n",
    "                \n",
    "                # EXPANSION\n",
    "                node.expand(policy)\n",
    "    \n",
    "            # BACKPROP\n",
    "            node.backpropagate(value)\n",
    "\n",
    "        # probabilities of action being good\n",
    "        action_probs = np.zeros(self.game.action_size)\n",
    "        for child in root.children:\n",
    "            action_probs[child.action_taken] = child.visit_count\n",
    "\n",
    "        action_probs /= np.sum(action_probs)\n",
    "        return action_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "919a7ac9-28a6-4c49-b069-004b79290181",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlphaZero:\n",
    "    ''' AlphaZero class for self-play and training '''\n",
    "    \n",
    "    def __init__(self, model, optimizer, game, args):\n",
    "        ''' Initialize the AlphaZero instance '''\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.game = game\n",
    "        self.args = args\n",
    "        self.mcts = MCTS(game, args, model)\n",
    "\n",
    "    def self_play(self):\n",
    "        ''' Run a single self-play game until completion and generate outcome-appended training data '''\n",
    "        \n",
    "        memory = []\n",
    "        player = 1\n",
    "        state = self.game.get_initial_state()\n",
    "\n",
    "        while True:\n",
    "            # get the current state and action probabilities from MCTS\n",
    "            neutral_state = self.game.change_perspective(state, player)\n",
    "            action_probs = self.mcts.search(neutral_state)\n",
    "\n",
    "            # record a game snapshot\n",
    "            memory.append((neutral_state, action_probs, player))\n",
    "\n",
    "            # randomly sample an action from the distribution\n",
    "            adjusted_action_probs = action_probs ** (1 / self.args['temperature']) # add flexibility for exploration / exploitation\n",
    "            adjusted_action_probs /= np.sum(adjusted_action_probs)\n",
    "            action = np.random.choice(self.game.action_size, p=adjusted_action_probs)\n",
    "\n",
    "            # get the next state given the chosen action\n",
    "            state = self.game.get_next_state(state, action, player)\n",
    "\n",
    "            # check for game completion\n",
    "            value, is_terminal = self.game.get_value_and_terminated(state, action)\n",
    "\n",
    "            if is_terminal:\n",
    "                # get all states and policies from the game and append the outcome\n",
    "                return [(\n",
    "                    self.game.get_encoded_state(h_state),\n",
    "                    h_action_probs,\n",
    "                    value if h_player == player else self.game.get_opponent_value(value)\n",
    "                ) for h_state, h_action_probs, h_player in memory]\n",
    "\n",
    "            # swap the player and loop\n",
    "            player = self.game.get_opponent(player)\n",
    "\n",
    "    def train(self, memory):\n",
    "        ''' Train the model '''\n",
    "\n",
    "        # randomize training data\n",
    "        random.shuffle(memory)\n",
    "        \n",
    "        for batch_i in range(0, len(memory), self.args['batch_size']):\n",
    "            # sample a batch from training data\n",
    "            sample = memory[batch_i : min(len(memory) - 1, batch_i + self.args['batch_size'])]\n",
    "\n",
    "            # transpose list of tuples to independent lists\n",
    "            state, policy_targets, value_targets = zip(*sample)\n",
    "\n",
    "            # convert to numpy arrays\n",
    "            state = np.array(state)\n",
    "            policy_targets = np.array(policy_targets)\n",
    "            value_targets = np.array(value_targets).reshape(-1, 1) # wrap each value in its own array for simplicity later\n",
    "\n",
    "            # convert to tensors\n",
    "            state = torch.tensor(state, dtype=torch.float32, device=self.model.device)\n",
    "            policy_targets = torch.tensor(policy_targets, dtype=torch.float32, device=self.model.device)\n",
    "            value_targets = torch.tensor(value_targets, dtype=torch.float32, device=self.model.device)\n",
    "\n",
    "            # get model outputs\n",
    "            out_policy, out_value = self.model(state)\n",
    "\n",
    "            # get loss\n",
    "            policy_loss = F.cross_entropy(out_policy, policy_targets)\n",
    "            value_loss = F.mse_loss(out_value, value_targets)\n",
    "            loss = policy_loss + value_loss\n",
    "\n",
    "            # minimize loss via backpropagation\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    def learn(self):\n",
    "        ''' Generate self-play training data and train the model on it '''\n",
    "        \n",
    "        for iter in range(self.args['num_iters']):\n",
    "            memory = []\n",
    "            \n",
    "            self.model.eval()\n",
    "            for self_play_iter in trange(self.args['num_self_play_iters']):\n",
    "                memory += self.self_play()\n",
    "\n",
    "            self.model.train()\n",
    "            for epoch in trange(self.args['num_epochs']):\n",
    "                self.train(memory)\n",
    "\n",
    "            torch.save(self.model.state_dict(), f'models/{self.game}/model_{iter}.pt')\n",
    "            torch.save(self.optimizer.state_dict(), f'models/{self.game}/optimizer_{iter}.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b77aafbd-5988-42c8-a8c0-fae0fbee464c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MCTSParallel:\n",
    "    ''' A parallelized Monte Carlo Tree Search '''\n",
    "    \n",
    "    def __init__(self, game, args, model):\n",
    "        self.game = game\n",
    "        self.args = args\n",
    "        self.model = model\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def search(self, states, sp_games):\n",
    "        ''' Expand and explore the MCTS and update value sums and visit counts '''\n",
    "\n",
    "        # add some random noise to policy to increase exploration\n",
    "        policy, _ = self.model(torch.tensor(self.game.get_encoded_state(states), device=self.model.device))\n",
    "        policy = torch.softmax(policy, axis=1).cpu().numpy()\n",
    "        policy = ((1 - self.args['dirichlet_epsilon']) * \n",
    "                    policy + \n",
    "                    self.args['dirichlet_epsilon'] * \n",
    "                    np.random.dirichlet([self.args['dirichlet_alpha']] * \n",
    "                    self.game.action_size,\n",
    "                    size=policy.shape[0]\n",
    "                ))\n",
    "\n",
    "        for i, spg in enumerate(sp_games):\n",
    "            # get policy for this self-play game\n",
    "            spg_policy = policy[i]\n",
    "\n",
    "            # mask illegal moves out of policy\n",
    "            valid_moves = self.game.get_valid_moves(states[i])\n",
    "            spg_policy *= valid_moves\n",
    "            spg_policy /= np.sum(spg_policy)\n",
    "    \n",
    "            spg.root = Node(self.game, self.args, states[i], visit_count=1)\n",
    "            \n",
    "            spg.root.expand(spg_policy)\n",
    "        \n",
    "        for search in range(self.args['num_searches']):\n",
    "            for spg in sp_games:\n",
    "                spg.node = None\n",
    "                node = spg.root\n",
    "                \n",
    "                # SELECTION\n",
    "                while node.is_expanded():\n",
    "                    node = node.select()\n",
    "                    \n",
    "                # check for end of game\n",
    "                value, is_terminal = self.game.get_value_and_terminated(node.state, node.action_taken)\n",
    "    \n",
    "                # flip parent value\n",
    "                value = self.game.get_opponent_value(value)\n",
    "\n",
    "                # backpropagate value sums and visit counts if game ended\n",
    "                if is_terminal:\n",
    "                    node.backpropagate(value)\n",
    "\n",
    "                # otherwise, store the current node\n",
    "                else:\n",
    "                    spg.node = node\n",
    "\n",
    "            expandable_sp_games = [map_i for map_i in range(len(sp_games)) if sp_games[map_i].node is not None]\n",
    "\n",
    "            if len(expandable_sp_games) > 0:\n",
    "                states = np.stack([sp_games[map_i].node.state for map_i in expandable_sp_games])\n",
    "\n",
    "                # get output from model\n",
    "                policy, value = self.model(\n",
    "                    torch.tensor(self.game.get_encoded_state(states), device=self.model.device)\n",
    "                )\n",
    "\n",
    "                # change policy to proabability distribution\n",
    "                policy = torch.softmax(policy, axis=1).cpu().numpy()\n",
    "\n",
    "                # get value as numpy\n",
    "                value = value.cpu().numpy()\n",
    "\n",
    "            for i, map_i in enumerate(expandable_sp_games):\n",
    "                # get the current node for the self-play game\n",
    "                node = sp_games[map_i].node\n",
    "                \n",
    "                # get policy and value from self-play game\n",
    "                spg_policy, spg_value = policy[i], value[i]\n",
    "                \n",
    "                 # mask out illegal moves\n",
    "                valid_moves = self.game.get_valid_moves(node.state)\n",
    "                spg_policy *= valid_moves\n",
    "\n",
    "                # readjust back to probability distribution\n",
    "                spg_policy /= np.sum(spg_policy)\n",
    "                \n",
    "                # expand the node\n",
    "                node.expand(spg_policy)\n",
    "\n",
    "                # backpropagate values and visit counts\n",
    "                node.backpropagate(spg_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "721d5076-3088-441a-9871-2795d7d3e99c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlphaZeroParallel:\n",
    "    ''' Parallelized AlphaZero class for self-play and training '''\n",
    "    \n",
    "    def __init__(self, model, optimizer, game, args):\n",
    "        ''' Initialize the AlphaZero instance '''\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.game = game\n",
    "        self.args = args\n",
    "        self.mcts = MCTSParallel(game, args, model)\n",
    "\n",
    "    def self_play(self):\n",
    "        ''' Run a single self-play game until completion and generate outcome-appended training data '''\n",
    "        \n",
    "        return_memory = []\n",
    "        player = 1\n",
    "        sp_games = [SPG(self.game) for _ in range(self.args['num_parallel_games'])]\n",
    "\n",
    "        while len(sp_games) > 0:\n",
    "            # get states from all games\n",
    "            states = np.stack([spg.state for spg in sp_games])\n",
    "            \n",
    "            # get neutral states from all games\n",
    "            neutral_states = self.game.change_perspective(states, player)\n",
    "\n",
    "            # MCTS call\n",
    "            self.mcts.search(neutral_states, sp_games)\n",
    "\n",
    "            # loop games in reverse to avoid issues when removing terminated games from list\n",
    "            for i in range(len(sp_games))[::-1]:\n",
    "                spg = sp_games[i]\n",
    "                \n",
    "                # probabilities of action being good\n",
    "                action_probs = np.zeros(self.game.action_size)\n",
    "                for child in spg.root.children:\n",
    "                    action_probs[child.action_taken] = child.visit_count\n",
    "                action_probs /= np.sum(action_probs)\n",
    "    \n",
    "                # record a game snapshot\n",
    "                spg.memory.append((spg.root.state, action_probs, player))\n",
    "    \n",
    "                # randomly sample an action from the distribution\n",
    "                adjusted_action_probs = action_probs ** (1 / self.args['temperature']) # add flexibility for exploration / exploitation\n",
    "                adjusted_action_probs /= np.sum(adjusted_action_probs)\n",
    "                action = np.random.choice(self.game.action_size, p=adjusted_action_probs)\n",
    "    \n",
    "                # get the next state given the chosen action\n",
    "                spg.state = self.game.get_next_state(spg.state, action, player)\n",
    "    \n",
    "                # check for game completion\n",
    "                value, is_terminal = self.game.get_value_and_terminated(spg.state, action)\n",
    "    \n",
    "                if is_terminal:\n",
    "                    # get all states and policies from the games and append the outcomes\n",
    "                    for h_neutral_state, h_action_probs, h_player in spg.memory:\n",
    "                        h_outcome = value if h_player == player else self.game.get_opponent_value(value)\n",
    "                        return_memory.append((\n",
    "                            self.game.get_encoded_state(h_neutral_state),\n",
    "                            h_action_probs,\n",
    "                            h_outcome\n",
    "                        ))\n",
    "    \n",
    "                    del sp_games[i]\n",
    "\n",
    "            # swap the player and loop\n",
    "            player = self.game.get_opponent(player)\n",
    "\n",
    "        return return_memory\n",
    "\n",
    "    def train(self, memory):\n",
    "        ''' Train the model '''\n",
    "\n",
    "        # randomize training data\n",
    "        random.shuffle(memory)\n",
    "        \n",
    "        for batch_i in range(0, len(memory), self.args['batch_size']):\n",
    "            # sample a batch from training data\n",
    "            sample = memory[batch_i : min(len(memory) - 1, batch_i + self.args['batch_size'])]\n",
    "\n",
    "            # transpose list of tuples to independent lists\n",
    "            state, policy_targets, value_targets = zip(*sample)\n",
    "\n",
    "            # convert to numpy arrays\n",
    "            state = np.array(state)\n",
    "            policy_targets = np.array(policy_targets)\n",
    "            value_targets = np.array(value_targets).reshape(-1, 1) # wrap each value in its own array for simplicity later\n",
    "\n",
    "            # convert to tensors\n",
    "            state = torch.tensor(state, dtype=torch.float32, device=self.model.device)\n",
    "            policy_targets = torch.tensor(policy_targets, dtype=torch.float32, device=self.model.device)\n",
    "            value_targets = torch.tensor(value_targets, dtype=torch.float32, device=self.model.device)\n",
    "\n",
    "            # get model outputs\n",
    "            out_policy, out_value = self.model(state)\n",
    "\n",
    "            # get loss\n",
    "            policy_loss = F.cross_entropy(out_policy, policy_targets)\n",
    "            value_loss = F.mse_loss(out_value, value_targets)\n",
    "            loss = policy_loss + value_loss\n",
    "\n",
    "            # minimize loss via backpropagation\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    def learn(self):\n",
    "        ''' Generate self-play training data and train the model on it '''\n",
    "        \n",
    "        for iter in range(self.args['num_iters']):\n",
    "            memory = []\n",
    "            \n",
    "            self.model.eval()\n",
    "            for self_play_iter in trange(self.args['num_self_play_iters'] // self.args['num_parallel_games']):\n",
    "                memory += self.self_play()\n",
    "\n",
    "            self.model.train()\n",
    "            for epoch in trange(self.args['num_epochs']):\n",
    "                self.train(memory)\n",
    "\n",
    "            torch.save(self.model.state_dict(), f'models/{self.game}/model_{iter}.pt')\n",
    "            torch.save(self.optimizer.state_dict(), f'models/{self.game}/optimizer_{iter}.pt')\n",
    "\n",
    "class SPG:\n",
    "    ''' A self-play game '''\n",
    "    def __init__(self, game):\n",
    "        self.state = game.get_initial_state()\n",
    "        self.memory = []\n",
    "        self.root = None\n",
    "        self.node = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f989da5-eba5-4beb-8edd-5eeff772d7ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1c4faac156c4496bda628f8c0c7b0f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "game = ConnectFour()\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = ResNet(game, 9, 128, device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=0.0001)\n",
    "\n",
    "args = {\n",
    "    'C': 2,\n",
    "    'num_searches': 600,\n",
    "    'num_iters': 8,\n",
    "    'num_self_play_iters': 500,\n",
    "    'num_parallel_games': 100,\n",
    "    'num_epochs': 4,\n",
    "    'batch_size': 128,\n",
    "    'temperature': 1.25,\n",
    "    'dirichlet_epsilon': 0.25,\n",
    "    'dirichlet_alpha': 0.3\n",
    "}\n",
    "\n",
    "alphazero = AlphaZeroParallel(model, optimizer, game, args)\n",
    "alphazero.learn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "703608a9-b8be-4b03-b7da-2ba45921465c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]]\n",
      "Valid Moves: [0, 1, 2, 3, 4, 5, 6]\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "1:  3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0.]]\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0. -1.  0.  1.  0.  0.  0.]]\n",
      "Valid Moves: [0, 1, 2, 3, 4, 5, 6]\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "1:  4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0. -1.  0.  1.  1.  0.  0.]]\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0. -1.  0.  0.  0.  0.  0.]\n",
      " [ 0. -1.  0.  1.  1.  0.  0.]]\n",
      "Valid Moves: [0, 1, 2, 3, 4, 5, 6]\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "1:  2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0. -1.  0.  0.  0.  0.  0.]\n",
      " [ 0. -1.  1.  1.  1.  0.  0.]]\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0. -1.  0.  0.  0.  0.  0.]\n",
      " [ 0. -1.  1.  1.  1. -1.  0.]]\n",
      "Valid Moves: [0, 1, 2, 3, 4, 5, 6]\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "1:  4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0. -1.  0.  0.  1.  0.  0.]\n",
      " [ 0. -1.  1.  1.  1. -1.  0.]]\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0. -1.  0.  0.  0.  0.  0.]\n",
      " [ 0. -1.  0.  0.  1.  0.  0.]\n",
      " [ 0. -1.  1.  1.  1. -1.  0.]]\n",
      "Valid Moves: [0, 1, 2, 3, 4, 5, 6]\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "1:  1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  0.  0.]\n",
      " [ 0. -1.  0.  0.  0.  0.  0.]\n",
      " [ 0. -1.  0.  0.  1.  0.  0.]\n",
      " [ 0. -1.  1.  1.  1. -1.  0.]]\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0. -1.  0.  0.  0.  0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  0.  0.]\n",
      " [ 0. -1.  0.  0.  0.  0.  0.]\n",
      " [ 0. -1.  0.  0.  1.  0.  0.]\n",
      " [ 0. -1.  1.  1.  1. -1.  0.]]\n",
      "Valid Moves: [0, 1, 2, 3, 4, 5, 6]\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "1:  5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0. -1.  0.  0.  0.  0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  0.  0.]\n",
      " [ 0. -1.  0.  0.  0.  0.  0.]\n",
      " [ 0. -1.  0.  0.  1.  1.  0.]\n",
      " [ 0. -1.  1.  1.  1. -1.  0.]]\n",
      "[[ 0. -1.  0.  0.  0.  0.  0.]\n",
      " [ 0. -1.  0.  0.  0.  0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  0.  0.]\n",
      " [ 0. -1.  0.  0.  0.  0.  0.]\n",
      " [ 0. -1.  0.  0.  1.  1.  0.]\n",
      " [ 0. -1.  1.  1.  1. -1.  0.]]\n",
      "Valid Moves: [0, 2, 3, 4, 5, 6]\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "1:  6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0. -1.  0.  0.  0.  0.  0.]\n",
      " [ 0. -1.  0.  0.  0.  0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  0.  0.]\n",
      " [ 0. -1.  0.  0.  0.  0.  0.]\n",
      " [ 0. -1.  0.  0.  1.  1.  0.]\n",
      " [ 0. -1.  1.  1.  1. -1.  1.]]\n",
      "[[ 0. -1.  0.  0.  0.  0.  0.]\n",
      " [ 0. -1.  0.  0.  0.  0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  0.  0.]\n",
      " [ 0. -1.  0.  0.  0.  0.  0.]\n",
      " [ 0. -1.  0.  0.  1.  1.  0.]\n",
      " [-1. -1.  1.  1.  1. -1.  1.]]\n",
      "Valid Moves: [0, 2, 3, 4, 5, 6]\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "1:  5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0. -1.  0.  0.  0.  0.  0.]\n",
      " [ 0. -1.  0.  0.  0.  0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  0.  0.]\n",
      " [ 0. -1.  0.  0.  0.  1.  0.]\n",
      " [ 0. -1.  0.  0.  1.  1.  0.]\n",
      " [-1. -1.  1.  1.  1. -1.  1.]]\n",
      "[[ 0. -1.  0.  0.  0.  0.  0.]\n",
      " [ 0. -1.  0.  0.  0.  0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  0.  0.]\n",
      " [ 0. -1.  0.  0.  0.  1.  0.]\n",
      " [-1. -1.  0.  0.  1.  1.  0.]\n",
      " [-1. -1.  1.  1.  1. -1.  1.]]\n",
      "Valid Moves: [0, 2, 3, 4, 5, 6]\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "1:  5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0. -1.  0.  0.  0.  0.  0.]\n",
      " [ 0. -1.  0.  0.  0.  0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  1.  0.]\n",
      " [ 0. -1.  0.  0.  0.  1.  0.]\n",
      " [-1. -1.  0.  0.  1.  1.  0.]\n",
      " [-1. -1.  1.  1.  1. -1.  1.]]\n",
      "[[ 0. -1.  0.  0.  0.  0.  0.]\n",
      " [ 0. -1.  0.  0.  0. -1.  0.]\n",
      " [ 0.  1.  0.  0.  0.  1.  0.]\n",
      " [ 0. -1.  0.  0.  0.  1.  0.]\n",
      " [-1. -1.  0.  0.  1.  1.  0.]\n",
      " [-1. -1.  1.  1.  1. -1.  1.]]\n",
      "Valid Moves: [0, 2, 3, 4, 5, 6]\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "1:  3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0. -1.  0.  0.  0.  0.  0.]\n",
      " [ 0. -1.  0.  0.  0. -1.  0.]\n",
      " [ 0.  1.  0.  0.  0.  1.  0.]\n",
      " [ 0. -1.  0.  0.  0.  1.  0.]\n",
      " [-1. -1.  0.  1.  1.  1.  0.]\n",
      " [-1. -1.  1.  1.  1. -1.  1.]]\n",
      "[[ 0. -1.  0.  0.  0.  0.  0.]\n",
      " [ 0. -1.  0.  0.  0. -1.  0.]\n",
      " [ 0.  1.  0.  0.  0.  1.  0.]\n",
      " [ 0. -1.  0.  0.  0.  1.  0.]\n",
      " [-1. -1.  0.  1.  1.  1. -1.]\n",
      " [-1. -1.  1.  1.  1. -1.  1.]]\n",
      "Valid Moves: [0, 2, 3, 4, 5, 6]\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "1:  2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0. -1.  0.  0.  0.  0.  0.]\n",
      " [ 0. -1.  0.  0.  0. -1.  0.]\n",
      " [ 0.  1.  0.  0.  0.  1.  0.]\n",
      " [ 0. -1.  0.  0.  0.  1.  0.]\n",
      " [-1. -1.  1.  1.  1.  1. -1.]\n",
      " [-1. -1.  1.  1.  1. -1.  1.]]\n",
      "1 won\n"
     ]
    }
   ],
   "source": [
    "game = ConnectFour()\n",
    "player = 1\n",
    "\n",
    "args = {\n",
    "    'C': 2,\n",
    "    'num_searches': 100,\n",
    "    'dirichlet_epsilon': 0.0,\n",
    "    'dirichlet_alpha': 0.3\n",
    "}\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = ResNet(game, 9, 128, device)\n",
    "model.load_state_dict(torch.load(f'models/{game}/model_7.pt', map_location=device), strict=False)\n",
    "model.eval()\n",
    "\n",
    "mcts = MCTS(game, args, model)\n",
    "\n",
    "state = game.get_initial_state()\n",
    "\n",
    "while True:\n",
    "    print(state)\n",
    "\n",
    "    if player == 1:\n",
    "        valid_moves = game.get_valid_moves(state)\n",
    "        print('Valid Moves:' , [i for i in range(game.action_size) if valid_moves[i] == 1])\n",
    "        action = int(input(f'{player}: '))\n",
    "    \n",
    "        if valid_moves[action] == 0:\n",
    "            print('Invalid move')\n",
    "            continue\n",
    "    else:\n",
    "        neutral_state = game.change_perspective(state, player)\n",
    "        mcts_probs = mcts.search(neutral_state)\n",
    "        action = np.argmax(mcts_probs)\n",
    "\n",
    "    state = game.get_next_state(state, action, player)\n",
    "\n",
    "    value, is_terminal = game.get_value_and_terminated(state, action)\n",
    "\n",
    "    if is_terminal:\n",
    "        print(state)\n",
    "        if value == 1:\n",
    "            print(player, 'won')\n",
    "        else:\n",
    "            print('Draw')\n",
    "        break\n",
    "\n",
    "    player = game.get_opponent(player)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e4eb825-e431-4ea0-998f-d1da412b5daf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8251b500-c17c-49af-899a-2d3c5f6420fe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
