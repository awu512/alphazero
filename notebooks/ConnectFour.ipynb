{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7620efee-0dba-4939-ad3d-880af53233b5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-24T03:24:22.152294Z",
     "start_time": "2023-11-24T03:24:20.805377Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.24.4\n",
      "2.1.1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fa038849230>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "import random\n",
    "from tqdm.notebook import tqdm, trange\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "import numpy as np\n",
    "print(np.__version__)\n",
    "\n",
    "import torch\n",
    "print(torch.__version__)\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d894942-1528-4af0-8993-7d0d1e30b243",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConnectFour:\n",
    "    ''' Game definition for ConnectFour '''\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.row_count = 6\n",
    "        self.col_count = 7\n",
    "        self.action_size = self.col_count\n",
    "        self.win_condition = 4\n",
    "\n",
    "    def __repr__(self):\n",
    "        return 'ConnectFour'\n",
    "\n",
    "    def get_initial_state(self):\n",
    "        ''' Get board with all zeros '''\n",
    "        return np.zeros((self.row_count, self.col_count))\n",
    "\n",
    "    def get_next_state(self, state, action, player):\n",
    "        ''' Get the next state given the given action by the given player '''\n",
    "        row = np.max(np.where(state[:, action] == 0))\n",
    "        state[row, action] = player\n",
    "        return state\n",
    "\n",
    "    def get_valid_moves(self, state):\n",
    "        ''' Get all the legal moves in the position '''\n",
    "        return (state[0] == 0).astype(np.uint8)\n",
    "\n",
    "    def check_win(self, state, action):\n",
    "        ''' Check if the given action has led to a win '''\n",
    "        if action == None:\n",
    "            return False\n",
    "        \n",
    "        row = np.min(np.where(state[:, action] != 0))\n",
    "        column = action\n",
    "        player = state[row][column]\n",
    "\n",
    "        def count(offset_row, offset_column):\n",
    "            for i in range(1, self.win_condition):\n",
    "                r = row + offset_row * i\n",
    "                c = action + offset_column * i\n",
    "                if (\n",
    "                    r < 0 \n",
    "                    or r >= self.row_count\n",
    "                    or c < 0 \n",
    "                    or c >= self.col_count\n",
    "                    or state[r][c] != player\n",
    "                ):\n",
    "                    return i - 1\n",
    "            return self.win_condition - 1\n",
    "\n",
    "        return (\n",
    "            count(1, 0) >= self.win_condition - 1 # vertical\n",
    "            or (count(0, 1) + count(0, -1)) >= self.win_condition - 1 # horizontal\n",
    "            or (count(1, 1) + count(-1, -1)) >= self.win_condition - 1 # top left diagonal\n",
    "            or (count(1, -1) + count(-1, 1)) >= self.win_condition - 1 # top right diagonal\n",
    "        )\n",
    "\n",
    "    def get_value_and_terminated(self, state, action):\n",
    "        ''' Get the value (win/tie) and if the game has terminated '''\n",
    "        if self.check_win(state, action):\n",
    "            return 1, True\n",
    "        if np.sum(self.get_valid_moves(state)) == 0:\n",
    "            return 0, True\n",
    "        return 0, False\n",
    "\n",
    "    def get_opponent(self, player):\n",
    "        ''' Get the opponent of the player '''\n",
    "        return -player\n",
    "\n",
    "    def get_opponent_value(self, value):\n",
    "        ''' Get the value from the opponent's perspective '''\n",
    "        return -value\n",
    "\n",
    "    def change_perspective(self, state, player):\n",
    "        ''' Transform the state to be from the opponent's perspective '''\n",
    "        return state * player\n",
    "\n",
    "    def get_encoded_state(self, state):\n",
    "        ''' Get the network-ready encoded state of the game '''\n",
    "        encoded_state = np.stack((state == -1, state == 0, state == 1)).astype(np.float32)\n",
    "\n",
    "        # check for batched states\n",
    "        if len(state.shape) == 3:\n",
    "            encoded_state = np.swapaxes(encoded_state, 0, 1)\n",
    "        \n",
    "        return encoded_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3bd0cd66-2da5-4e7e-97eb-becc878292d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    "    ''' The residual neural network implementation for AlphaZero '''\n",
    "    \n",
    "    def __init__(self, game, num_resblocks, num_hidden, device):\n",
    "        super().__init__()\n",
    "\n",
    "        # store the specified device for computation\n",
    "        self.device = device\n",
    "\n",
    "        # store the game for predictions\n",
    "        self.game = game\n",
    "\n",
    "        # initial NN block\n",
    "        self.start_block = nn.Sequential(\n",
    "            nn.Conv2d(3, num_hidden, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(num_hidden),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        # shared portion of network between policy and value heads\n",
    "        self.backbone = nn.ModuleList(\n",
    "            [ResBlock(num_hidden) for _ in range(num_resblocks)]\n",
    "        )\n",
    "\n",
    "        # the portion of the network responsible for outputting policies\n",
    "        self.policy_head = nn.Sequential(\n",
    "            nn.Conv2d(num_hidden, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(32 * game.row_count * game.col_count, game.action_size)\n",
    "        )\n",
    "\n",
    "        # the portion of the network responsible for outputting values\n",
    "        self.value_head = nn.Sequential(\n",
    "            nn.Conv2d(num_hidden, 3, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(3),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(3 * game.row_count * game.col_count, 1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "        # send computation to the device\n",
    "        self.to(device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        ''' Feed forward implementation for this network '''\n",
    "        x = self.start_block(x)\n",
    "        for resblock in self.backbone:\n",
    "            x = resblock(x)\n",
    "        policy = self.policy_head(x)\n",
    "        value = self.value_head(x)\n",
    "\n",
    "        return policy, value\n",
    "        \n",
    "\n",
    "class ResBlock(nn.Module):\n",
    "    ''' ResNet block '''\n",
    "    \n",
    "    def __init__(self, num_hidden):\n",
    "        ''' Initialize the block '''\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(num_hidden, num_hidden, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(num_hidden)\n",
    "        self.conv2 = nn.Conv2d(num_hidden, num_hidden, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(num_hidden)\n",
    "\n",
    "    def forward(self, x):\n",
    "        ''' Feed forward implementation for this block '''\n",
    "        residual = x\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.bn2(self.conv2(x))\n",
    "        x += residual\n",
    "        x = F.relu(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e888f91a-00c3-46bf-a9f5-d0685368479e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  1.]\n",
      " [ 0.  0. -1.  0. -1.  0.  1.]]\n",
      "[[[0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 1. 0. 1. 0. 0.]]\n",
      "\n",
      " [[1. 1. 1. 1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1. 1. 1. 0.]\n",
      "  [1. 1. 0. 1. 0. 1. 0.]]\n",
      "\n",
      " [[0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 1.]\n",
      "  [0. 0. 0. 0. 0. 0. 1.]]]\n",
      "-0.0350162647664547\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W NNPACK.cpp:64] Could not initialize NNPACK! Reason: Unsupported hardware.\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "game = ConnectFour()\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "state = game.get_initial_state()\n",
    "state = game.get_next_state(state, 2, -1)\n",
    "state = game.get_next_state(state, 4, -1)\n",
    "state = game.get_next_state(state, 6, 1)\n",
    "state = game.get_next_state(state, 6, 1)\n",
    "\n",
    "print(state)\n",
    "\n",
    "encoded_state = game.get_encoded_state(state)\n",
    "\n",
    "print(encoded_state)\n",
    "\n",
    "tensor_state = torch.tensor(encoded_state, device=device).unsqueeze(0)\n",
    "\n",
    "model = ResNet(game, 9, 128, device=device)\n",
    "model.load_state_dict(torch.load(f'../models/{game}/model_7.pt', map_location=device), strict=False)\n",
    "model.eval()\n",
    "\n",
    "policy, value = model(tensor_state)\n",
    "value = value.item()\n",
    "policy = torch.softmax(policy, axis=1).squeeze(0).detach().cpu().numpy()\n",
    "\n",
    "print(value)\n",
    "\n",
    "plt.bar(range(game.action_size), policy)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fc005069-4b4f-41cb-97fb-65bad1af58cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    ''' A Monte Carlo Tree Search node '''\n",
    "    \n",
    "    def __init__(self, game, args, state, parent=None, action_taken=None, prior=0, visit_count=0):\n",
    "        self.game = game\n",
    "        self.args = args\n",
    "        self.state = state\n",
    "        self.parent = parent\n",
    "        self.action_taken = action_taken\n",
    "        self.prior = prior\n",
    "\n",
    "        self.children = []\n",
    "\n",
    "        self.visit_count = visit_count\n",
    "        self.value_sum = 0\n",
    "\n",
    "    def is_expanded(self):\n",
    "        ''' Check if the node already has children. Since we expand all children of a node at once, we check for >0 children '''\n",
    "        return len(self.children) > 0\n",
    "\n",
    "    def select(self):\n",
    "        ''' Select a child to explore '''\n",
    "        best_child = None\n",
    "        best_ucb = -np.inf\n",
    "\n",
    "        for child in self.children:\n",
    "            ucb = self.get_ucb(child)\n",
    "            if ucb > best_ucb:\n",
    "                best_child = child\n",
    "                best_ucb = ucb\n",
    "\n",
    "        return best_child\n",
    "\n",
    "    def get_ucb(self, child):\n",
    "        ''' Get how promising a move is from the opponent's perspective, normalized on [0,1] '''\n",
    "        if child.visit_count == 0:\n",
    "            q = 0\n",
    "        else:\n",
    "            q = 1 - ((child.value_sum / child.visit_count) + 1) / 2 \n",
    "\n",
    "        return q + self.args['C'] * (math.sqrt(self.visit_count) / (child.visit_count + 1)) * child.prior\n",
    "\n",
    "    def expand(self, policy):\n",
    "        ''' Expand a node by adding all legal child moves '''\n",
    "        for action, prob in enumerate(policy):\n",
    "            if prob > 0:\n",
    "                child_state = self.state.copy()\n",
    "                child_state = self.game.get_next_state(child_state, action, 1)\n",
    "                child_state = self.game.change_perspective(child_state, player=-1)\n",
    "        \n",
    "                child = Node(self.game, self.args, child_state, self, action, prob)\n",
    "                self.children.append(child)\n",
    "\n",
    "    def backpropagate(self, value):\n",
    "        ''' Propagate value sums and visit counts from children to all parents '''\n",
    "        self.value_sum += value\n",
    "        self.visit_count += 1\n",
    "\n",
    "        # flip value for opponent (parent)\n",
    "        value = self.game.get_opponent_value(value)\n",
    "\n",
    "        if self.parent is not None:\n",
    "            self.parent.backpropagate(value)\n",
    "\n",
    "class MCTS:\n",
    "    ''' A Monte Carlo Tree Search '''\n",
    "    \n",
    "    def __init__(self, game, args, model):\n",
    "        self.game = game\n",
    "        self.args = args\n",
    "        self.model = model\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def search(self, state):\n",
    "        ''' Expand and explore the MCTS and update value sums and visit counts '''\n",
    "        # DEFINE ROOT\n",
    "        root = Node(self.game, self.args, state, visit_count=1)\n",
    "\n",
    "        # add some random noise to policy to increase exploration\n",
    "        policy, _ = self.model(\n",
    "            torch.tensor(self.game.get_encoded_state(state), device=self.model.device).unsqueeze(0)\n",
    "        )\n",
    "        policy = torch.softmax(policy, axis=1).squeeze(0).cpu().numpy()\n",
    "        policy = ((1 - self.args['dirichlet_epsilon']) * \n",
    "                    policy + \n",
    "                    self.args['dirichlet_epsilon'] * \n",
    "                    np.random.dirichlet([self.args['dirichlet_alpha']] * \n",
    "                    self.game.action_size\n",
    "                ))\n",
    "        valid_moves = self.game.get_valid_moves(state)\n",
    "        policy *= valid_moves\n",
    "        policy /= np.sum(policy)\n",
    "        root.expand(policy)\n",
    "        \n",
    "        for search in range(self.args['num_searches']):\n",
    "            node = root\n",
    "            \n",
    "            # SELECTION\n",
    "            while node.is_expanded():\n",
    "                node = node.select()\n",
    "                \n",
    "            # check for end of game\n",
    "            value, is_terminal = self.game.get_value_and_terminated(node.state, node.action_taken)\n",
    "\n",
    "            # flip parent value\n",
    "            value = self.game.get_opponent_value(value)\n",
    "\n",
    "            if not is_terminal:\n",
    "                # get output from model\n",
    "                policy, value = self.model(\n",
    "                    torch.tensor(self.game.get_encoded_state(node.state), device=self.model.device).unsqueeze(0)\n",
    "                )\n",
    "\n",
    "                # change policy to proabability distribution\n",
    "                policy = torch.softmax(policy, axis=1).squeeze(0).cpu().numpy()\n",
    "\n",
    "                # mask out illegal moves\n",
    "                valid_moves = self.game.get_valid_moves(node.state)\n",
    "                policy *= valid_moves\n",
    "\n",
    "                # readjust back to probability distribution\n",
    "                policy /= np.sum(policy)\n",
    "\n",
    "                # get the value as a number from singleton tensor\n",
    "                value = value.item()\n",
    "                \n",
    "                # EXPANSION\n",
    "                node.expand(policy)\n",
    "    \n",
    "            # BACKPROP\n",
    "            node.backpropagate(value)\n",
    "\n",
    "        # probabilities of action being good\n",
    "        action_probs = np.zeros(self.game.action_size)\n",
    "        for child in root.children:\n",
    "            action_probs[child.action_taken] = child.visit_count\n",
    "\n",
    "        action_probs /= np.sum(action_probs)\n",
    "        return action_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "919a7ac9-28a6-4c49-b069-004b79290181",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlphaZero:\n",
    "    ''' AlphaZero class for self-play and training '''\n",
    "    \n",
    "    def __init__(self, model, optimizer, game, args):\n",
    "        ''' Initialize the AlphaZero instance '''\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.game = game\n",
    "        self.args = args\n",
    "        self.mcts = MCTS(game, args, model)\n",
    "\n",
    "    def self_play(self):\n",
    "        ''' Run a single self-play game until completion and generate outcome-appended training data '''\n",
    "        \n",
    "        memory = []\n",
    "        player = 1\n",
    "        state = self.game.get_initial_state()\n",
    "\n",
    "        while True:\n",
    "            # get the current state and action probabilities from MCTS\n",
    "            neutral_state = self.game.change_perspective(state, player)\n",
    "            action_probs = self.mcts.search(neutral_state)\n",
    "\n",
    "            # record a game snapshot\n",
    "            memory.append((neutral_state, action_probs, player))\n",
    "\n",
    "            # randomly sample an action from the distribution\n",
    "            adjusted_action_probs = action_probs ** (1 / self.args['temperature']) # add flexibility for exploration / exploitation\n",
    "            adjusted_action_probs /= np.sum(adjusted_action_probs)\n",
    "            action = np.random.choice(self.game.action_size, p=adjusted_action_probs)\n",
    "\n",
    "            # get the next state given the chosen action\n",
    "            state = self.game.get_next_state(state, action, player)\n",
    "\n",
    "            # check for game completion\n",
    "            value, is_terminal = self.game.get_value_and_terminated(state, action)\n",
    "\n",
    "            if is_terminal:\n",
    "                # get all states and policies from the game and append the outcome\n",
    "                return [(\n",
    "                    self.game.get_encoded_state(h_state),\n",
    "                    h_action_probs,\n",
    "                    value if h_player == player else self.game.get_opponent_value(value)\n",
    "                ) for h_state, h_action_probs, h_player in memory]\n",
    "\n",
    "            # swap the player and loop\n",
    "            player = self.game.get_opponent(player)\n",
    "\n",
    "    def train(self, memory):\n",
    "        ''' Train the model '''\n",
    "\n",
    "        # randomize training data\n",
    "        random.shuffle(memory)\n",
    "        \n",
    "        for batch_i in range(0, len(memory), self.args['batch_size']):\n",
    "            # sample a batch from training data\n",
    "            sample = memory[batch_i : min(len(memory) - 1, batch_i + self.args['batch_size'])]\n",
    "\n",
    "            # transpose list of tuples to independent lists\n",
    "            state, policy_targets, value_targets = zip(*sample)\n",
    "\n",
    "            # convert to numpy arrays\n",
    "            state = np.array(state)\n",
    "            policy_targets = np.array(policy_targets)\n",
    "            value_targets = np.array(value_targets).reshape(-1, 1) # wrap each value in its own array for simplicity later\n",
    "\n",
    "            # convert to tensors\n",
    "            state = torch.tensor(state, dtype=torch.float32, device=self.model.device)\n",
    "            policy_targets = torch.tensor(policy_targets, dtype=torch.float32, device=self.model.device)\n",
    "            value_targets = torch.tensor(value_targets, dtype=torch.float32, device=self.model.device)\n",
    "\n",
    "            # get model outputs\n",
    "            out_policy, out_value = self.model(state)\n",
    "\n",
    "            # get loss\n",
    "            policy_loss = F.cross_entropy(out_policy, policy_targets)\n",
    "            value_loss = F.mse_loss(out_value, value_targets)\n",
    "            loss = policy_loss + value_loss\n",
    "\n",
    "            # minimize loss via backpropagation\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    def learn(self):\n",
    "        ''' Generate self-play training data and train the model on it '''\n",
    "        \n",
    "        for iter in range(self.args['num_iters']):\n",
    "            memory = []\n",
    "            \n",
    "            self.model.eval()\n",
    "            with tqdm(total=self.args['num_self_play_iters']) as pbar:\n",
    "                with ThreadPoolExecutor(max_workers=self.args['num_threads']) as executor:\n",
    "                    futures = [executor.submit(self.self_play) for _ in range(self.args['num_self_play_iters'])]\n",
    "                    for future in as_completed(futures):\n",
    "                        memory += future.result()\n",
    "                        pbar.update(1)\n",
    "\n",
    "            self.model.train()\n",
    "            for epoch in trange(self.args['num_epochs']):\n",
    "                self.train(memory)\n",
    "\n",
    "            torch.save(self.model.state_dict(), f'../models/{self.game}/model_{iter}.pt')\n",
    "            torch.save(self.optimizer.state_dict(), f'../models/{self.game}/optimizer_{iter}.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b77aafbd-5988-42c8-a8c0-fae0fbee464c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MCTSParallel:\n",
    "    ''' A parallelized Monte Carlo Tree Search '''\n",
    "    \n",
    "    def __init__(self, game, args, model):\n",
    "        self.game = game\n",
    "        self.args = args\n",
    "        self.model = model\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def search(self, states, sp_games):\n",
    "        ''' Expand and explore the MCTS and update value sums and visit counts '''\n",
    "\n",
    "        # add some random noise to policy to increase exploration\n",
    "        policy, _ = self.model(torch.tensor(self.game.get_encoded_state(states), device=self.model.device))\n",
    "        policy = torch.softmax(policy, axis=1).cpu().numpy()\n",
    "        policy = ((1 - self.args['dirichlet_epsilon']) * \n",
    "                    policy + \n",
    "                    self.args['dirichlet_epsilon'] * \n",
    "                    np.random.dirichlet([self.args['dirichlet_alpha']] * \n",
    "                    self.game.action_size,\n",
    "                    size=policy.shape[0]\n",
    "                ))\n",
    "\n",
    "        for i, spg in enumerate(sp_games):\n",
    "            # get policy for this self-play game\n",
    "            spg_policy = policy[i]\n",
    "\n",
    "            # mask illegal moves out of policy\n",
    "            valid_moves = self.game.get_valid_moves(states[i])\n",
    "            spg_policy *= valid_moves\n",
    "            spg_policy /= np.sum(spg_policy)\n",
    "    \n",
    "            spg.root = Node(self.game, self.args, states[i], visit_count=1)\n",
    "            \n",
    "            spg.root.expand(spg_policy)\n",
    "        \n",
    "        for search in range(self.args['num_searches']):\n",
    "            for spg in sp_games:\n",
    "                spg.node = None\n",
    "                node = spg.root\n",
    "                \n",
    "                # SELECTION\n",
    "                while node.is_expanded():\n",
    "                    node = node.select()\n",
    "                    \n",
    "                # check for end of game\n",
    "                value, is_terminal = self.game.get_value_and_terminated(node.state, node.action_taken)\n",
    "    \n",
    "                # flip parent value\n",
    "                value = self.game.get_opponent_value(value)\n",
    "\n",
    "                # backpropagate value sums and visit counts if game ended\n",
    "                if is_terminal:\n",
    "                    node.backpropagate(value)\n",
    "\n",
    "                # otherwise, store the current node\n",
    "                else:\n",
    "                    spg.node = node\n",
    "\n",
    "            expandable_sp_games = [map_i for map_i in range(len(sp_games)) if sp_games[map_i].node is not None]\n",
    "\n",
    "            if len(expandable_sp_games) > 0:\n",
    "                states = np.stack([sp_games[map_i].node.state for map_i in expandable_sp_games])\n",
    "\n",
    "                # get output from model\n",
    "                policy, value = self.model(\n",
    "                    torch.tensor(self.game.get_encoded_state(states), device=self.model.device)\n",
    "                )\n",
    "\n",
    "                # change policy to proabability distribution\n",
    "                policy = torch.softmax(policy, axis=1).cpu().numpy()\n",
    "\n",
    "                # get value as numpy\n",
    "                value = value.cpu().numpy()\n",
    "\n",
    "            for i, map_i in enumerate(expandable_sp_games):\n",
    "                # get the current node for the self-play game\n",
    "                node = sp_games[map_i].node\n",
    "                \n",
    "                # get policy and value from self-play game\n",
    "                spg_policy, spg_value = policy[i], value[i]\n",
    "                \n",
    "                 # mask out illegal moves\n",
    "                valid_moves = self.game.get_valid_moves(node.state)\n",
    "                spg_policy *= valid_moves\n",
    "\n",
    "                # readjust back to probability distribution\n",
    "                spg_policy /= np.sum(spg_policy)\n",
    "                \n",
    "                # expand the node\n",
    "                node.expand(spg_policy)\n",
    "\n",
    "                # backpropagate values and visit counts\n",
    "                node.backpropagate(spg_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "721d5076-3088-441a-9871-2795d7d3e99c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlphaZeroParallel:\n",
    "    ''' Parallelized AlphaZero class for self-play and training '''\n",
    "    \n",
    "    def __init__(self, model, optimizer, game, args):\n",
    "        ''' Initialize the AlphaZero instance '''\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.game = game\n",
    "        self.args = args\n",
    "        self.mcts = MCTSParallel(game, args, model)\n",
    "\n",
    "    def self_play(self):\n",
    "        ''' Run self-play games until completion and generate outcome-appended training data '''\n",
    "        \n",
    "        return_memory = []\n",
    "        player = 1\n",
    "        sp_games = [SPG(self.game) for _ in range(self.args['num_parallel_games'])]\n",
    "\n",
    "        while len(sp_games) > 0:\n",
    "            # get states from all games\n",
    "            states = np.stack([spg.state for spg in sp_games])\n",
    "            \n",
    "            # get neutral states from all games\n",
    "            neutral_states = self.game.change_perspective(states, player)\n",
    "\n",
    "            # MCTS call\n",
    "            self.mcts.search(neutral_states, sp_games)\n",
    "\n",
    "            # loop games in reverse to avoid issues when removing terminated games from list\n",
    "            for i in range(len(sp_games))[::-1]:\n",
    "                spg = sp_games[i]\n",
    "                \n",
    "                # probabilities of action being good\n",
    "                action_probs = np.zeros(self.game.action_size)\n",
    "                for child in spg.root.children:\n",
    "                    action_probs[child.action_taken] = child.visit_count\n",
    "                action_probs /= np.sum(action_probs)\n",
    "    \n",
    "                # record a game snapshot\n",
    "                spg.memory.append((spg.root.state, action_probs, player))\n",
    "    \n",
    "                # randomly sample an action from the distribution\n",
    "                adjusted_action_probs = action_probs ** (1 / self.args['temperature']) # add flexibility for exploration / exploitation\n",
    "                adjusted_action_probs /= np.sum(adjusted_action_probs)\n",
    "                action = np.random.choice(self.game.action_size, p=adjusted_action_probs)\n",
    "    \n",
    "                # get the next state given the chosen action\n",
    "                spg.state = self.game.get_next_state(spg.state, action, player)\n",
    "    \n",
    "                # check for game completion\n",
    "                value, is_terminal = self.game.get_value_and_terminated(spg.state, action)\n",
    "    \n",
    "                if is_terminal:\n",
    "                    # get all states and policies from the games and append the outcomes\n",
    "                    for h_neutral_state, h_action_probs, h_player in spg.memory:\n",
    "                        h_outcome = value if h_player == player else self.game.get_opponent_value(value)\n",
    "                        return_memory.append((\n",
    "                            self.game.get_encoded_state(h_neutral_state),\n",
    "                            h_action_probs,\n",
    "                            h_outcome\n",
    "                        ))\n",
    "    \n",
    "                    del sp_games[i]\n",
    "\n",
    "            # swap the player and loop\n",
    "            player = self.game.get_opponent(player)\n",
    "\n",
    "        return return_memory\n",
    "\n",
    "    def train(self, memory):\n",
    "        ''' Train the model '''\n",
    "\n",
    "        # randomize training data\n",
    "        random.shuffle(memory)\n",
    "        \n",
    "        for batch_i in range(0, len(memory), self.args['batch_size']):\n",
    "            # sample a batch from training data\n",
    "            sample = memory[batch_i : min(len(memory) - 1, batch_i + self.args['batch_size'])]\n",
    "\n",
    "            # transpose list of tuples to independent lists\n",
    "            state, policy_targets, value_targets = zip(*sample)\n",
    "\n",
    "            # convert to numpy arrays\n",
    "            state = np.array(state)\n",
    "            policy_targets = np.array(policy_targets)\n",
    "            value_targets = np.array(value_targets).reshape(-1, 1) # wrap each value in its own array for simplicity later\n",
    "\n",
    "            # convert to tensors\n",
    "            state = torch.tensor(state, dtype=torch.float32, device=self.model.device)\n",
    "            policy_targets = torch.tensor(policy_targets, dtype=torch.float32, device=self.model.device)\n",
    "            value_targets = torch.tensor(value_targets, dtype=torch.float32, device=self.model.device)\n",
    "\n",
    "            # get model outputs\n",
    "            out_policy, out_value = self.model(state)\n",
    "\n",
    "            # get loss\n",
    "            policy_loss = F.cross_entropy(out_policy, policy_targets)\n",
    "            value_loss = F.mse_loss(out_value, value_targets)\n",
    "            loss = policy_loss + value_loss\n",
    "\n",
    "            # minimize loss via backpropagation\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    def learn(self):\n",
    "        ''' Generate self-play training data and train the model on it '''\n",
    "        \n",
    "        for iter in range(self.args['num_iters']):\n",
    "            memory = []\n",
    "            \n",
    "            self.model.eval()\n",
    "            for self_play_iter in trange(self.args['num_self_play_iters'] // self.args['num_parallel_games']):\n",
    "                memory += self.self_play()\n",
    "\n",
    "            self.model.train()\n",
    "            for epoch in trange(self.args['num_epochs']):\n",
    "                self.train(memory)\n",
    "\n",
    "            torch.save(self.model.state_dict(), f'../models/{self.game}/model_{iter}.pt')\n",
    "            torch.save(self.optimizer.state_dict(), f'../models/{self.game}/optimizer_{iter}.pt')\n",
    "\n",
    "class SPG:\n",
    "    ''' A self-play game '''\n",
    "    def __init__(self, game):\n",
    "        self.state = game.get_initial_state()\n",
    "        self.memory = []\n",
    "        self.root = None\n",
    "        self.node = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5f989da5-eba5-4beb-8edd-5eeff772d7ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c01268751c44f4e8d23a204f087375d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab1d22d7f0e6413797a5dccfddef82ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "game = ConnectFour()\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = ResNet(game, 9, 128, device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=0.0001)\n",
    "\n",
    "args = {\n",
    "    'num_threads': 12,\n",
    "    'C': 2,\n",
    "    'num_searches': 200,\n",
    "    'num_iters': 1,\n",
    "    'num_self_play_iters': 24,\n",
    "    'num_parallel_games': 100,\n",
    "    'num_epochs': 4,\n",
    "    'batch_size': 128,\n",
    "    'temperature': 1.25,\n",
    "    'dirichlet_epsilon': 0.25,\n",
    "    'dirichlet_alpha': 0.3\n",
    "}\n",
    "\n",
    "alphazero = AlphaZero(model, optimizer, game, args)\n",
    "alphazero.learn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5dcd3a3-ac48-478b-a375-c3bb3a887472",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]]\n",
      "Valid Moves: [0, 1, 2, 3, 4, 5, 6]\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "1:  3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0.]]\n",
      "[0.01       0.05       0.02333333 0.88166667 0.02666667 0.00333333\n",
      " 0.005     ]\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0. -1.  0.  0.  0.]\n",
      " [ 0.  0.  0.  1.  0.  0.  0.]]\n",
      "Valid Moves: [0, 1, 2, 3, 4, 5, 6]\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "1:  2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0. -1.  0.  0.  0.]\n",
      " [ 0.  0.  1.  1.  0.  0.  0.]]\n",
      "[0.01166667 0.07833333 0.025      0.02666667 0.82       0.025\n",
      " 0.01333333]\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0. -1.  0.  0.  0.]\n",
      " [ 0.  0.  1.  1. -1.  0.  0.]]\n",
      "Valid Moves: [0, 1, 2, 3, 4, 5, 6]\n"
     ]
    }
   ],
   "source": [
    "game = ConnectFour()\n",
    "player = 1\n",
    "\n",
    "args = {\n",
    "    'C': 2,\n",
    "    'num_searches': 600,\n",
    "    'dirichlet_epsilon': 0.0,\n",
    "    'dirichlet_alpha': 0.3\n",
    "}\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = ResNet(game, 9, 128, device)\n",
    "model.load_state_dict(torch.load(f'../models/cf0_control/model_11.pt', map_location=device), strict=False)\n",
    "model.eval()\n",
    "\n",
    "mcts = MCTS(game, args, model)\n",
    "\n",
    "state = game.get_initial_state()\n",
    "\n",
    "while True:\n",
    "    print(state)\n",
    "\n",
    "    if player == 1:\n",
    "        valid_moves = game.get_valid_moves(state)\n",
    "        print('Valid Moves:' , [i for i in range(game.action_size) if valid_moves[i] == 1])\n",
    "        action = int(input(f'{player}: '))\n",
    "    \n",
    "        if valid_moves[action] == 0:\n",
    "            print('Invalid move')\n",
    "            continue\n",
    "    else:\n",
    "        neutral_state = game.change_perspective(state, player)\n",
    "        mcts_probs = mcts.search(neutral_state)\n",
    "        action = np.argmax(mcts_probs)\n",
    "\n",
    "    state = game.get_next_state(state, action, player)\n",
    "\n",
    "    value, is_terminal = game.get_value_and_terminated(state, action)\n",
    "\n",
    "    if is_terminal:\n",
    "        print(state)\n",
    "        if value == 1:\n",
    "            print(player, 'won')\n",
    "        else:\n",
    "            print('Draw')\n",
    "        break\n",
    "\n",
    "    player = game.get_opponent(player)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "90774e21-50dd-40b5-bc89-f506fa4e337e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]]\n",
      "[[0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0.]]\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0. -1.  0.  0.  0.]\n",
      " [ 0.  0.  0.  1.  0.  0.  0.]]\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0. -1.  0.  0.  0.]\n",
      " [ 0.  0.  1.  1.  0.  0.  0.]]\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0. -1.  0.  0.  0.]\n",
      " [ 0.  0.  1.  1. -1.  0.  0.]]\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0. -1.  0.  0.  0.]\n",
      " [ 0.  1.  1.  1. -1.  0.  0.]]\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0. -1.  0.  0.  0.]\n",
      " [-1.  1.  1.  1. -1.  0.  0.]]\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0. -1.  1.  0.  0.]\n",
      " [-1.  1.  1.  1. -1.  0.  0.]]\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0. -1.  0.  0.]\n",
      " [ 0.  0.  0. -1.  1.  0.  0.]\n",
      " [-1.  1.  1.  1. -1.  0.  0.]]\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0. -1.  0.  0.]\n",
      " [ 0.  0.  1. -1.  1.  0.  0.]\n",
      " [-1.  1.  1.  1. -1.  0.  0.]]\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0. -1. -1.  0.  0.]\n",
      " [ 0.  0.  1. -1.  1.  0.  0.]\n",
      " [-1.  1.  1.  1. -1.  0.  0.]]\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  1.  0.  0.  0.]\n",
      " [ 0.  0.  0. -1. -1.  0.  0.]\n",
      " [ 0.  0.  1. -1.  1.  0.  0.]\n",
      " [-1.  1.  1.  1. -1.  0.  0.]]\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  1.  0.  0.  0.]\n",
      " [ 0.  0. -1. -1. -1.  0.  0.]\n",
      " [ 0.  0.  1. -1.  1.  0.  0.]\n",
      " [-1.  1.  1.  1. -1.  0.  0.]]\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  1.  1.  0.  0.  0.]\n",
      " [ 0.  0. -1. -1. -1.  0.  0.]\n",
      " [ 0.  0.  1. -1.  1.  0.  0.]\n",
      " [-1.  1.  1.  1. -1.  0.  0.]]\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  1.  1.  0.  0.  0.]\n",
      " [ 0.  0. -1. -1. -1.  0.  0.]\n",
      " [ 0. -1.  1. -1.  1.  0.  0.]\n",
      " [-1.  1.  1.  1. -1.  0.  0.]]\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  1.  1.  0.  0.  0.]\n",
      " [ 0.  1. -1. -1. -1.  0.  0.]\n",
      " [ 0. -1.  1. -1.  1.  0.  0.]\n",
      " [-1.  1.  1.  1. -1.  0.  0.]]\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0. -1.  1.  1.  0.  0.  0.]\n",
      " [ 0.  1. -1. -1. -1.  0.  0.]\n",
      " [ 0. -1.  1. -1.  1.  0.  0.]\n",
      " [-1.  1.  1.  1. -1.  0.  0.]]\n",
      "Agent 2 won\n",
      "92.4530439376831\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self, model, game, args):\n",
    "        self.model = model\n",
    "        self.game = game\n",
    "        self.args = args\n",
    "        self.mcts = MCTS(self.game, self.args, self.model)\n",
    "\n",
    "    def get_action(self, state):\n",
    "        neutral_state = self.game.change_perspective(state, player)\n",
    "        mcts_probs = self.mcts.search(neutral_state)\n",
    "        action = np.argmax(mcts_probs)\n",
    "\n",
    "        return action\n",
    "\n",
    "game = ConnectFour()\n",
    "player = 1\n",
    "\n",
    "args = {\n",
    "    'C': 2,\n",
    "    'num_searches': 200,\n",
    "    'dirichlet_epsilon': 0.0,\n",
    "    'dirichlet_alpha': 0.3\n",
    "}\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = ResNet(game, 9, 128, device)\n",
    "model.load_state_dict(torch.load(f'../models/cf0_control/model_11.pt', map_location=device))\n",
    "model.eval()\n",
    "\n",
    "a1 = Agent(model, game, args)\n",
    "a2 = Agent(model, game, args)\n",
    "\n",
    "state = game.get_initial_state()\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "while True:\n",
    "    print(state)\n",
    "\n",
    "    if player == 1:\n",
    "        action = a1.get_action(state)\n",
    "    else:\n",
    "        action = a2.get_action(state)\n",
    "\n",
    "    state = game.get_next_state(state, action, player)\n",
    "\n",
    "    value, is_terminal = game.get_value_and_terminated(state, action)\n",
    "\n",
    "    if is_terminal:\n",
    "        print(state)\n",
    "        if value == 1:\n",
    "            print('Agent 1' if player == 1 else 'Agent 2', 'won')\n",
    "        else:\n",
    "            print('Draw')\n",
    "        break\n",
    "\n",
    "    player = game.get_opponent(player)\n",
    "\n",
    "print(time.time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b50cae-75ea-473a-b96f-a1a5c7901fd2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7e4eb825-e431-4ea0-998f-d1da412b5daf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe srcdoc=\"<!--\n",
       "  Copyright 2020 Kaggle Inc\n",
       "\n",
       "  Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);\n",
       "  you may not use this file except in compliance with the License.\n",
       "  You may obtain a copy of the License at\n",
       "\n",
       "      http://www.apache.org/licenses/LICENSE-2.0\n",
       "\n",
       "  Unless required by applicable law or agreed to in writing, software\n",
       "  distributed under the License is distributed on an &quot;AS IS&quot; BASIS,\n",
       "  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
       "  See the License for the specific language governing permissions and\n",
       "  limitations under the License.\n",
       "-->\n",
       "<!DOCTYPE html>\n",
       "<html lang=&quot;en&quot;>\n",
       "  <head>\n",
       "    <title>Kaggle Simulation Player</title>\n",
       "    <meta name=&quot;viewport&quot; content=&quot;width=device-width,initial-scale=1&quot; />\n",
       "    <link\n",
       "      rel=&quot;stylesheet&quot;\n",
       "      href=&quot;https://cdnjs.cloudflare.com/ajax/libs/meyer-reset/2.0/reset.css&quot;\n",
       "      crossorigin=&quot;anonymous&quot;\n",
       "    />\n",
       "    <style type=&quot;text/css&quot;>\n",
       "      html,\n",
       "      body {\n",
       "        height: 100%;\n",
       "        font-family: sans-serif;\n",
       "        margin: 0px;\n",
       "      }\n",
       "      canvas {\n",
       "        /* image-rendering: -moz-crisp-edges;\n",
       "        image-rendering: -webkit-crisp-edges;\n",
       "        image-rendering: pixelated;\n",
       "        image-rendering: crisp-edges; */\n",
       "      }\n",
       "    </style>\n",
       "    <script src=&quot;https://unpkg.com/preact@10.0.1/dist/preact.umd.js&quot;></script>\n",
       "    <script src=&quot;https://unpkg.com/preact@10.0.1/hooks/dist/hooks.umd.js&quot;></script>\n",
       "    <script src=&quot;https://unpkg.com/htm@2.2.1/dist/htm.umd.js&quot;></script>\n",
       "    <script>\n",
       "      // Polyfill for Styled Components\n",
       "      window.React = {\n",
       "        ...preact,\n",
       "        createElement: preact.h,\n",
       "        PropTypes: { func: {} },\n",
       "      };\n",
       "    </script>\n",
       "    <script src=&quot;https://unpkg.com/styled-components@3.5.0-0/dist/styled-components.min.js&quot;></script>\n",
       "  </head>\n",
       "  <body>\n",
       "    <script>\n",
       "      \n",
       "window.kaggle = {\n",
       "  &quot;debug&quot;: false,\n",
       "  &quot;playing&quot;: true,\n",
       "  &quot;step&quot;: 0,\n",
       "  &quot;controls&quot;: true,\n",
       "  &quot;environment&quot;: {\n",
       "    &quot;id&quot;: &quot;708c2b2e-93f8-11ee-bd95-8e9599bc5980&quot;,\n",
       "    &quot;name&quot;: &quot;connectx&quot;,\n",
       "    &quot;title&quot;: &quot;ConnectX&quot;,\n",
       "    &quot;description&quot;: &quot;Classic Connect in a row but configurable.&quot;,\n",
       "    &quot;version&quot;: &quot;1.0.1&quot;,\n",
       "    &quot;configuration&quot;: {\n",
       "      &quot;episodeSteps&quot;: 1000,\n",
       "      &quot;actTimeout&quot;: 2,\n",
       "      &quot;runTimeout&quot;: 1200,\n",
       "      &quot;columns&quot;: 7,\n",
       "      &quot;rows&quot;: 6,\n",
       "      &quot;inarow&quot;: 4,\n",
       "      &quot;agentTimeout&quot;: 60,\n",
       "      &quot;timeout&quot;: 2\n",
       "    },\n",
       "    &quot;specification&quot;: {\n",
       "      &quot;action&quot;: {\n",
       "        &quot;description&quot;: &quot;Column to drop a checker onto the board.&quot;,\n",
       "        &quot;type&quot;: &quot;integer&quot;,\n",
       "        &quot;minimum&quot;: 0,\n",
       "        &quot;default&quot;: 0\n",
       "      },\n",
       "      &quot;agents&quot;: [\n",
       "        2\n",
       "      ],\n",
       "      &quot;configuration&quot;: {\n",
       "        &quot;episodeSteps&quot;: {\n",
       "          &quot;description&quot;: &quot;Maximum number of steps in the episode.&quot;,\n",
       "          &quot;type&quot;: &quot;integer&quot;,\n",
       "          &quot;minimum&quot;: 1,\n",
       "          &quot;default&quot;: 1000\n",
       "        },\n",
       "        &quot;actTimeout&quot;: {\n",
       "          &quot;description&quot;: &quot;Maximum runtime (seconds) to obtain an action from an agent.&quot;,\n",
       "          &quot;type&quot;: &quot;number&quot;,\n",
       "          &quot;minimum&quot;: 0,\n",
       "          &quot;default&quot;: 2\n",
       "        },\n",
       "        &quot;runTimeout&quot;: {\n",
       "          &quot;description&quot;: &quot;Maximum runtime (seconds) of an episode (not necessarily DONE).&quot;,\n",
       "          &quot;type&quot;: &quot;number&quot;,\n",
       "          &quot;minimum&quot;: 0,\n",
       "          &quot;default&quot;: 1200\n",
       "        },\n",
       "        &quot;columns&quot;: {\n",
       "          &quot;description&quot;: &quot;The number of columns on the board&quot;,\n",
       "          &quot;type&quot;: &quot;integer&quot;,\n",
       "          &quot;default&quot;: 7,\n",
       "          &quot;minimum&quot;: 1\n",
       "        },\n",
       "        &quot;rows&quot;: {\n",
       "          &quot;description&quot;: &quot;The number of rows on the board&quot;,\n",
       "          &quot;type&quot;: &quot;integer&quot;,\n",
       "          &quot;default&quot;: 6,\n",
       "          &quot;minimum&quot;: 1\n",
       "        },\n",
       "        &quot;inarow&quot;: {\n",
       "          &quot;description&quot;: &quot;The number of checkers in a row required to win.&quot;,\n",
       "          &quot;type&quot;: &quot;integer&quot;,\n",
       "          &quot;default&quot;: 4,\n",
       "          &quot;minimum&quot;: 1\n",
       "        },\n",
       "        &quot;agentTimeout&quot;: {\n",
       "          &quot;description&quot;: &quot;Obsolete field kept for backwards compatibility, please use observation.remainingOverageTime.&quot;,\n",
       "          &quot;type&quot;: &quot;number&quot;,\n",
       "          &quot;minimum&quot;: 0,\n",
       "          &quot;default&quot;: 60\n",
       "        },\n",
       "        &quot;timeout&quot;: {\n",
       "          &quot;description&quot;: &quot;Obsolete copy of actTimeout maintained for backwards compatibility. May be removed in the future.&quot;,\n",
       "          &quot;type&quot;: &quot;integer&quot;,\n",
       "          &quot;default&quot;: 2,\n",
       "          &quot;minimum&quot;: 0\n",
       "        }\n",
       "      },\n",
       "      &quot;info&quot;: {},\n",
       "      &quot;observation&quot;: {\n",
       "        &quot;remainingOverageTime&quot;: {\n",
       "          &quot;description&quot;: &quot;Total remaining banked time (seconds) that can be used in excess of per-step actTimeouts -- agent is disqualified with TIMEOUT status when this drops below 0.&quot;,\n",
       "          &quot;shared&quot;: false,\n",
       "          &quot;type&quot;: &quot;number&quot;,\n",
       "          &quot;minimum&quot;: 0,\n",
       "          &quot;default&quot;: 60\n",
       "        },\n",
       "        &quot;step&quot;: {\n",
       "          &quot;description&quot;: &quot;Current step within the episode.&quot;,\n",
       "          &quot;type&quot;: &quot;integer&quot;,\n",
       "          &quot;shared&quot;: true,\n",
       "          &quot;minimum&quot;: 0,\n",
       "          &quot;default&quot;: 0\n",
       "        },\n",
       "        &quot;board&quot;: {\n",
       "          &quot;description&quot;: &quot;Serialized grid (rows x columns). 0 = Empty, 1 = P1, 2 = P2&quot;,\n",
       "          &quot;type&quot;: &quot;array&quot;,\n",
       "          &quot;shared&quot;: true,\n",
       "          &quot;default&quot;: []\n",
       "        },\n",
       "        &quot;mark&quot;: {\n",
       "          &quot;defaults&quot;: [\n",
       "            1,\n",
       "            2\n",
       "          ],\n",
       "          &quot;description&quot;: &quot;Which checkers are the agents.&quot;,\n",
       "          &quot;enum&quot;: [\n",
       "            1,\n",
       "            2\n",
       "          ]\n",
       "        }\n",
       "      },\n",
       "      &quot;reward&quot;: {\n",
       "        &quot;description&quot;: &quot;-1 = Lost, 0 = Draw/Ongoing, 1 = Won&quot;,\n",
       "        &quot;enum&quot;: [\n",
       "          -1,\n",
       "          0,\n",
       "          1\n",
       "        ],\n",
       "        &quot;default&quot;: 0,\n",
       "        &quot;type&quot;: [\n",
       "          &quot;number&quot;,\n",
       "          &quot;null&quot;\n",
       "        ]\n",
       "      }\n",
       "    },\n",
       "    &quot;steps&quot;: [\n",
       "      [\n",
       "        {\n",
       "          &quot;action&quot;: 0,\n",
       "          &quot;reward&quot;: 0,\n",
       "          &quot;info&quot;: {},\n",
       "          &quot;observation&quot;: {\n",
       "            &quot;remainingOverageTime&quot;: 60,\n",
       "            &quot;step&quot;: 0,\n",
       "            &quot;board&quot;: [\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0\n",
       "            ],\n",
       "            &quot;mark&quot;: 1\n",
       "          },\n",
       "          &quot;status&quot;: &quot;ACTIVE&quot;\n",
       "        },\n",
       "        {\n",
       "          &quot;action&quot;: 0,\n",
       "          &quot;reward&quot;: 0,\n",
       "          &quot;info&quot;: {},\n",
       "          &quot;observation&quot;: {\n",
       "            &quot;remainingOverageTime&quot;: 60,\n",
       "            &quot;mark&quot;: 2\n",
       "          },\n",
       "          &quot;status&quot;: &quot;INACTIVE&quot;\n",
       "        }\n",
       "      ],\n",
       "      [\n",
       "        {\n",
       "          &quot;action&quot;: 3,\n",
       "          &quot;reward&quot;: 0,\n",
       "          &quot;info&quot;: {},\n",
       "          &quot;observation&quot;: {\n",
       "            &quot;remainingOverageTime&quot;: 56.199872,\n",
       "            &quot;step&quot;: 1,\n",
       "            &quot;board&quot;: [\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              1,\n",
       "              0,\n",
       "              0,\n",
       "              0\n",
       "            ],\n",
       "            &quot;mark&quot;: 1\n",
       "          },\n",
       "          &quot;status&quot;: &quot;INACTIVE&quot;\n",
       "        },\n",
       "        {\n",
       "          &quot;action&quot;: 0,\n",
       "          &quot;reward&quot;: 0,\n",
       "          &quot;info&quot;: {},\n",
       "          &quot;observation&quot;: {\n",
       "            &quot;remainingOverageTime&quot;: 60,\n",
       "            &quot;mark&quot;: 2\n",
       "          },\n",
       "          &quot;status&quot;: &quot;ACTIVE&quot;\n",
       "        }\n",
       "      ],\n",
       "      [\n",
       "        {\n",
       "          &quot;action&quot;: 0,\n",
       "          &quot;reward&quot;: 0,\n",
       "          &quot;info&quot;: {},\n",
       "          &quot;observation&quot;: {\n",
       "            &quot;remainingOverageTime&quot;: 56.199872,\n",
       "            &quot;step&quot;: 2,\n",
       "            &quot;board&quot;: [\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              2,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              1,\n",
       "              0,\n",
       "              0,\n",
       "              0\n",
       "            ],\n",
       "            &quot;mark&quot;: 1\n",
       "          },\n",
       "          &quot;status&quot;: &quot;ACTIVE&quot;\n",
       "        },\n",
       "        {\n",
       "          &quot;action&quot;: 3,\n",
       "          &quot;reward&quot;: 0,\n",
       "          &quot;info&quot;: {},\n",
       "          &quot;observation&quot;: {\n",
       "            &quot;remainingOverageTime&quot;: 55.889753999999996,\n",
       "            &quot;mark&quot;: 2\n",
       "          },\n",
       "          &quot;status&quot;: &quot;INACTIVE&quot;\n",
       "        }\n",
       "      ],\n",
       "      [\n",
       "        {\n",
       "          &quot;action&quot;: 2,\n",
       "          &quot;reward&quot;: 0,\n",
       "          &quot;info&quot;: {},\n",
       "          &quot;observation&quot;: {\n",
       "            &quot;remainingOverageTime&quot;: 51.302399,\n",
       "            &quot;step&quot;: 3,\n",
       "            &quot;board&quot;: [\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              2,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              1,\n",
       "              1,\n",
       "              0,\n",
       "              0,\n",
       "              0\n",
       "            ],\n",
       "            &quot;mark&quot;: 1\n",
       "          },\n",
       "          &quot;status&quot;: &quot;INACTIVE&quot;\n",
       "        },\n",
       "        {\n",
       "          &quot;action&quot;: 0,\n",
       "          &quot;reward&quot;: 0,\n",
       "          &quot;info&quot;: {},\n",
       "          &quot;observation&quot;: {\n",
       "            &quot;remainingOverageTime&quot;: 55.889753999999996,\n",
       "            &quot;mark&quot;: 2\n",
       "          },\n",
       "          &quot;status&quot;: &quot;ACTIVE&quot;\n",
       "        }\n",
       "      ],\n",
       "      [\n",
       "        {\n",
       "          &quot;action&quot;: 0,\n",
       "          &quot;reward&quot;: 0,\n",
       "          &quot;info&quot;: {},\n",
       "          &quot;observation&quot;: {\n",
       "            &quot;remainingOverageTime&quot;: 51.302399,\n",
       "            &quot;step&quot;: 4,\n",
       "            &quot;board&quot;: [\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              2,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              1,\n",
       "              1,\n",
       "              2,\n",
       "              0,\n",
       "              0\n",
       "            ],\n",
       "            &quot;mark&quot;: 1\n",
       "          },\n",
       "          &quot;status&quot;: &quot;ACTIVE&quot;\n",
       "        },\n",
       "        {\n",
       "          &quot;action&quot;: 4,\n",
       "          &quot;reward&quot;: 0,\n",
       "          &quot;info&quot;: {},\n",
       "          &quot;observation&quot;: {\n",
       "            &quot;remainingOverageTime&quot;: 51.53885399999999,\n",
       "            &quot;mark&quot;: 2\n",
       "          },\n",
       "          &quot;status&quot;: &quot;INACTIVE&quot;\n",
       "        }\n",
       "      ],\n",
       "      [\n",
       "        {\n",
       "          &quot;action&quot;: 1,\n",
       "          &quot;reward&quot;: 0,\n",
       "          &quot;info&quot;: {},\n",
       "          &quot;observation&quot;: {\n",
       "            &quot;remainingOverageTime&quot;: 47.350442,\n",
       "            &quot;step&quot;: 5,\n",
       "            &quot;board&quot;: [\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              2,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              2,\n",
       "              0,\n",
       "              0\n",
       "            ],\n",
       "            &quot;mark&quot;: 1\n",
       "          },\n",
       "          &quot;status&quot;: &quot;INACTIVE&quot;\n",
       "        },\n",
       "        {\n",
       "          &quot;action&quot;: 0,\n",
       "          &quot;reward&quot;: 0,\n",
       "          &quot;info&quot;: {},\n",
       "          &quot;observation&quot;: {\n",
       "            &quot;remainingOverageTime&quot;: 51.53885399999999,\n",
       "            &quot;mark&quot;: 2\n",
       "          },\n",
       "          &quot;status&quot;: &quot;ACTIVE&quot;\n",
       "        }\n",
       "      ],\n",
       "      [\n",
       "        {\n",
       "          &quot;action&quot;: 0,\n",
       "          &quot;reward&quot;: 0,\n",
       "          &quot;info&quot;: {},\n",
       "          &quot;observation&quot;: {\n",
       "            &quot;remainingOverageTime&quot;: 47.350442,\n",
       "            &quot;step&quot;: 6,\n",
       "            &quot;board&quot;: [\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              2,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              2,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              2,\n",
       "              0,\n",
       "              0\n",
       "            ],\n",
       "            &quot;mark&quot;: 1\n",
       "          },\n",
       "          &quot;status&quot;: &quot;ACTIVE&quot;\n",
       "        },\n",
       "        {\n",
       "          &quot;action&quot;: 0,\n",
       "          &quot;reward&quot;: 0,\n",
       "          &quot;info&quot;: {},\n",
       "          &quot;observation&quot;: {\n",
       "            &quot;remainingOverageTime&quot;: 49.096157999999996,\n",
       "            &quot;mark&quot;: 2\n",
       "          },\n",
       "          &quot;status&quot;: &quot;INACTIVE&quot;\n",
       "        }\n",
       "      ],\n",
       "      [\n",
       "        {\n",
       "          &quot;action&quot;: 4,\n",
       "          &quot;reward&quot;: 0,\n",
       "          &quot;info&quot;: {},\n",
       "          &quot;observation&quot;: {\n",
       "            &quot;remainingOverageTime&quot;: 43.078485,\n",
       "            &quot;step&quot;: 7,\n",
       "            &quot;board&quot;: [\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              2,\n",
       "              1,\n",
       "              0,\n",
       "              0,\n",
       "              2,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              2,\n",
       "              0,\n",
       "              0\n",
       "            ],\n",
       "            &quot;mark&quot;: 1\n",
       "          },\n",
       "          &quot;status&quot;: &quot;INACTIVE&quot;\n",
       "        },\n",
       "        {\n",
       "          &quot;action&quot;: 0,\n",
       "          &quot;reward&quot;: 0,\n",
       "          &quot;info&quot;: {},\n",
       "          &quot;observation&quot;: {\n",
       "            &quot;remainingOverageTime&quot;: 49.096157999999996,\n",
       "            &quot;mark&quot;: 2\n",
       "          },\n",
       "          &quot;status&quot;: &quot;ACTIVE&quot;\n",
       "        }\n",
       "      ],\n",
       "      [\n",
       "        {\n",
       "          &quot;action&quot;: 0,\n",
       "          &quot;reward&quot;: 0,\n",
       "          &quot;info&quot;: {},\n",
       "          &quot;observation&quot;: {\n",
       "            &quot;remainingOverageTime&quot;: 43.078485,\n",
       "            &quot;step&quot;: 8,\n",
       "            &quot;board&quot;: [\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              2,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              2,\n",
       "              1,\n",
       "              0,\n",
       "              0,\n",
       "              2,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              2,\n",
       "              0,\n",
       "              0\n",
       "            ],\n",
       "            &quot;mark&quot;: 1\n",
       "          },\n",
       "          &quot;status&quot;: &quot;ACTIVE&quot;\n",
       "        },\n",
       "        {\n",
       "          &quot;action&quot;: 4,\n",
       "          &quot;reward&quot;: 0,\n",
       "          &quot;info&quot;: {},\n",
       "          &quot;observation&quot;: {\n",
       "            &quot;remainingOverageTime&quot;: 44.744150999999995,\n",
       "            &quot;mark&quot;: 2\n",
       "          },\n",
       "          &quot;status&quot;: &quot;INACTIVE&quot;\n",
       "        }\n",
       "      ],\n",
       "      [\n",
       "        {\n",
       "          &quot;action&quot;: 2,\n",
       "          &quot;reward&quot;: 0,\n",
       "          &quot;info&quot;: {},\n",
       "          &quot;observation&quot;: {\n",
       "            &quot;remainingOverageTime&quot;: 38.495221,\n",
       "            &quot;step&quot;: 9,\n",
       "            &quot;board&quot;: [\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              2,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              1,\n",
       "              2,\n",
       "              1,\n",
       "              0,\n",
       "              0,\n",
       "              2,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              2,\n",
       "              0,\n",
       "              0\n",
       "            ],\n",
       "            &quot;mark&quot;: 1\n",
       "          },\n",
       "          &quot;status&quot;: &quot;INACTIVE&quot;\n",
       "        },\n",
       "        {\n",
       "          &quot;action&quot;: 0,\n",
       "          &quot;reward&quot;: 0,\n",
       "          &quot;info&quot;: {},\n",
       "          &quot;observation&quot;: {\n",
       "            &quot;remainingOverageTime&quot;: 44.744150999999995,\n",
       "            &quot;mark&quot;: 2\n",
       "          },\n",
       "          &quot;status&quot;: &quot;ACTIVE&quot;\n",
       "        }\n",
       "      ],\n",
       "      [\n",
       "        {\n",
       "          &quot;action&quot;: 0,\n",
       "          &quot;reward&quot;: 0,\n",
       "          &quot;info&quot;: {},\n",
       "          &quot;observation&quot;: {\n",
       "            &quot;remainingOverageTime&quot;: 38.495221,\n",
       "            &quot;step&quot;: 10,\n",
       "            &quot;board&quot;: [\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              2,\n",
       "              2,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              1,\n",
       "              2,\n",
       "              1,\n",
       "              0,\n",
       "              0,\n",
       "              2,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              2,\n",
       "              0,\n",
       "              0\n",
       "            ],\n",
       "            &quot;mark&quot;: 1\n",
       "          },\n",
       "          &quot;status&quot;: &quot;ACTIVE&quot;\n",
       "        },\n",
       "        {\n",
       "          &quot;action&quot;: 3,\n",
       "          &quot;reward&quot;: 0,\n",
       "          &quot;info&quot;: {},\n",
       "          &quot;observation&quot;: {\n",
       "            &quot;remainingOverageTime&quot;: 40.484562999999994,\n",
       "            &quot;mark&quot;: 2\n",
       "          },\n",
       "          &quot;status&quot;: &quot;INACTIVE&quot;\n",
       "        }\n",
       "      ],\n",
       "      [\n",
       "        {\n",
       "          &quot;action&quot;: 3,\n",
       "          &quot;reward&quot;: 0,\n",
       "          &quot;info&quot;: {},\n",
       "          &quot;observation&quot;: {\n",
       "            &quot;remainingOverageTime&quot;: 36.065085,\n",
       "            &quot;step&quot;: 11,\n",
       "            &quot;board&quot;: [\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              1,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              2,\n",
       "              2,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              1,\n",
       "              2,\n",
       "              1,\n",
       "              0,\n",
       "              0,\n",
       "              2,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              2,\n",
       "              0,\n",
       "              0\n",
       "            ],\n",
       "            &quot;mark&quot;: 1\n",
       "          },\n",
       "          &quot;status&quot;: &quot;INACTIVE&quot;\n",
       "        },\n",
       "        {\n",
       "          &quot;action&quot;: 0,\n",
       "          &quot;reward&quot;: 0,\n",
       "          &quot;info&quot;: {},\n",
       "          &quot;observation&quot;: {\n",
       "            &quot;remainingOverageTime&quot;: 40.484562999999994,\n",
       "            &quot;mark&quot;: 2\n",
       "          },\n",
       "          &quot;status&quot;: &quot;ACTIVE&quot;\n",
       "        }\n",
       "      ],\n",
       "      [\n",
       "        {\n",
       "          &quot;action&quot;: 0,\n",
       "          &quot;reward&quot;: 0,\n",
       "          &quot;info&quot;: {},\n",
       "          &quot;observation&quot;: {\n",
       "            &quot;remainingOverageTime&quot;: 36.065085,\n",
       "            &quot;step&quot;: 12,\n",
       "            &quot;board&quot;: [\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              1,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              2,\n",
       "              2,\n",
       "              2,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              1,\n",
       "              2,\n",
       "              1,\n",
       "              0,\n",
       "              0,\n",
       "              2,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              2,\n",
       "              0,\n",
       "              0\n",
       "            ],\n",
       "            &quot;mark&quot;: 1\n",
       "          },\n",
       "          &quot;status&quot;: &quot;ACTIVE&quot;\n",
       "        },\n",
       "        {\n",
       "          &quot;action&quot;: 2,\n",
       "          &quot;reward&quot;: 0,\n",
       "          &quot;info&quot;: {},\n",
       "          &quot;observation&quot;: {\n",
       "            &quot;remainingOverageTime&quot;: 38.990185999999994,\n",
       "            &quot;mark&quot;: 2\n",
       "          },\n",
       "          &quot;status&quot;: &quot;INACTIVE&quot;\n",
       "        }\n",
       "      ],\n",
       "      [\n",
       "        {\n",
       "          &quot;action&quot;: 2,\n",
       "          &quot;reward&quot;: 0,\n",
       "          &quot;info&quot;: {},\n",
       "          &quot;observation&quot;: {\n",
       "            &quot;remainingOverageTime&quot;: 35.46951,\n",
       "            &quot;step&quot;: 13,\n",
       "            &quot;board&quot;: [\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              1,\n",
       "              1,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              2,\n",
       "              2,\n",
       "              2,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              1,\n",
       "              2,\n",
       "              1,\n",
       "              0,\n",
       "              0,\n",
       "              2,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              2,\n",
       "              0,\n",
       "              0\n",
       "            ],\n",
       "            &quot;mark&quot;: 1\n",
       "          },\n",
       "          &quot;status&quot;: &quot;INACTIVE&quot;\n",
       "        },\n",
       "        {\n",
       "          &quot;action&quot;: 0,\n",
       "          &quot;reward&quot;: 0,\n",
       "          &quot;info&quot;: {},\n",
       "          &quot;observation&quot;: {\n",
       "            &quot;remainingOverageTime&quot;: 38.990185999999994,\n",
       "            &quot;mark&quot;: 2\n",
       "          },\n",
       "          &quot;status&quot;: &quot;ACTIVE&quot;\n",
       "        }\n",
       "      ],\n",
       "      [\n",
       "        {\n",
       "          &quot;action&quot;: 0,\n",
       "          &quot;reward&quot;: 0,\n",
       "          &quot;info&quot;: {},\n",
       "          &quot;observation&quot;: {\n",
       "            &quot;remainingOverageTime&quot;: 35.46951,\n",
       "            &quot;step&quot;: 14,\n",
       "            &quot;board&quot;: [\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              1,\n",
       "              1,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              2,\n",
       "              2,\n",
       "              2,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              2,\n",
       "              1,\n",
       "              2,\n",
       "              1,\n",
       "              0,\n",
       "              0,\n",
       "              2,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              2,\n",
       "              0,\n",
       "              0\n",
       "            ],\n",
       "            &quot;mark&quot;: 1\n",
       "          },\n",
       "          &quot;status&quot;: &quot;ACTIVE&quot;\n",
       "        },\n",
       "        {\n",
       "          &quot;action&quot;: 1,\n",
       "          &quot;reward&quot;: 0,\n",
       "          &quot;info&quot;: {},\n",
       "          &quot;observation&quot;: {\n",
       "            &quot;remainingOverageTime&quot;: 38.990185999999994,\n",
       "            &quot;mark&quot;: 2\n",
       "          },\n",
       "          &quot;status&quot;: &quot;INACTIVE&quot;\n",
       "        }\n",
       "      ],\n",
       "      [\n",
       "        {\n",
       "          &quot;action&quot;: 1,\n",
       "          &quot;reward&quot;: 0,\n",
       "          &quot;info&quot;: {},\n",
       "          &quot;observation&quot;: {\n",
       "            &quot;remainingOverageTime&quot;: 35.46951,\n",
       "            &quot;step&quot;: 15,\n",
       "            &quot;board&quot;: [\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              1,\n",
       "              1,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              1,\n",
       "              2,\n",
       "              2,\n",
       "              2,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              2,\n",
       "              1,\n",
       "              2,\n",
       "              1,\n",
       "              0,\n",
       "              0,\n",
       "              2,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              2,\n",
       "              0,\n",
       "              0\n",
       "            ],\n",
       "            &quot;mark&quot;: 1\n",
       "          },\n",
       "          &quot;status&quot;: &quot;INACTIVE&quot;\n",
       "        },\n",
       "        {\n",
       "          &quot;action&quot;: 0,\n",
       "          &quot;reward&quot;: 0,\n",
       "          &quot;info&quot;: {},\n",
       "          &quot;observation&quot;: {\n",
       "            &quot;remainingOverageTime&quot;: 38.990185999999994,\n",
       "            &quot;mark&quot;: 2\n",
       "          },\n",
       "          &quot;status&quot;: &quot;ACTIVE&quot;\n",
       "        }\n",
       "      ],\n",
       "      [\n",
       "        {\n",
       "          &quot;action&quot;: 0,\n",
       "          &quot;reward&quot;: -1,\n",
       "          &quot;info&quot;: {},\n",
       "          &quot;observation&quot;: {\n",
       "            &quot;remainingOverageTime&quot;: 35.46951,\n",
       "            &quot;step&quot;: 16,\n",
       "            &quot;board&quot;: [\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              2,\n",
       "              1,\n",
       "              1,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              1,\n",
       "              2,\n",
       "              2,\n",
       "              2,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              2,\n",
       "              1,\n",
       "              2,\n",
       "              1,\n",
       "              0,\n",
       "              0,\n",
       "              2,\n",
       "              1,\n",
       "              1,\n",
       "              1,\n",
       "              2,\n",
       "              0,\n",
       "              0\n",
       "            ],\n",
       "            &quot;mark&quot;: 1\n",
       "          },\n",
       "          &quot;status&quot;: &quot;DONE&quot;\n",
       "        },\n",
       "        {\n",
       "          &quot;action&quot;: 1,\n",
       "          &quot;reward&quot;: 1,\n",
       "          &quot;info&quot;: {},\n",
       "          &quot;observation&quot;: {\n",
       "            &quot;remainingOverageTime&quot;: 38.990185999999994,\n",
       "            &quot;mark&quot;: 2\n",
       "          },\n",
       "          &quot;status&quot;: &quot;DONE&quot;\n",
       "        }\n",
       "      ]\n",
       "    ],\n",
       "    &quot;rewards&quot;: [\n",
       "      -1,\n",
       "      1\n",
       "    ],\n",
       "    &quot;statuses&quot;: [\n",
       "      &quot;DONE&quot;,\n",
       "      &quot;DONE&quot;\n",
       "    ],\n",
       "    &quot;schema_version&quot;: 1,\n",
       "    &quot;info&quot;: {}\n",
       "  },\n",
       "  &quot;logs&quot;: [\n",
       "    [],\n",
       "    [],\n",
       "    [\n",
       "      {\n",
       "        &quot;duration&quot;: 5.800128,\n",
       "        &quot;stdout&quot;: &quot;&quot;,\n",
       "        &quot;stderr&quot;: &quot;&quot;\n",
       "      },\n",
       "      {}\n",
       "    ],\n",
       "    [\n",
       "      {},\n",
       "      {\n",
       "        &quot;duration&quot;: 6.110246,\n",
       "        &quot;stdout&quot;: &quot;&quot;,\n",
       "        &quot;stderr&quot;: &quot;&quot;\n",
       "      }\n",
       "    ],\n",
       "    [\n",
       "      {\n",
       "        &quot;duration&quot;: 6.897473,\n",
       "        &quot;stdout&quot;: &quot;&quot;,\n",
       "        &quot;stderr&quot;: &quot;&quot;\n",
       "      },\n",
       "      {}\n",
       "    ],\n",
       "    [\n",
       "      {},\n",
       "      {\n",
       "        &quot;duration&quot;: 6.3509,\n",
       "        &quot;stdout&quot;: &quot;&quot;,\n",
       "        &quot;stderr&quot;: &quot;&quot;\n",
       "      }\n",
       "    ],\n",
       "    [\n",
       "      {\n",
       "        &quot;duration&quot;: 5.951957,\n",
       "        &quot;stdout&quot;: &quot;&quot;,\n",
       "        &quot;stderr&quot;: &quot;&quot;\n",
       "      },\n",
       "      {}\n",
       "    ],\n",
       "    [\n",
       "      {},\n",
       "      {\n",
       "        &quot;duration&quot;: 4.442696,\n",
       "        &quot;stdout&quot;: &quot;&quot;,\n",
       "        &quot;stderr&quot;: &quot;&quot;\n",
       "      }\n",
       "    ],\n",
       "    [\n",
       "      {\n",
       "        &quot;duration&quot;: 6.271957,\n",
       "        &quot;stdout&quot;: &quot;&quot;,\n",
       "        &quot;stderr&quot;: &quot;&quot;\n",
       "      },\n",
       "      {}\n",
       "    ],\n",
       "    [\n",
       "      {},\n",
       "      {\n",
       "        &quot;duration&quot;: 6.352007,\n",
       "        &quot;stdout&quot;: &quot;&quot;,\n",
       "        &quot;stderr&quot;: &quot;&quot;\n",
       "      }\n",
       "    ],\n",
       "    [\n",
       "      {\n",
       "        &quot;duration&quot;: 6.583264,\n",
       "        &quot;stdout&quot;: &quot;&quot;,\n",
       "        &quot;stderr&quot;: &quot;&quot;\n",
       "      },\n",
       "      {}\n",
       "    ],\n",
       "    [\n",
       "      {},\n",
       "      {\n",
       "        &quot;duration&quot;: 6.259588,\n",
       "        &quot;stdout&quot;: &quot;&quot;,\n",
       "        &quot;stderr&quot;: &quot;&quot;\n",
       "      }\n",
       "    ],\n",
       "    [\n",
       "      {\n",
       "        &quot;duration&quot;: 4.430136,\n",
       "        &quot;stdout&quot;: &quot;&quot;,\n",
       "        &quot;stderr&quot;: &quot;&quot;\n",
       "      },\n",
       "      {}\n",
       "    ],\n",
       "    [\n",
       "      {},\n",
       "      {\n",
       "        &quot;duration&quot;: 3.494377,\n",
       "        &quot;stdout&quot;: &quot;&quot;,\n",
       "        &quot;stderr&quot;: &quot;&quot;\n",
       "      }\n",
       "    ],\n",
       "    [\n",
       "      {\n",
       "        &quot;duration&quot;: 2.595575,\n",
       "        &quot;stdout&quot;: &quot;&quot;,\n",
       "        &quot;stderr&quot;: &quot;&quot;\n",
       "      },\n",
       "      {}\n",
       "    ],\n",
       "    [\n",
       "      {},\n",
       "      {\n",
       "        &quot;duration&quot;: 1.173488,\n",
       "        &quot;stdout&quot;: &quot;&quot;,\n",
       "        &quot;stderr&quot;: &quot;&quot;\n",
       "      }\n",
       "    ],\n",
       "    [\n",
       "      {\n",
       "        &quot;duration&quot;: 0.682341,\n",
       "        &quot;stdout&quot;: &quot;&quot;,\n",
       "        &quot;stderr&quot;: &quot;&quot;\n",
       "      },\n",
       "      {}\n",
       "    ],\n",
       "    [\n",
       "      {},\n",
       "      {\n",
       "        &quot;duration&quot;: 0.525957,\n",
       "        &quot;stdout&quot;: &quot;&quot;,\n",
       "        &quot;stderr&quot;: &quot;&quot;\n",
       "      }\n",
       "    ]\n",
       "  ],\n",
       "  &quot;mode&quot;: &quot;ipython&quot;,\n",
       "  &quot;width&quot;: 500,\n",
       "  &quot;height&quot;: 500\n",
       "};\n",
       "\n",
       "\n",
       "window.kaggle.renderer = // Copyright 2020 Kaggle Inc\n",
       "//\n",
       "// Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);\n",
       "// you may not use this file except in compliance with the License.\n",
       "// You may obtain a copy of the License at\n",
       "//\n",
       "//      http://www.apache.org/licenses/LICENSE-2.0\n",
       "//\n",
       "// Unless required by applicable law or agreed to in writing, software\n",
       "// distributed under the License is distributed on an &quot;AS IS&quot; BASIS,\n",
       "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
       "// See the License for the specific language governing permissions and\n",
       "// limitations under the License.\n",
       "\n",
       "function renderer({\n",
       "  act,\n",
       "  agents,\n",
       "  environment,\n",
       "  frame,\n",
       "  height = 400,\n",
       "  interactive,\n",
       "  isInteractive,\n",
       "  parent,\n",
       "  step,\n",
       "  update,\n",
       "  width = 400,\n",
       "}) {\n",
       "  // Configuration.\n",
       "  const { rows, columns, inarow } = environment.configuration;\n",
       "\n",
       "  // Common Dimensions.\n",
       "  const unit = 8;\n",
       "  const minCanvasSize = Math.min(height, width);\n",
       "  const minOffset = minCanvasSize > 400 ? 30 : unit / 2;\n",
       "  const cellSize = Math.min(\n",
       "    (width - minOffset * 2) / columns,\n",
       "    (height - minOffset * 2) / rows\n",
       "  );\n",
       "  const cellInset = 0.8;\n",
       "  const pieceScale = cellSize / 100;\n",
       "  const xOffset = Math.max(0, (width - cellSize * columns) / 2);\n",
       "  const yOffset = Math.max(0, (height - cellSize * rows) / 2);\n",
       "\n",
       "  // Canvas Setup.\n",
       "  let canvas = parent.querySelector(&quot;canvas&quot;);\n",
       "  if (!canvas) {\n",
       "    canvas = document.createElement(&quot;canvas&quot;);\n",
       "    parent.appendChild(canvas);\n",
       "\n",
       "    if (interactive) {\n",
       "      canvas.addEventListener(&quot;click&quot;, evt => {\n",
       "        if (!isInteractive()) return;\n",
       "        const rect = evt.target.getBoundingClientRect();\n",
       "        const col = Math.floor((evt.clientX - rect.left - xOffset) / cellSize);\n",
       "        if (col >= 0 && col < columns) act(col);\n",
       "      });\n",
       "    }\n",
       "  }\n",
       "  canvas.style.cursor = isInteractive() ? &quot;pointer&quot; : &quot;default&quot;;\n",
       "\n",
       "  // Character Paths (based on 100x100 tiles).\n",
       "  const kPath = new Path2D(\n",
       "    `M78.3,96.5c-0.1,0.4-0.5,0.6-1.1,0.6H64.9c-0.7,0-1.4-0.3-1.9-1l-20.3-26L37,75.5v20.1 c0,0.9-0.5,1.4-1.4,1.4H26c-0.9,0-1.4-0.5-1.4-1.4V3.9c0-0.9,0.5-1.4,1.4-1.4h9.5C36.5,2.5,37,3,37,3.9v56.5l24.3-24.7 c0.6-0.6,1.3-1,1.9-1H76c0.6,0,0.9,0.2,1.1,0.7c0.2,0.6,0.1,1-0.1,1.2l-25.7,25L78,95.1C78.4,95.5,78.5,95.9,78.3,96.5z`\n",
       "  );\n",
       "  const goose1Path = new Path2D(\n",
       "    `M8.8,92.7c-4-18.5,4.7-37.2,20.7-46.2c0,0,2.7-1.4,3.4-1.9c2.2-1.6,3-2.1,3-5c0-5-2.1-7.2-2.1-7.2 c-3.9-3.3-6.3-8.2-6.3-13.7c0-10,8.1-18.1,18.1-18.1s18.1,8.1,18.1,18.1c0,6-1.5,32.7-2.3,38.8l-0.1,1`\n",
       "  );\n",
       "  const goose2Path = new Path2D(\n",
       "    `M27.4,19L8.2,27.6c0,0-7.3,2.9,2.6,5c6.1,1.3,24,5.9,24,5.9l1,0.3`\n",
       "  );\n",
       "  const goose3Path = new Path2D(\n",
       "    `M63.7,99.6C52.3,99.6,43,90.3,43,78.9s9.3-20.7,20.7-20.7c10.6,0,34.4,0.1,35.8,9`\n",
       "  );\n",
       "\n",
       "  // Canvas setup and reset.\n",
       "  let c = canvas.getContext(&quot;2d&quot;);\n",
       "  canvas.width = width;\n",
       "  canvas.height = height;\n",
       "  c.fillStyle = &quot;#000B2A&quot;;\n",
       "  c.fillRect(0, 0, canvas.width, canvas.height);\n",
       "\n",
       "  const getRowCol = cell => [Math.floor(cell / columns), cell % columns];\n",
       "\n",
       "  const getColor = (mark, opacity = 1) => {\n",
       "    if (mark === 1) return `rgba(0,255,255,${opacity})`;\n",
       "    if (mark === 2) return `rgba(255,255,255,${opacity})`;\n",
       "    return &quot;#fff&quot;;\n",
       "  };\n",
       "\n",
       "  const drawCellCircle = (cell, xFrame = 1, yFrame = 1, radiusOffset = 0) => {\n",
       "    const [row, col] = getRowCol(cell);\n",
       "    c.arc(\n",
       "      xOffset + xFrame * (col * cellSize + cellSize / 2),\n",
       "      yOffset + yFrame * (row * cellSize + cellSize / 2),\n",
       "      (cellInset * cellSize) / 2 - radiusOffset,\n",
       "      2 * Math.PI,\n",
       "      false\n",
       "    );\n",
       "  };\n",
       "\n",
       "  // Render the pieces.\n",
       "  const board = environment.steps[step][0].observation.board;\n",
       "\n",
       "  const drawPiece = mark => {\n",
       "    // Base Styles.\n",
       "    const opacity = minCanvasSize < 300 ? 0.6 - minCanvasSize / 1000 : 0.1;\n",
       "    c.fillStyle = getColor(mark, opacity);\n",
       "    c.strokeStyle = getColor(mark);\n",
       "    c.shadowColor = getColor(mark);\n",
       "    c.shadowBlur = 8 / cellInset;\n",
       "    c.lineWidth = 1 / cellInset;\n",
       "\n",
       "    // Outer circle.\n",
       "    c.save();\n",
       "    c.beginPath();\n",
       "    c.arc(50, 50, 50, 2 * Math.PI, false);\n",
       "    c.closePath();\n",
       "    c.lineWidth *= 4;\n",
       "    c.stroke();\n",
       "    c.fill();\n",
       "    c.restore();\n",
       "\n",
       "    // Inner circle.\n",
       "    c.beginPath();\n",
       "    c.arc(50, 50, 40, 2 * Math.PI, false);\n",
       "    c.closePath();\n",
       "    c.stroke();\n",
       "\n",
       "    // Kaggle &quot;K&quot;.\n",
       "    if (mark === 1) {\n",
       "      const scale = 0.54;\n",
       "      c.save();\n",
       "      c.translate(23, 23);\n",
       "      c.scale(scale, scale);\n",
       "      c.lineWidth /= scale;\n",
       "      c.shadowBlur /= scale;\n",
       "      c.stroke(kPath);\n",
       "      c.restore();\n",
       "    }\n",
       "\n",
       "    // Kaggle &quot;Goose&quot;.\n",
       "    if (mark === 2) {\n",
       "      const scale = 0.6;\n",
       "      c.save();\n",
       "      c.translate(24, 28);\n",
       "      c.scale(scale, scale);\n",
       "      c.lineWidth /= scale;\n",
       "      c.shadowBlur /= scale;\n",
       "      c.stroke(goose1Path);\n",
       "      c.stroke(goose2Path);\n",
       "      c.stroke(goose3Path);\n",
       "      c.beginPath();\n",
       "      c.arc(38.5, 18.6, 2.7, 0, Math.PI * 2, false);\n",
       "      c.closePath();\n",
       "      c.fill();\n",
       "      c.restore();\n",
       "    }\n",
       "  };\n",
       "\n",
       "  for (let i = 0; i < board.length; i++) {\n",
       "    const [row, col] = getRowCol(i);\n",
       "    if (board[i] === 0) continue;\n",
       "    // Easing In.\n",
       "    let yFrame = Math.min(\n",
       "      (columns * Math.pow(frame, 3)) / Math.floor(i / columns),\n",
       "      1\n",
       "    );\n",
       "\n",
       "    if (\n",
       "      step > 1 &&\n",
       "      environment.steps[step - 1][0].observation.board[i] === board[i]\n",
       "    ) {\n",
       "      yFrame = 1;\n",
       "    }\n",
       "\n",
       "    c.save();\n",
       "    c.translate(\n",
       "      xOffset + cellSize * col + (cellSize - cellSize * cellInset) / 2,\n",
       "      yOffset +\n",
       "        yFrame * (cellSize * row) +\n",
       "        (cellSize - cellSize * cellInset) / 2\n",
       "    );\n",
       "    c.scale(pieceScale * cellInset, pieceScale * cellInset);\n",
       "    drawPiece(board[i]);\n",
       "    c.restore();\n",
       "  }\n",
       "\n",
       "  // Background Gradient.\n",
       "  const bgRadius = (Math.min(rows, columns) * cellSize) / 2;\n",
       "  const bgStyle = c.createRadialGradient(\n",
       "    xOffset + (cellSize * columns) / 2,\n",
       "    yOffset + (cellSize * rows) / 2,\n",
       "    0,\n",
       "    xOffset + (cellSize * columns) / 2,\n",
       "    yOffset + (cellSize * rows) / 2,\n",
       "    bgRadius\n",
       "  );\n",
       "  bgStyle.addColorStop(0, &quot;#000B49&quot;);\n",
       "  bgStyle.addColorStop(1, &quot;#000B2A&quot;);\n",
       "\n",
       "  // Render the board overlay.\n",
       "  c.beginPath();\n",
       "  c.rect(0, 0, canvas.width, canvas.height);\n",
       "  c.closePath();\n",
       "  c.shadowBlur = 0;\n",
       "  for (let i = 0; i < board.length; i++) {\n",
       "    drawCellCircle(i);\n",
       "    c.closePath();\n",
       "  }\n",
       "  c.fillStyle = bgStyle;\n",
       "  c.fill(&quot;evenodd&quot;);\n",
       "\n",
       "  // Render the board overlay cell outlines.\n",
       "  for (let i = 0; i < board.length; i++) {\n",
       "    c.beginPath();\n",
       "    drawCellCircle(i);\n",
       "    c.strokeStyle = &quot;#0361B2&quot;;\n",
       "    c.lineWidth = 1;\n",
       "    c.stroke();\n",
       "    c.closePath();\n",
       "  }\n",
       "\n",
       "  const drawLine = (fromCell, toCell) => {\n",
       "    if (frame < 0.5) return;\n",
       "    const lineFrame = (frame - 0.5) / 0.5;\n",
       "    const x1 = xOffset + (fromCell % columns) * cellSize + cellSize / 2;\n",
       "    const x2 =\n",
       "      x1 +\n",
       "      lineFrame *\n",
       "        (xOffset + ((toCell % columns) * cellSize + cellSize / 2) - x1);\n",
       "    const y1 =\n",
       "      yOffset + Math.floor(fromCell / columns) * cellSize + cellSize / 2;\n",
       "    const y2 =\n",
       "      y1 +\n",
       "      lineFrame *\n",
       "        (yOffset + Math.floor(toCell / columns) * cellSize + cellSize / 2 - y1);\n",
       "    c.beginPath();\n",
       "    c.lineCap = &quot;round&quot;;\n",
       "    c.lineWidth = 4;\n",
       "    c.strokeStyle = getColor(board[fromCell]);\n",
       "    c.shadowBlur = 8;\n",
       "    c.shadowColor = getColor(board[fromCell]);\n",
       "    c.moveTo(x1, y1);\n",
       "    c.lineTo(x2, y2);\n",
       "    c.stroke();\n",
       "  };\n",
       "\n",
       "  // Generate a graph of the board.\n",
       "  const getCell = (cell, rowOffset, columnOffset) => {\n",
       "    const row = Math.floor(cell / columns) + rowOffset;\n",
       "    const col = (cell % columns) + columnOffset;\n",
       "    if (row < 0 || row >= rows || col < 0 || col >= columns) return -1;\n",
       "    return col + row * columns;\n",
       "  };\n",
       "  const makeNode = cell => {\n",
       "    const node = { cell, directions: [], value: board[cell] };\n",
       "    for (let r = -1; r <= 1; r++) {\n",
       "      for (let c = -1; c <= 1; c++) {\n",
       "        if (r === 0 && c === 0) continue;\n",
       "        node.directions.push(getCell(cell, r, c));\n",
       "      }\n",
       "    }\n",
       "    return node;\n",
       "  };\n",
       "  const graph = board.map((_, i) => makeNode(i));\n",
       "\n",
       "  // Check for any wins!\n",
       "  const getSequence = (node, direction) => {\n",
       "    const sequence = [node.cell];\n",
       "    while (sequence.length < inarow) {\n",
       "      const next = graph[node.directions[direction]];\n",
       "      if (!next || node.value !== next.value || next.value === 0) return;\n",
       "      node = next;\n",
       "      sequence.push(node.cell);\n",
       "    }\n",
       "    return sequence;\n",
       "  };\n",
       "\n",
       "  // Check all nodes.\n",
       "  for (let i = 0; i < board.length; i++) {\n",
       "    // Check all directions (not the most efficient).\n",
       "    for (let d = 0; d < 8; d++) {\n",
       "      const seq = getSequence(graph[i], d);\n",
       "      if (seq) {\n",
       "        drawLine(seq[0], seq[inarow - 1]);\n",
       "        i = board.length;\n",
       "        break;\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "\n",
       "  // Upgrade the legend.\n",
       "  if (agents.length && (!agents[0].color || !agents[0].image)) {\n",
       "    const getPieceImage = mark => {\n",
       "      const pieceCanvas = document.createElement(&quot;canvas&quot;);\n",
       "      parent.appendChild(pieceCanvas);\n",
       "      pieceCanvas.style.marginLeft = &quot;10000px&quot;;\n",
       "      pieceCanvas.width = 100;\n",
       "      pieceCanvas.height = 100;\n",
       "      c = pieceCanvas.getContext(&quot;2d&quot;);\n",
       "      c.translate(10, 10);\n",
       "      c.scale(0.8, 0.8);\n",
       "      drawPiece(mark);\n",
       "      const dataUrl = pieceCanvas.toDataURL();\n",
       "      parent.removeChild(pieceCanvas);\n",
       "      return dataUrl;\n",
       "    };\n",
       "\n",
       "    agents.forEach(agent => {\n",
       "      agent.color = getColor(agent.index + 1);\n",
       "      agent.image = getPieceImage(agent.index + 1);\n",
       "    });\n",
       "    update({ agents });\n",
       "  }\n",
       "};\n",
       "\n",
       "\n",
       "    \n",
       "    </script>\n",
       "    <script>\n",
       "      const h = htm.bind(preact.h);\n",
       "      const { useContext, useEffect, useRef, useState } = preactHooks;\n",
       "      const styled = window.styled.default;\n",
       "\n",
       "      const Context = preact.createContext({});\n",
       "\n",
       "      const Loading = styled.div`\n",
       "        animation: rotate360 1.1s infinite linear;\n",
       "        border: 8px solid rgba(255, 255, 255, 0.2);\n",
       "        border-left-color: #0cb1ed;\n",
       "        border-radius: 50%;\n",
       "        height: 40px;\n",
       "        position: relative;\n",
       "        transform: translateZ(0);\n",
       "        width: 40px;\n",
       "\n",
       "        @keyframes rotate360 {\n",
       "          0% {\n",
       "            transform: rotate(0deg);\n",
       "          }\n",
       "          100% {\n",
       "            transform: rotate(360deg);\n",
       "          }\n",
       "        }\n",
       "      `;\n",
       "\n",
       "      const Logo = styled(\n",
       "        (props) => h`\n",
       "        <a href=&quot;https://kaggle.com&quot; target=&quot;_blank&quot; className=${props.className}>\n",
       "          <svg width=&quot;62px&quot; height=&quot;20px&quot; viewBox=&quot;0 0 62 24&quot; version=&quot;1.1&quot; xmlns=&quot;http://www.w3.org/2000/svg&quot;>\n",
       "            <g fill=&quot;#1EBEFF&quot; fill-rule=&quot;nonzero&quot;>\n",
       "              <path d=&quot;M10.2,17.8c0,0.1-0.1,0.1-0.2,0.1H7.7c-0.1,0-0.3-0.1-0.4-0.2l-3.8-4.9l-1.1,1v3.8 c0,0.2-0.1,0.3-0.3,0.3H0.3c-0.2,0-0.3-0.1-0.3-0.3V0.3C0.1,0.1,0.2,0,0.3,0h1.8c0.2,0,0.3,0.1,0.3,0.3V11L7,6.3 c0.1-0.1,0.2-0.2,0.4-0.2h2.4c0.1,0,0.2,0,0.2,0.1c0,0.1,0,0.2,0,0.2l-4.9,4.7l5.1,6.3C10.2,17.6,10.2,17.7,10.2,17.8z&quot;/>\n",
       "              <path d=&quot;M19.6,17.9h-1.8c-0.2,0-0.3-0.1-0.3-0.3v-0.4c-0.8,0.6-1.8,0.9-3,0.9c-1.1,0-2-0.3-2.8-1 c-0.8-0.7-1.2-1.6-1.2-2.7c0-1.7,1.1-2.9,3.2-3.5c0.8-0.2,2.1-0.5,3.8-0.6c0.1-0.6-0.1-1.2-0.5-1.7c-0.4-0.5-1-0.7-1.7-0.7 c-1,0-2,0.4-3,1C12.2,9.1,12.1,9.1,12,9l-0.9-1.3C11,7.5,11,7.4,11.1,7.3c1.3-0.9,2.7-1.4,4.2-1.4c1.1,0,2.1,0.3,2.8,0.8 c1.1,0.8,1.7,2,1.7,3.7v7.3C19.9,17.8,19.8,17.9,19.6,17.9z M17.5,12.4c-1.7,0.2-2.9,0.4-3.5,0.7c-0.9,0.4-1.2,0.9-1.1,1.6 c0.1,0.4,0.2,0.7,0.6,0.9c0.3,0.2,0.7,0.4,1.1,0.4c1.2,0.1,2.2-0.2,2.9-1V12.4z&quot;/>\n",
       "              <path d=&quot;M30.6,22.5c-0.9,1-2.3,1.5-4,1.5c-1,0-2-0.3-2.9-0.8c-0.2-0.1-0.4-0.3-0.7-0.5 c-0.3-0.2-0.6-0.5-0.9-0.7c-0.1-0.1-0.1-0.2,0-0.4l1.2-1.2c0.1-0.1,0.1-0.1,0.2-0.1c0.1,0,0.1,0,0.2,0.1c1,1,1.9,1.5,2.8,1.5 c2.1,0,3.2-1.1,3.2-3.3v-1.4c-0.8,0.7-1.9,1-3.3,1c-1.7,0-3-0.6-4-1.9c-0.8-1.1-1.3-2.5-1.3-4.2c0-1.6,0.4-3,1.2-4.1 c0.9-1.3,2.3-2,4-2c1.3,0,2.4,0.3,3.3,1V6.4c0-0.2,0.1-0.3,0.3-0.3h1.8c0.2,0,0.3,0.1,0.3,0.3v11.7C32,20,31.5,21.5,30.6,22.5z M29.7,9.9c-0.4-1.1-1.4-1.7-3-1.7c-2,0-3.1,1.3-3.1,3.8c0,1.4,0.3,2.4,1,3.1c0.5,0.5,1.2,0.8,2,0.8c1.6,0,2.7-0.6,3.1-1.7V9.9z&quot;/>\n",
       "              <path d=&quot;M42.9,22.5c-0.9,1-2.3,1.5-4,1.5c-1,0-2-0.3-2.9-0.8c-0.2-0.1-0.4-0.3-0.7-0.5 c-0.3-0.2-0.6-0.5-0.9-0.7c-0.1-0.1-0.1-0.2,0-0.4l1.2-1.2c0.1-0.1,0.1-0.1,0.2-0.1c0.1,0,0.1,0,0.2,0.1c1,1,1.9,1.5,2.8,1.5 c2.1,0,3.2-1.1,3.2-3.3v-1.4c-0.8,0.7-1.9,1-3.3,1c-1.7,0-3-0.6-4-1.9c-0.8-1.1-1.3-2.5-1.3-4.2c0-1.6,0.4-3,1.2-4.1 c0.9-1.3,2.3-2,4-2c1.3,0,2.4,0.3,3.3,1V6.4c0-0.2,0.1-0.3,0.3-0.3H44c0.2,0,0.3,0.1,0.3,0.3v11.7C44.3,20,43.8,21.5,42.9,22.5z M42,9.9c-0.4-1.1-1.4-1.7-3-1.7c-2,0-3.1,1.3-3.1,3.8c0,1.4,0.3,2.4,1,3.1c0.5,0.5,1.2,0.8,2,0.8c1.6,0,2.7-0.6,3.1-1.7L42,9.9 L42,9.9z&quot;/>\n",
       "              <path d=&quot;M48.3,17.9h-1.8c-0.2,0-0.3-0.1-0.3-0.3V0.3c0-0.2,0.1-0.3,0.3-0.3h1.8c0.2,0,0.3,0.1,0.3,0.3 v17.3C48.5,17.8,48.5,17.9,48.3,17.9z&quot;/>\n",
       "              <path d=&quot;M61.4,12.6c0,0.2-0.1,0.3-0.3,0.3h-8.5c0.1,0.9,0.5,1.6,1.1,2.2c0.7,0.6,1.6,0.9,2.7,0.9 c1,0,1.8-0.3,2.6-0.8c0.2-0.1,0.3-0.1,0.4,0l1.2,1.3c0.1,0.1,0.1,0.3,0,0.4c-1.3,0.9-2.7,1.4-4.4,1.4c-1.8,0-3.3-0.6-4.4-1.8 c-1.1-1.2-1.7-2.7-1.7-4.5c0-1.7,0.6-3.2,1.7-4.4c1-1.1,2.4-1.6,4.1-1.6c1.6,0,2.9,0.6,4,1.7c1.1,1.2,1.6,2.6,1.5,4.4L61.4,12.6 z M58,8.7c-0.6-0.5-1.3-0.8-2.1-0.8c-0.8,0-1.5,0.3-2.1,0.8c-0.6,0.5-1,1.2-1.1,2H59C59,9.9,58.6,9.3,58,8.7z&quot;/>\n",
       "            </g>\n",
       "          </svg>\n",
       "        </a>\n",
       "      `\n",
       "      )`\n",
       "        display: inline-flex;\n",
       "      `;\n",
       "\n",
       "      const Header = styled((props) => {\n",
       "        const { environment } = useContext(Context);\n",
       "\n",
       "        return h`<div className=${props.className} >\n",
       "          <${Logo} />\n",
       "          <span><b>Left / Right Arrow:</b> Increase / Decrease Step</span><span><b>0-9 Row Keys:</b> Playback Speed</span><span><b>Space:</b> Pause / Play</span>\n",
       "          ${environment.title}\n",
       "        </div>`;\n",
       "      })`\n",
       "        align-items: center;\n",
       "        border-bottom: 4px solid #212121;\n",
       "        box-sizing: border-box;\n",
       "        color: #fff;\n",
       "        display: flex;\n",
       "        flex: 0 0 36px;\n",
       "        font-size: 14px;\n",
       "        justify-content: space-between;\n",
       "        padding: 0 8px;\n",
       "        width: 100%;\n",
       "      `;\n",
       "\n",
       "      const Renderer = styled((props) => {\n",
       "        const context = useContext(Context);\n",
       "        const { animate, debug, playing, renderer, speed } = context;\n",
       "        const ref = preact.createRef();\n",
       "\n",
       "        useEffect(async () => {\n",
       "          if (!ref.current) return;\n",
       "\n",
       "          const renderFrame = async (start, step, lastFrame) => {\n",
       "            if (step !== context.step) return;\n",
       "            if (lastFrame === 1) {\n",
       "              if (!animate) return;\n",
       "              start = Date.now();\n",
       "            }\n",
       "            const frame =\n",
       "              playing || animate\n",
       "                ? Math.min((Date.now() - start) / speed, 1)\n",
       "                : 1;\n",
       "            try {\n",
       "              if (debug) console.time(&quot;render&quot;);\n",
       "              await renderer({\n",
       "                ...context,\n",
       "                frame,\n",
       "                height: ref.current.clientHeight,\n",
       "                hooks: preactHooks,\n",
       "                parent: ref.current,\n",
       "                preact,\n",
       "                styled,\n",
       "                width: ref.current.clientWidth,\n",
       "              });\n",
       "            } catch (error) {\n",
       "              if (debug) console.error(error);\n",
       "              console.log({ ...context, frame, error });\n",
       "            } finally {\n",
       "              if (debug) console.timeEnd(&quot;render&quot;);\n",
       "            }\n",
       "            window.requestAnimationFrame(() => renderFrame(start, step, frame));\n",
       "          };\n",
       "\n",
       "          await renderFrame(Date.now(), context.step);\n",
       "        }, [ref.current, context.step, context.renderer]);\n",
       "\n",
       "        return h`<div className=${props.className} ref=${ref} />`;\n",
       "      })`\n",
       "        align-items: center;\n",
       "        box-sizing: border-box;\n",
       "        display: flex;\n",
       "        height: 100%;\n",
       "        left: 0;\n",
       "        justify-content: center;\n",
       "        position: absolute;\n",
       "        top: 0;\n",
       "        width: 100%;\n",
       "      `;\n",
       "\n",
       "      const Processing = styled((props) => {\n",
       "        const { processing } = useContext(Context);\n",
       "        const text = processing === true ? &quot;Processing...&quot; : processing;\n",
       "        return h`<div className=${props.className}>${text}</div>`;\n",
       "      })`\n",
       "        bottom: 0;\n",
       "        color: #fff;\n",
       "        font-size: 12px;\n",
       "        left: 0;\n",
       "        line-height: 24px;\n",
       "        position: absolute;\n",
       "        text-align: center;\n",
       "        width: 100%;\n",
       "      `;\n",
       "\n",
       "      const Viewer = styled((props) => {\n",
       "        const { processing } = useContext(Context);\n",
       "        return h`<div className=${props.className}>\n",
       "          <${Renderer} />\n",
       "          ${processing && h`<${Processing} />`}\n",
       "        </div>`;\n",
       "      })`\n",
       "        background-color: #000b2a;\n",
       "        background-image: radial-gradient(\n",
       "          circle closest-side,\n",
       "          #000b49,\n",
       "          #000b2a\n",
       "        );\n",
       "        display: flex;\n",
       "        flex: 1;\n",
       "        overflow: hidden;\n",
       "        position: relative;\n",
       "        width: 100%;\n",
       "      `;\n",
       "\n",
       "      // Partitions the elements of arr into subarrays of max length num.\n",
       "      const groupIntoSets = (arr, num) => {\n",
       "        const sets = [];\n",
       "        arr.forEach(a => {\n",
       "          if (sets.length === 0 || sets[sets.length - 1].length === num) {\n",
       "            sets.push([]);\n",
       "          }\n",
       "          sets[sets.length - 1].push(a);\n",
       "        });\n",
       "        return sets;\n",
       "      }\n",
       "\n",
       "      // Expects `width` input prop to set proper max-width for agent name span.\n",
       "      const Legend = styled((props) => {\n",
       "        const { agents, legend } = useContext(Context);\n",
       "\n",
       "        const agentPairs = groupIntoSets(agents.sort((a, b) => a.index - b.index), 2);\n",
       "\n",
       "        return h`<div className=${props.className}>\n",
       "          ${agentPairs.map(agentList =>\n",
       "            h`<ul>\n",
       "                ${agentList.map(a =>\n",
       "                  h`<li key=${a.id} title=&quot;id: ${a.id}&quot; style=&quot;color:${a.color || &quot;#FFF&quot;}&quot;>\n",
       "                      ${a.image && h`<img src=${a.image} />`}\n",
       "                      <span>${a.name}</span>\n",
       "                    </li>`\n",
       "                )}\n",
       "              </ul>`)}\n",
       "        </div>`;\n",
       "      })`\n",
       "        background-color: #000b2a;\n",
       "        font-family: sans-serif;\n",
       "        font-size: 14px;\n",
       "        height: 48px;\n",
       "        width: 100%;\n",
       "\n",
       "        ul {\n",
       "          align-items: center;\n",
       "          display: flex;\n",
       "          flex-direction: row;\n",
       "          justify-content: center;\n",
       "        }\n",
       "\n",
       "        li {\n",
       "          align-items: center;\n",
       "          display: inline-flex;\n",
       "          transition: color 1s;\n",
       "        }\n",
       "\n",
       "        span {\n",
       "          max-width: ${p => (p.width || 400) * 0.5 - 36}px;\n",
       "          overflow: hidden;\n",
       "          text-overflow: ellipsis;\n",
       "          white-space: nowrap;\n",
       "        }\n",
       "\n",
       "        img {\n",
       "          height: 24px;\n",
       "          margin-left: 4px;\n",
       "          margin-right: 4px;\n",
       "          width: 24px;\n",
       "        }\n",
       "      `;\n",
       "\n",
       "      const StepInput = styled.input.attrs({\n",
       "        type: &quot;range&quot;,\n",
       "      })`\n",
       "        appearance: none;\n",
       "        background: rgba(255, 255, 255, 0.15);\n",
       "        border-radius: 2px;\n",
       "        display: block;\n",
       "        flex: 1;\n",
       "        height: 4px;\n",
       "        opacity: 0.8;\n",
       "        outline: none;\n",
       "        transition: opacity 0.2s;\n",
       "        width: 100%;\n",
       "\n",
       "        &:hover {\n",
       "          opacity: 1;\n",
       "        }\n",
       "\n",
       "        &::-webkit-slider-thumb {\n",
       "          appearance: none;\n",
       "          background: #1ebeff;\n",
       "          border-radius: 100%;\n",
       "          cursor: pointer;\n",
       "          height: 12px;\n",
       "          margin: 0;\n",
       "          position: relative;\n",
       "          width: 12px;\n",
       "\n",
       "          &::after {\n",
       "            content: &quot;&quot;;\n",
       "            position: absolute;\n",
       "            top: 0px;\n",
       "            left: 0px;\n",
       "            width: 200px;\n",
       "            height: 8px;\n",
       "            background: green;\n",
       "          }\n",
       "        }\n",
       "      `;\n",
       "\n",
       "      const PlayButton = styled.button`\n",
       "        align-items: center;\n",
       "        background: none;\n",
       "        border: none;\n",
       "        color: white;\n",
       "        cursor: pointer;\n",
       "        display: flex;\n",
       "        flex: 0 0 56px;\n",
       "        font-size: 20px;\n",
       "        height: 40px;\n",
       "        justify-content: center;\n",
       "        opacity: 0.8;\n",
       "        outline: none;\n",
       "        transition: opacity 0.2s;\n",
       "\n",
       "        &:hover {\n",
       "          opacity: 1;\n",
       "        }\n",
       "      `;\n",
       "\n",
       "      const StepCount = styled.span`\n",
       "        align-items: center;\n",
       "        color: white;\n",
       "        display: flex;\n",
       "        font-size: 14px;\n",
       "        justify-content: center;\n",
       "        opacity: 0.8;\n",
       "        padding: 0 16px;\n",
       "        pointer-events: none;\n",
       "      `;\n",
       "\n",
       "      const Controls = styled((props) => {\n",
       "        const { environment, pause, play, playing, setStep, step } = useContext(\n",
       "          Context\n",
       "        );\n",
       "        const value = step + 1;\n",
       "        const onClick = () => (playing ? pause() : play());\n",
       "        const onInput = (e) => {\n",
       "          pause();\n",
       "          setStep(parseInt(e.target.value) - 1);\n",
       "        };\n",
       "\n",
       "        return h`\n",
       "          <div className=${props.className}>\n",
       "            <${PlayButton} onClick=${onClick}><svg xmlns=&quot;http://www.w3.org/2000/svg&quot; width=&quot;24px&quot; height=&quot;24px&quot; viewBox=&quot;0 0 24 24&quot; fill=&quot;#FFFFFF&quot;>${\n",
       "          playing\n",
       "            ? h`<path d=&quot;M6 19h4V5H6v14zm8-14v14h4V5h-4z&quot;/><path d=&quot;M0 0h24v24H0z&quot; fill=&quot;none&quot;/>`\n",
       "            : h`<path d=&quot;M8 5v14l11-7z&quot;/><path d=&quot;M0 0h24v24H0z&quot; fill=&quot;none&quot;/>`\n",
       "        }</svg><//>\n",
       "            <${StepInput} min=&quot;1&quot; max=${\n",
       "          environment.steps.length\n",
       "        } value=&quot;${value}&quot; onInput=${onInput} />\n",
       "            <${StepCount}>${value} / ${environment.steps.length}<//>\n",
       "          </div>\n",
       "        `;\n",
       "      })`\n",
       "        align-items: center;\n",
       "        border-top: 4px solid #212121;\n",
       "        display: flex;\n",
       "        flex: 0 0 44px;\n",
       "        width: 100%;\n",
       "      `;\n",
       "\n",
       "      const Info = styled((props) => {\n",
       "        const {\n",
       "          environment,\n",
       "          playing,\n",
       "          step,\n",
       "          speed,\n",
       "          animate,\n",
       "          header,\n",
       "          controls,\n",
       "          settings,\n",
       "        } = useContext(Context);\n",
       "\n",
       "        return h`\n",
       "          <div className=${props.className}>\n",
       "            info:\n",
       "            step(${step}),\n",
       "            playing(${playing ? &quot;T&quot; : &quot;F&quot;}),\n",
       "            speed(${speed}),\n",
       "            animate(${animate ? &quot;T&quot; : &quot;F&quot;})\n",
       "          </div>`;\n",
       "      })`\n",
       "        color: #888;\n",
       "        font-family: monospace;\n",
       "        font-size: 12px;\n",
       "      `;\n",
       "\n",
       "      const Settings = styled((props) => {\n",
       "        const { environment, pause, play, playing, setStep, step } = useContext(\n",
       "          Context\n",
       "        );\n",
       "\n",
       "        return h`\n",
       "          <div className=${props.className}>\n",
       "            <${Info} />\n",
       "          </div>\n",
       "        `;\n",
       "      })`\n",
       "        background: #fff;\n",
       "        border-top: 4px solid #212121;\n",
       "        box-sizing: border-box;\n",
       "        padding: 20px;\n",
       "        width: 100%;\n",
       "\n",
       "        h1 {\n",
       "          font-size: 20px;\n",
       "        }\n",
       "      `;\n",
       "\n",
       "      const Player = styled((props) => {\n",
       "        const context = useContext(Context);\n",
       "        const { agents, controls, header, legend, loading, settings, width } = context;\n",
       "        return h`\n",
       "          <div className=${props.className}>\n",
       "            ${loading && h`<${Loading} />`}\n",
       "            ${!loading && header && h`<${Header} />`}\n",
       "            ${!loading && h`<${Viewer} />`}\n",
       "            ${!loading && legend && h`<${Legend} width=${width}/>`}\n",
       "            ${!loading && controls && h`<${Controls} />`}\n",
       "            ${!loading && settings && h`<${Settings} />`}\n",
       "          </div>`;\n",
       "      })`\n",
       "        align-items: center;\n",
       "        background: #212121;\n",
       "        border: 4px solid #212121;\n",
       "        box-sizing: border-box;\n",
       "        display: flex;\n",
       "        flex-direction: column;\n",
       "        height: 100%;\n",
       "        justify-content: center;\n",
       "        position: relative;\n",
       "        width: 100%;\n",
       "      `;\n",
       "\n",
       "      const App = () => {\n",
       "        const renderCountRef = useRef(0);\n",
       "        const [_, setRenderCount] = useState(0);\n",
       "\n",
       "        // These are bindings to the 0-9 keys and are milliseconds of timeout per step\n",
       "        const speeds = [\n",
       "          0,\n",
       "          3000,\n",
       "          1000,\n",
       "          500,\n",
       "          333, // Default\n",
       "          200,\n",
       "          100,\n",
       "          50,\n",
       "          25,\n",
       "          10,\n",
       "        ];\n",
       "\n",
       "        const contextRef = useRef({\n",
       "          animate: false,\n",
       "          agents: [],\n",
       "          controls: false,\n",
       "          debug: false,\n",
       "          environment: { steps: [], info: {} },\n",
       "          header: window.innerHeight >= 600,\n",
       "          height: window.innerHeight,\n",
       "          interactive: false,\n",
       "          legend: true,\n",
       "          loading: false,\n",
       "          playing: false,\n",
       "          processing: false,\n",
       "          renderer: () => &quot;DNE&quot;,\n",
       "          settings: false,\n",
       "          speed: speeds[4],\n",
       "          step: 0,\n",
       "          width: window.innerWidth,\n",
       "        });\n",
       "\n",
       "        // Context helpers.\n",
       "        const rerender = (contextRef.current.rerender = () =>\n",
       "          setRenderCount((renderCountRef.current += 1)));\n",
       "        const setStep = (contextRef.current.setStep = (newStep) => {\n",
       "          contextRef.current.step = newStep;\n",
       "          rerender();\n",
       "        });\n",
       "        const setPlaying = (contextRef.current.setPlaying = (playing) => {\n",
       "          contextRef.current.playing = playing;\n",
       "          rerender();\n",
       "        });\n",
       "        const pause = (contextRef.current.pause = () => setPlaying(false));\n",
       "\n",
       "        const playNext = () => {\n",
       "          const context = contextRef.current;\n",
       "\n",
       "          if (\n",
       "            context.playing &&\n",
       "            context.step < context.environment.steps.length - 1\n",
       "          ) {\n",
       "            setStep(context.step + 1);\n",
       "            play(true);\n",
       "          } else {\n",
       "            pause();\n",
       "          }\n",
       "        };\n",
       "\n",
       "        const play = (contextRef.current.play = (continuing) => {\n",
       "          const context = contextRef.current;\n",
       "          if (context.playing && !continuing) return;\n",
       "          if (!context.playing) setPlaying(true);\n",
       "          if (\n",
       "            !continuing &&\n",
       "            context.step === context.environment.steps.length - 1\n",
       "          ) {\n",
       "            setStep(0);\n",
       "          }\n",
       "          setTimeout(playNext, context.speed);\n",
       "        });\n",
       "\n",
       "        const updateContext = (o) => {\n",
       "          const context = contextRef.current;\n",
       "          Object.assign(context, o, {\n",
       "            environment: { ...context.environment, ...(o.environment || {}) },\n",
       "          });\n",
       "          rerender();\n",
       "        };\n",
       "\n",
       "        // First time setup.\n",
       "        useEffect(() => {\n",
       "          // Timeout is used to ensure useEffect renders once.\n",
       "          setTimeout(() => {\n",
       "            // Initialize context with window.kaggle.\n",
       "            updateContext(window.kaggle || {});\n",
       "\n",
       "            if (window.kaggle.playing) {\n",
       "                play(true);\n",
       "            }\n",
       "\n",
       "            // Listen for messages received to update the context.\n",
       "            window.addEventListener(\n",
       "              &quot;message&quot;,\n",
       "              (event) => {\n",
       "                // Ensure the environment names match before updating.\n",
       "                try {\n",
       "                  if (\n",
       "                    event.data.environment.name ==\n",
       "                    contextRef.current.environment.name\n",
       "                  ) {\n",
       "                    updateContext(event.data);\n",
       "                  }\n",
       "                } catch {}\n",
       "              },\n",
       "              false\n",
       "            );\n",
       "            // Listen for keyboard commands.\n",
       "            window.addEventListener(\n",
       "              &quot;keydown&quot;,\n",
       "              (event) => {\n",
       "                const {\n",
       "                  interactive,\n",
       "                  isInteractive,\n",
       "                  playing,\n",
       "                  step,\n",
       "                  environment,\n",
       "                } = contextRef.current;\n",
       "                const key = event.keyCode;\n",
       "                const zero_key = 48\n",
       "                const nine_key = 57\n",
       "                if (\n",
       "                  interactive ||\n",
       "                  isInteractive() ||\n",
       "                  (key !== 32 && key !== 37 && key !== 39 && !(key >= zero_key && key <= nine_key))\n",
       "                )\n",
       "                  return;\n",
       "\n",
       "                if (key === 32) {\n",
       "                  playing ? pause() : play();\n",
       "                } else if (key === 39) {\n",
       "                  contextRef.current.playing = false;\n",
       "                  if (step < environment.steps.length - 1) setStep(step + 1);\n",
       "                  rerender();\n",
       "                } else if (key === 37) {\n",
       "                  contextRef.current.playing = false;\n",
       "                  if (step > 0) setStep(step - 1);\n",
       "                  rerender();\n",
       "                } else if (key >= zero_key && key <= nine_key) {\n",
       "                  contextRef.current.speed = speeds[key - zero_key];\n",
       "                }\n",
       "                event.preventDefault();\n",
       "                return false;\n",
       "              },\n",
       "              false\n",
       "            );\n",
       "          }, 1);\n",
       "        }, []);\n",
       "\n",
       "        if (contextRef.current.debug) {\n",
       "          console.log(&quot;context&quot;, contextRef.current);\n",
       "        }\n",
       "\n",
       "        // Ability to update context.\n",
       "        contextRef.current.update = updateContext;\n",
       "\n",
       "        // Ability to communicate with ipython.\n",
       "        const execute = (contextRef.current.execute = (source) =>\n",
       "          new Promise((resolve, reject) => {\n",
       "            try {\n",
       "              window.parent.IPython.notebook.kernel.execute(source, {\n",
       "                iopub: {\n",
       "                  output: (resp) => {\n",
       "                    const type = resp.msg_type;\n",
       "                    if (type === &quot;stream&quot;) return resolve(resp.content.text);\n",
       "                    if (type === &quot;error&quot;) return reject(new Error(resp.evalue));\n",
       "                    return reject(new Error(&quot;Unknown message type: &quot; + type));\n",
       "                  },\n",
       "                },\n",
       "              });\n",
       "            } catch (e) {\n",
       "              reject(new Error(&quot;IPython Unavailable: &quot; + e));\n",
       "            }\n",
       "          }));\n",
       "\n",
       "        // Ability to return an action from an interactive session.\n",
       "        contextRef.current.act = (action) => {\n",
       "          const id = contextRef.current.environment.id;\n",
       "          updateContext({ processing: true });\n",
       "          execute(`\n",
       "            import json\n",
       "            from kaggle_environments import interactives\n",
       "            if &quot;${id}&quot; in interactives:\n",
       "                action = json.loads('${JSON.stringify(action)}')\n",
       "                env, trainer = interactives[&quot;${id}&quot;]\n",
       "                trainer.step(action)\n",
       "                print(json.dumps(env.steps))`)\n",
       "            .then((resp) => {\n",
       "              try {\n",
       "                updateContext({\n",
       "                  processing: false,\n",
       "                  environment: { steps: JSON.parse(resp) },\n",
       "                });\n",
       "                play();\n",
       "              } catch (e) {\n",
       "                updateContext({ processing: resp.split(&quot;\\n&quot;)[0] });\n",
       "                console.error(resp, e);\n",
       "              }\n",
       "            })\n",
       "            .catch((e) => console.error(e));\n",
       "        };\n",
       "\n",
       "        // Check if currently interactive.\n",
       "        contextRef.current.isInteractive = () => {\n",
       "          const context = contextRef.current;\n",
       "          const steps = context.environment.steps;\n",
       "          return (\n",
       "            context.interactive &&\n",
       "            !context.processing &&\n",
       "            context.step === steps.length - 1 &&\n",
       "            steps[context.step].some((s) => s.status === &quot;ACTIVE&quot;)\n",
       "          );\n",
       "        };\n",
       "\n",
       "        return h`\n",
       "          <${Context.Provider} value=${contextRef.current}>\n",
       "            <${Player} />\n",
       "          <//>`;\n",
       "      };\n",
       "\n",
       "      preact.render(h`<${App} />`, document.body);\n",
       "    </script>\n",
       "  </body>\n",
       "</html>\n",
       "\" width=\"500\" height=\"500\" frameborder=\"0\"></iframe> "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import kaggle_environments\n",
    "from kaggle_environments import evaluate\n",
    "\n",
    "class KaggleAgent:\n",
    "    def __init__(self, model, game, args):\n",
    "        self.model = model\n",
    "        self.game = game\n",
    "        self.args = args\n",
    "        self.mcts = MCTS(self.game, self.args, self.model)\n",
    "\n",
    "    def run(self, obs, conf):\n",
    "        player = obs['mark'] if obs['mark'] == 1 else -1\n",
    "        state = np.array(obs['board']).reshape(self.game.row_count, self.game.col_count)\n",
    "        state[state==2] = -1\n",
    "        \n",
    "        state = self.game.change_perspective(state, player)        \n",
    "        \n",
    "        policy = self.mcts.search(state)\n",
    "\n",
    "        valid_moves = self.game.get_valid_moves(state)\n",
    "        policy *= valid_moves\n",
    "        policy /= np.sum(policy)\n",
    "\n",
    "        if self.args['temperature'] == 0:\n",
    "            action = int(np.argmax(policy))\n",
    "        elif self.args['temperature'] == float('inf'):\n",
    "            action = np.random.choice([r for r in range(self.game.action_size) if policy[r] > 0])\n",
    "        else:\n",
    "            policy = policy ** (1 / self.args['temperature'])\n",
    "            policy /= np.sum(policy)\n",
    "            action = np.random.choice(self.game.action_size, p=policy)\n",
    "\n",
    "        return action\n",
    "\n",
    "game = ConnectFour()\n",
    "\n",
    "args = {\n",
    "    'C': 2,\n",
    "    'num_searches': 200,\n",
    "    'dirichlet_epsilon': 0.0,\n",
    "    'dirichlet_alpha': 0.3,\n",
    "    'temperature': 0,\n",
    "}\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = ResNet(game, 9, 128, device)\n",
    "model.load_state_dict(torch.load(f'../models/cf0_control/model_11.pt', map_location=device))\n",
    "model.eval()\n",
    "\n",
    "env = kaggle_environments.make('connectx')\n",
    "\n",
    "player1 = KaggleAgent(model, game, args)\n",
    "player2 = KaggleAgent(model, game, args)\n",
    "\n",
    "players = [player1.run, player2.run]\n",
    "\n",
    "env.run(players)\n",
    "\n",
    "env.render(mode=\"ipython\", width=500, height=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "388574b7-a704-4591-aed0-fd305d458079",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
