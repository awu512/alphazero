{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7620efee-0dba-4939-ad3d-880af53233b5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-24T03:24:22.152294Z",
     "start_time": "2023-11-24T03:24:20.805377Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.24.4\n",
      "2.1.1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f9580ba0b90>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "import random\n",
    "from tqdm.notebook import trange\n",
    "\n",
    "import numpy as np\n",
    "print(np.__version__)\n",
    "\n",
    "import torch\n",
    "print(torch.__version__)\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d894942-1528-4af0-8993-7d0d1e30b243",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConnectFour:\n",
    "    ''' Game definition for ConnectFour '''\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.row_count = 6\n",
    "        self.col_count = 7\n",
    "        self.action_size = self.col_count\n",
    "        self.win_condition = 4\n",
    "\n",
    "    def __repr__(self):\n",
    "        return 'ConnectFour'\n",
    "\n",
    "    def get_initial_state(self):\n",
    "        ''' Get board with all zeros '''\n",
    "        return np.zeros((self.row_count, self.col_count))\n",
    "\n",
    "    def get_next_state(self, state, action, player):\n",
    "        ''' Get the next state given the given action by the given player '''\n",
    "        row = np.max(np.where(state[:, action] == 0))\n",
    "        state[row, action] = player\n",
    "        return state\n",
    "\n",
    "    def get_valid_moves(self, state):\n",
    "        ''' Get all the legal moves in the position '''\n",
    "        return (state[0] == 0).astype(np.uint8)\n",
    "\n",
    "    def check_win(self, state, action):\n",
    "        ''' Check if the given action has led to a win '''\n",
    "        if action == None:\n",
    "            return False\n",
    "        \n",
    "        row = np.min(np.where(state[:, action] != 0))\n",
    "        column = action\n",
    "        player = state[row][column]\n",
    "\n",
    "        def count(offset_row, offset_column):\n",
    "            for i in range(1, self.win_condition):\n",
    "                r = row + offset_row * i\n",
    "                c = action + offset_column * i\n",
    "                if (\n",
    "                    r < 0 \n",
    "                    or r >= self.row_count\n",
    "                    or c < 0 \n",
    "                    or c >= self.col_count\n",
    "                    or state[r][c] != player\n",
    "                ):\n",
    "                    return i - 1\n",
    "            return self.win_condition - 1\n",
    "\n",
    "        return (\n",
    "            count(1, 0) >= self.win_condition - 1 # vertical\n",
    "            or (count(0, 1) + count(0, -1)) >= self.win_condition - 1 # horizontal\n",
    "            or (count(1, 1) + count(-1, -1)) >= self.win_condition - 1 # top left diagonal\n",
    "            or (count(1, -1) + count(-1, 1)) >= self.win_condition - 1 # top right diagonal\n",
    "        )\n",
    "\n",
    "    def get_value_and_terminated(self, state, action):\n",
    "        ''' Get the value (win/tie) and if the game has terminated '''\n",
    "        if self.check_win(state, action):\n",
    "            return 1, True\n",
    "        if np.sum(self.get_valid_moves(state)) == 0:\n",
    "            return 0, True\n",
    "        return 0, False\n",
    "\n",
    "    def get_opponent(self, player):\n",
    "        ''' Get the opponent of the player '''\n",
    "        return -player\n",
    "\n",
    "    def get_opponent_value(self, value):\n",
    "        ''' Get the value from the opponent's perspective '''\n",
    "        return -value\n",
    "\n",
    "    def change_perspective(self, state, player):\n",
    "        ''' Transform the state to be from the opponent's perspective '''\n",
    "        return state * player\n",
    "\n",
    "    def get_encoded_state(self, state):\n",
    "        ''' Get the network-ready encoded state of the game '''\n",
    "        encoded_state = np.stack((state == -1, state == 0, state == 1)).astype(np.float32)\n",
    "\n",
    "        # check for batched states\n",
    "        if len(state.shape) == 3:\n",
    "            encoded_state = np.swapaxes(encoded_state, 0, 1)\n",
    "        \n",
    "        return encoded_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3bd0cd66-2da5-4e7e-97eb-becc878292d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    "    ''' The residual neural network implementation for AlphaZero '''\n",
    "    \n",
    "    def __init__(self, game, num_resblocks, num_hidden, device):\n",
    "        super().__init__()\n",
    "\n",
    "        # store the specified device for computation\n",
    "        self.device = device\n",
    "\n",
    "        # initial NN block\n",
    "        self.start_block = nn.Sequential(\n",
    "            nn.Conv2d(3, num_hidden, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(num_hidden),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        # shared portion of network between policy and value heads\n",
    "        self.backbone = nn.ModuleList(\n",
    "            [ResBlock(num_hidden) for _ in range(num_resblocks)]\n",
    "        )\n",
    "\n",
    "        # the portion of the network responsible for outputting policies\n",
    "        self.policy_head = nn.Sequential(\n",
    "            nn.Conv2d(num_hidden, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(32 * game.row_count * game.col_count, game.action_size)\n",
    "        )\n",
    "\n",
    "        # the portion of the network responsible for outputting values\n",
    "        self.value_head = nn.Sequential(\n",
    "            nn.Conv2d(num_hidden, 3, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(3),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(3 * game.row_count * game.col_count, 1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "        # send computation to the device\n",
    "        self.to(device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        ''' Feed forward implementation for this network '''\n",
    "        x = self.start_block(x)\n",
    "        for resblock in self.backbone:\n",
    "            x = resblock(x)\n",
    "        policy = self.policy_head(x)\n",
    "        value = self.value_head(x)\n",
    "\n",
    "        return policy, value\n",
    "\n",
    "class ResBlock(nn.Module):\n",
    "    ''' ResNet block '''\n",
    "    \n",
    "    def __init__(self, num_hidden):\n",
    "        ''' Initialize the block '''\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(num_hidden, num_hidden, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(num_hidden)\n",
    "        self.conv2 = nn.Conv2d(num_hidden, num_hidden, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(num_hidden)\n",
    "\n",
    "    def forward(self, x):\n",
    "        ''' Feed forward implementation for this block '''\n",
    "        residual = x\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.bn2(self.conv2(x))\n",
    "        x += residual\n",
    "        x = F.relu(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e888f91a-00c3-46bf-a9f5-d0685368479e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  1.]\n",
      " [ 0.  0. -1.  0. -1.  0.  1.]]\n",
      "[[[0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 1. 0. 1. 0. 0.]]\n",
      "\n",
      " [[1. 1. 1. 1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1. 1. 1. 0.]\n",
      "  [1. 1. 0. 1. 0. 1. 0.]]\n",
      "\n",
      " [[0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 1.]\n",
      "  [0. 0. 0. 0. 0. 0. 1.]]]\n",
      "-0.0350162647664547\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W NNPACK.cpp:64] Could not initialize NNPACK! Reason: Unsupported hardware.\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiwAAAGgCAYAAACJ7TzXAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8WgzjOAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAnc0lEQVR4nO3df1DUd37H8Rc/AvgLksi5CBpXqxflVFAQitqQa3ayXp1LuKaEON5BiUMmLdtotqU5rJFmbLP2ohYTGTnSMelNz4HaVs9LDDluE0w74hFB5qKexrtLDka7i8xdQbEHDrv9w2adPRfiEmU/wvMx852T736+X977nZvkmS/fxSi/3+8XAACAwaIjPQAAAMDnIVgAAIDxCBYAAGA8ggUAABiPYAEAAMYjWAAAgPEIFgAAYDyCBQAAGI9gAQAAxiNYAACA8UYVLDU1NbJarUpISFBubq5aW1uHXXv69Gk98cQTslqtioqKUnV1dch1Fy5c0De/+U1Nnz5dkyZN0pIlS3TixInRjAcAAMaZ2HAPaGhokNPpVG1trXJzc1VdXS273a5z585pxowZN62/evWq5s2bp8LCQj3//PMhz/mb3/xGq1at0le/+lW98847+tKXvqTz58/rvvvuu+W5fD6fLl68qGnTpikqKirctwUAACLA7/fr8uXLSk1NVXT0CPdR/GHKycnxl5eXB74eGhryp6am+l0u1+ceO2fOHP8//uM/3rT/hRde8K9evTrcUYJ0dXX5JbGxsbGxsbHdhVtXV9eI/54P6w7L4OCg2traVFlZGdgXHR0tm82mlpaWcE4V5PDhw7Lb7SosLNTRo0eVlpamP//zP1dZWdmwxwwMDGhgYCDwtf///9Lprq4uJSYmjnoWAAAwdvr6+jR79mxNmzZtxHVhBUtPT4+GhoZksViC9lssFp09ezb8Kf/fL3/5S+3du1dOp1ObN2/Whx9+qOeee05xcXEqKSkJeYzL5dJLL7100/7ExESCBQCAu8znPc5hxKeEfD6fli9frpdfflnLli3TM888o7KyMtXW1g57TGVlpXp7ewNbV1fXGE4MAADGUljBkpycrJiYGHm93qD9Xq9XKSkpox5i5syZSk9PD9q3aNEidXZ2DntMfHx84G4Kd1UAABjfwgqWuLg4ZWVlye12B/b5fD653W7l5eWNeohVq1bp3LlzQfs+/vhjzZkzZ9TnBAAA40fYH2t2Op0qKSlRdna2cnJyVF1drf7+fpWWlkqSiouLlZaWJpfLJen6g7pnzpwJ/PnChQvq6OjQ1KlTNX/+fEnS888/r5UrV+rll1/Wk08+qdbWVtXV1amuru52vU8AAHAXi/J/9vGaMOzZs0evvPKKPB6PMjMz9eqrryo3N1eS9PDDD8tqterNN9+UJH366aeaO3fuTefIz89Xc3Nz4Ou33npLlZWVOn/+vObOnSun0znip4R+V19fn5KSktTb28uPhwAAuEvc6r+/RxUsJiJYAAC4+9zqv7+N+JQQAADASAgWAABgPIIFAAAYj2ABAADGI1gAAIDxCBYAAGA8ggUAABiPYAEAAMYL+1fzAwCCWb/9dqRHuO0+3b420iMAQbjDAgAAjEewAAAA4xEsAADAeAQLAAAwHsECAACMR7AAAADjESwAAMB4BAsAADAewQIAAIxHsAAAAOMRLAAAwHgECwAAMB7BAgAAjEewAAAA4xEsAADAeAQLAAAwHsECAACMR7AAAADjxUZ6ANw9rN9+O9Ij3BGfbl8b6REAAJ+DOywAAMB43GEBANw24/FOLHdhzcAdFgAAYDyCBQAAGI8fCd0CbnECABBZ3GEBAADGI1gAAIDxRhUsNTU1slqtSkhIUG5urlpbW4dde/r0aT3xxBOyWq2KiopSdXX1iOfevn27oqKitGnTptGMBgAAxqGwg6WhoUFOp1NVVVVqb29XRkaG7Ha7uru7Q66/evWq5s2bp+3btyslJWXEc3/44Yf67ne/q6VLl4Y7FgAAGMfCDpZdu3aprKxMpaWlSk9PV21trSZPnqx9+/aFXL9ixQq98soreuqppxQfHz/sea9cuaL169fr9ddf13333RfuWAAAYBwLK1gGBwfV1tYmm8124wTR0bLZbGppaflCg5SXl2vt2rVB5x7JwMCA+vr6gjYAADA+hfWx5p6eHg0NDclisQTtt1gsOnv27KiHqK+vV3t7uz788MNbPsblcumll14a9fcE8MXxkX8AYyXinxLq6urSxo0b9f3vf18JCQm3fFxlZaV6e3sDW1dX1x2cEgAARFJYd1iSk5MVExMjr9cbtN/r9X7uA7XDaWtrU3d3t5YvXx7YNzQ0pA8++EB79uzRwMCAYmJibjouPj5+xGdiAADA+BHWHZa4uDhlZWXJ7XYH9vl8PrndbuXl5Y1qgEceeUQfffSROjo6Alt2drbWr1+vjo6OkLECAAAmlrB/Nb/T6VRJSYmys7OVk5Oj6upq9ff3q7S0VJJUXFystLQ0uVwuSdcf1D1z5kzgzxcuXFBHR4emTp2q+fPna9q0aVq8eHHQ95gyZYqmT59+034AADAxhR0sRUVFunTpkrZu3SqPx6PMzEw1NjYGHsTt7OxUdPSNGzcXL17UsmXLAl/v2LFDO3bsUH5+vpqbm7/4OwAAAOPeqP7yQ4fDIYfDEfK1340Qq9Uqv98f1vkJGQDA3YxP0N1+Ef+UEAAAwOchWAAAgPEIFgAAYDyCBQAAGG9UD90CEx0P1AHA2OIOCwAAMB7BAgAAjEewAAAA4xEsAADAeAQLAAAwHsECAACMR7AAAADjESwAAMB4BAsAADAewQIAAIxHsAAAAOMRLAAAwHgECwAAMB7BAgAAjEewAAAA4xEsAADAeAQLAAAwHsECAACMR7AAAADjESwAAMB4BAsAADAewQIAAIxHsAAAAOMRLAAAwHgECwAAMB7BAgAAjEewAAAA4xEsAADAeAQLAAAwHsECAACMN6pgqampkdVqVUJCgnJzc9Xa2jrs2tOnT+uJJ56Q1WpVVFSUqqurb1rjcrm0YsUKTZs2TTNmzFBBQYHOnTs3mtEAAMA4FHawNDQ0yOl0qqqqSu3t7crIyJDdbld3d3fI9VevXtW8efO0fft2paSkhFxz9OhRlZeX6/jx42pqatK1a9f06KOPqr+/P9zxAADAOBQb7gG7du1SWVmZSktLJUm1tbV6++23tW/fPn3729++af2KFSu0YsUKSQr5uiQ1NjYGff3mm29qxowZamtr00MPPRTuiAAAYJwJ6w7L4OCg2traZLPZbpwgOlo2m00tLS23baje3l5J0v333z/smoGBAfX19QVtAABgfAorWHp6ejQ0NCSLxRK032KxyOPx3JaBfD6fNm3apFWrVmnx4sXDrnO5XEpKSgpss2fPvi3fHwAAmMe4TwmVl5fr1KlTqq+vH3FdZWWlent7A1tXV9cYTQgAAMZaWM+wJCcnKyYmRl6vN2i/1+sd9oHacDgcDr311lv64IMPNGvWrBHXxsfHKz4+/gt/TwAAYL6w7rDExcUpKytLbrc7sM/n88ntdisvL2/UQ/j9fjkcDh08eFDvvfee5s6dO+pzAQCA8SfsTwk5nU6VlJQoOztbOTk5qq6uVn9/f+BTQ8XFxUpLS5PL5ZJ0/UHdM2fOBP584cIFdXR0aOrUqZo/f76k6z8G2r9/v37wgx9o2rRpgedhkpKSNGnSpNvyRgEAwN0r7GApKirSpUuXtHXrVnk8HmVmZqqxsTHwIG5nZ6eio2/cuLl48aKWLVsW+HrHjh3asWOH8vPz1dzcLEnau3evJOnhhx8O+l5vvPGG/vRP/zTcEQEAwDgTdrBI1581cTgcIV/7LEI+Y7Va5ff7Rzzf570OAAAmNuM+JQQAAPC7CBYAAGA8ggUAABiPYAEAAMYjWAAAgPEIFgAAYDyCBQAAGI9gAQAAxiNYAACA8QgWAABgPIIFAAAYj2ABAADGI1gAAIDxCBYAAGA8ggUAABiPYAEAAMYjWAAAgPEIFgAAYDyCBQAAGI9gAQAAxiNYAACA8QgWAABgPIIFAAAYj2ABAADGI1gAAIDxCBYAAGA8ggUAABiPYAEAAMYjWAAAgPEIFgAAYDyCBQAAGI9gAQAAxiNYAACA8QgWAABgPIIFAAAYb1TBUlNTI6vVqoSEBOXm5qq1tXXYtadPn9YTTzwhq9WqqKgoVVdXf+FzAgCAiSXsYGloaJDT6VRVVZXa29uVkZEhu92u7u7ukOuvXr2qefPmafv27UpJSbkt5wQAABNL2MGya9culZWVqbS0VOnp6aqtrdXkyZO1b9++kOtXrFihV155RU899ZTi4+NvyzkBAMDEElawDA4Oqq2tTTab7cYJoqNls9nU0tIyqgFGe86BgQH19fUFbQAAYHwKK1h6eno0NDQki8UStN9iscjj8YxqgNGe0+VyKSkpKbDNnj17VN8fAACY7679lFBlZaV6e3sDW1dXV6RHAgAAd0hsOIuTk5MVExMjr9cbtN/r9Q77QO2dOmd8fPywz8QAAIDxJaw7LHFxccrKypLb7Q7s8/l8crvdysvLG9UAd+KcAABgfAnrDoskOZ1OlZSUKDs7Wzk5OaqurlZ/f79KS0slScXFxUpLS5PL5ZJ0/aHaM2fOBP584cIFdXR0aOrUqZo/f/4tnRMAAExsYQdLUVGRLl26pK1bt8rj8SgzM1ONjY2Bh2Y7OzsVHX3jxs3Fixe1bNmywNc7duzQjh07lJ+fr+bm5ls6JwAAmNjCDhZJcjgccjgcIV/7LEI+Y7Va5ff7v9A5AQDAxHbXfkoIAABMHAQLAAAwHsECAACMR7AAAADjESwAAMB4BAsAADAewQIAAIxHsAAAAOMRLAAAwHgECwAAMB7BAgAAjEewAAAA4xEsAADAeAQLAAAwHsECAACMR7AAAADjESwAAMB4BAsAADAewQIAAIxHsAAAAOMRLAAAwHgECwAAMB7BAgAAjEewAAAA4xEsAADAeAQLAAAwHsECAACMR7AAAADjESwAAMB4BAsAADAewQIAAIxHsAAAAOMRLAAAwHgECwAAMN6ogqWmpkZWq1UJCQnKzc1Va2vriOsPHDighQsXKiEhQUuWLNGRI0eCXr9y5YocDodmzZqlSZMmKT09XbW1taMZDQAAjENhB0tDQ4OcTqeqqqrU3t6ujIwM2e12dXd3h1x/7NgxrVu3Ths2bNDJkydVUFCggoICnTp1KrDG6XSqsbFR//Iv/6Kf/exn2rRpkxwOhw4fPjz6dwYAAMaNsINl165dKisrU2lpaeBOyOTJk7Vv376Q63fv3q01a9aooqJCixYt0rZt27R8+XLt2bMnsObYsWMqKSnRww8/LKvVqmeeeUYZGRmfe+cGAABMDGEFy+DgoNra2mSz2W6cIDpaNptNLS0tIY9paWkJWi9Jdrs9aP3KlSt1+PBhXbhwQX6/X++//74+/vhjPfroo+GMBwAAxqnYcBb39PRoaGhIFoslaL/FYtHZs2dDHuPxeEKu93g8ga9fe+01PfPMM5o1a5ZiY2MVHR2t119/XQ899NCwswwMDGhgYCDwdV9fXzhvBQAA3EWM+JTQa6+9puPHj+vw4cNqa2vTzp07VV5erh//+MfDHuNyuZSUlBTYZs+ePYYTAwCAsRTWHZbk5GTFxMTI6/UG7fd6vUpJSQl5TEpKyojr//d//1ebN2/WwYMHtXbtWknS0qVL1dHRoR07dtz046TPVFZWyul0Br7u6+sjWgAAGKfCusMSFxenrKwsud3uwD6fzye32628vLyQx+Tl5QWtl6SmpqbA+mvXrunatWuKjg4eJSYmRj6fb9hZ4uPjlZiYGLQBAIDxKaw7LNL1jyCXlJQoOztbOTk5qq6uVn9/v0pLSyVJxcXFSktLk8vlkiRt3LhR+fn52rlzp9auXav6+nqdOHFCdXV1kqTExETl5+eroqJCkyZN0pw5c3T06FF973vf065du27jWwUAAHersIOlqKhIly5d0tatW+XxeJSZmanGxsbAg7WdnZ1Bd0tWrlyp/fv3a8uWLdq8ebMWLFigQ4cOafHixYE19fX1qqys1Pr16/XrX/9ac+bM0d///d/r2WefvQ1vEQAA3O3CDhZJcjgccjgcIV9rbm6+aV9hYaEKCwuHPV9KSoreeOON0YwCAAAmACM+JQQAADASggUAABiPYAEAAMYjWAAAgPEIFgAAYDyCBQAAGI9gAQAAxiNYAACA8QgWAABgPIIFAAAYj2ABAADGI1gAAIDxCBYAAGA8ggUAABiPYAEAAMYjWAAAgPEIFgAAYDyCBQAAGI9gAQAAxiNYAACA8QgWAABgPIIFAAAYj2ABAADGI1gAAIDxCBYAAGA8ggUAABiPYAEAAMYjWAAAgPEIFgAAYDyCBQAAGI9gAQAAxiNYAACA8QgWAABgPIIFAAAYj2ABAADGG1Ww1NTUyGq1KiEhQbm5uWptbR1x/YEDB7Rw4UIlJCRoyZIlOnLkyE1rfvazn+mxxx5TUlKSpkyZohUrVqizs3M04wEAgHEm7GBpaGiQ0+lUVVWV2tvblZGRIbvdru7u7pDrjx07pnXr1mnDhg06efKkCgoKVFBQoFOnTgXW/OIXv9Dq1au1cOFCNTc366c//alefPFFJSQkjP6dAQCAcSPsYNm1a5fKyspUWlqq9PR01dbWavLkydq3b1/I9bt379aaNWtUUVGhRYsWadu2bVq+fLn27NkTWPM3f/M3+qM/+iN95zvf0bJly/R7v/d7euyxxzRjxozRvzMAADBuhBUsg4ODamtrk81mu3GC6GjZbDa1tLSEPKalpSVovSTZ7fbAep/Pp7fffltf/vKXZbfbNWPGDOXm5urQoUNhvhUAADBehRUsPT09GhoaksViCdpvsVjk8XhCHuPxeEZc393drStXrmj79u1as2aNfvSjH+kb3/iG/viP/1hHjx4ddpaBgQH19fUFbQAAYHyKjfQAPp9PkvT444/r+eeflyRlZmbq2LFjqq2tVX5+fsjjXC6XXnrppTGbEwAARE5Yd1iSk5MVExMjr9cbtN/r9SolJSXkMSkpKSOuT05OVmxsrNLT04PWLFq0aMRPCVVWVqq3tzewdXV1hfNWAADAXSSsYImLi1NWVpbcbndgn8/nk9vtVl5eXshj8vLygtZLUlNTU2B9XFycVqxYoXPnzgWt+fjjjzVnzpxhZ4mPj1diYmLQBgAAxqewfyTkdDpVUlKi7Oxs5eTkqLq6Wv39/SotLZUkFRcXKy0tTS6XS5K0ceNG5efna+fOnVq7dq3q6+t14sQJ1dXVBc5ZUVGhoqIiPfTQQ/rqV7+qxsZG/fCHP1Rzc/PteZcAAOCuFnawFBUV6dKlS9q6das8Ho8yMzPV2NgYeLC2s7NT0dE3btysXLlS+/fv15YtW7R582YtWLBAhw4d0uLFiwNrvvGNb6i2tlYul0vPPfecHnzwQf37v/+7Vq9efRveIgAAuNuN6qFbh8Mhh8MR8rVQd0UKCwtVWFg44jmffvppPf3006MZBwAAjHP8XUIAAMB4BAsAADAewQIAAIxHsAAAAOMRLAAAwHgECwAAMB7BAgAAjEewAAAA4xEsAADAeAQLAAAwHsECAACMR7AAAADjESwAAMB4BAsAADAewQIAAIxHsAAAAOMRLAAAwHgECwAAMB7BAgAAjEewAAAA4xEsAADAeAQLAAAwHsECAACMR7AAAADjESwAAMB4BAsAADAewQIAAIxHsAAAAOMRLAAAwHgECwAAMB7BAgAAjEewAAAA4xEsAADAeAQLAAAwHsECAACMN6pgqampkdVqVUJCgnJzc9Xa2jri+gMHDmjhwoVKSEjQkiVLdOTIkWHXPvvss4qKilJ1dfVoRgMAAONQ2MHS0NAgp9Opqqoqtbe3KyMjQ3a7Xd3d3SHXHzt2TOvWrdOGDRt08uRJFRQUqKCgQKdOnbpp7cGDB3X8+HGlpqaG/04AAMC4FXaw7Nq1S2VlZSotLVV6erpqa2s1efJk7du3L+T63bt3a82aNaqoqNCiRYu0bds2LV++XHv27Alad+HCBf3FX/yFvv/97+uee+4Z3bsBAADjUljBMjg4qLa2NtlsthsniI6WzWZTS0tLyGNaWlqC1kuS3W4PWu/z+fStb31LFRUV+spXvhLOSAAAYAKIDWdxT0+PhoaGZLFYgvZbLBadPXs25DEejyfkeo/HE/j6H/7hHxQbG6vnnnvulmcZGBjQwMBA4Ou+vr5bPhYAANxdIv4poba2Nu3evVtvvvmmoqKibvk4l8ulpKSkwDZ79uw7OCUAAIiksIIlOTlZMTEx8nq9Qfu9Xq9SUlJCHpOSkjLi+v/8z/9Ud3e3HnjgAcXGxio2Nla/+tWv9Jd/+ZeyWq3DzlJZWane3t7A1tXVFc5bAQAAd5GwgiUuLk5ZWVlyu92BfT6fT263W3l5eSGPycvLC1ovSU1NTYH13/rWt/TTn/5UHR0dgS01NVUVFRV69913h50lPj5eiYmJQRsAABifwnqGRZKcTqdKSkqUnZ2tnJwcVVdXq7+/X6WlpZKk4uJipaWlyeVySZI2btyo/Px87dy5U2vXrlV9fb1OnDihuro6SdL06dM1ffr0oO9xzz33KCUlRQ8++OAXfX8AAGAcCDtYioqKdOnSJW3dulUej0eZmZlqbGwMPFjb2dmp6OgbN25Wrlyp/fv3a8uWLdq8ebMWLFigQ4cOafHixbfvXQAAgHEt7GCRJIfDIYfDEfK15ubmm/YVFhaqsLDwls//6aefjmYsAAAwTkX8U0IAAACfh2ABAADGI1gAAIDxCBYAAGA8ggUAABiPYAEAAMYjWAAAgPEIFgAAYDyCBQAAGI9gAQAAxiNYAACA8QgWAABgPIIFAAAYj2ABAADGI1gAAIDxCBYAAGA8ggUAABiPYAEAAMYjWAAAgPEIFgAAYDyCBQAAGI9gAQAAxiNYAACA8QgWAABgPIIFAAAYj2ABAADGI1gAAIDxCBYAAGA8ggUAABiPYAEAAMYjWAAAgPEIFgAAYDyCBQAAGI9gAQAAxiNYAACA8UYVLDU1NbJarUpISFBubq5aW1tHXH/gwAEtXLhQCQkJWrJkiY4cORJ47dq1a3rhhRe0ZMkSTZkyRampqSouLtbFixdHMxoAABiHwg6WhoYGOZ1OVVVVqb29XRkZGbLb7eru7g65/tixY1q3bp02bNigkydPqqCgQAUFBTp16pQk6erVq2pvb9eLL76o9vZ2/cd//IfOnTunxx577Iu9MwAAMG6EHSy7du1SWVmZSktLlZ6ertraWk2ePFn79u0LuX737t1as2aNKioqtGjRIm3btk3Lly/Xnj17JElJSUlqamrSk08+qQcffFC///u/rz179qitrU2dnZ1f7N0BAIBxIaxgGRwcVFtbm2w2240TREfLZrOppaUl5DEtLS1B6yXJbrcPu16Sent7FRUVpXvvvXfYNQMDA+rr6wvaAADA+BRWsPT09GhoaEgWiyVov8VikcfjCXmMx+MJa/1vf/tbvfDCC1q3bp0SExOHncXlcikpKSmwzZ49O5y3AgAA7iJGfUro2rVrevLJJ+X3+7V3794R11ZWVqq3tzewdXV1jdGUAABgrMWGszg5OVkxMTHyer1B+71er1JSUkIek5KSckvrP4uVX/3qV3rvvfdGvLsiSfHx8YqPjw9nfAAAcJcK6w5LXFycsrKy5Ha7A/t8Pp/cbrfy8vJCHpOXlxe0XpKampqC1n8WK+fPn9ePf/xjTZ8+PZyxAADAOBfWHRZJcjqdKikpUXZ2tnJyclRdXa3+/n6VlpZKkoqLi5WWliaXyyVJ2rhxo/Lz87Vz506tXbtW9fX1OnHihOrq6iRdj5U/+ZM/UXt7u9566y0NDQ0Fnm+5//77FRcXd7veKwAAuEuFHSxFRUW6dOmStm7dKo/Ho8zMTDU2NgYerO3s7FR09I0bNytXrtT+/fu1ZcsWbd68WQsWLNChQ4e0ePFiSdKFCxd0+PBhSVJmZmbQ93r//ff18MMPj/KtAQCA8SLsYJEkh8Mhh8MR8rXm5uab9hUWFqqwsDDkeqvVKr/fP5oxAADABGHUp4QAAABCIVgAAIDxCBYAAGA8ggUAABiPYAEAAMYjWAAAgPEIFgAAYDyCBQAAGI9gAQAAxiNYAACA8QgWAABgPIIFAAAYj2ABAADGI1gAAIDxCBYAAGA8ggUAABiPYAEAAMYjWAAAgPEIFgAAYDyCBQAAGI9gAQAAxiNYAACA8QgWAABgPIIFAAAYj2ABAADGI1gAAIDxCBYAAGA8ggUAABiPYAEAAMYjWAAAgPEIFgAAYDyCBQAAGI9gAQAAxiNYAACA8QgWAABgvFEFS01NjaxWqxISEpSbm6vW1tYR1x84cEALFy5UQkKClixZoiNHjgS97vf7tXXrVs2cOVOTJk2SzWbT+fPnRzMaAAAYh8IOloaGBjmdTlVVVam9vV0ZGRmy2+3q7u4Ouf7YsWNat26dNmzYoJMnT6qgoEAFBQU6depUYM13vvMdvfrqq6qtrdVPfvITTZkyRXa7Xb/97W9H/84AAMC4EXaw7Nq1S2VlZSotLVV6erpqa2s1efJk7du3L+T63bt3a82aNaqoqNCiRYu0bds2LV++XHv27JF0/e5KdXW1tmzZoscff1xLly7V9773PV28eFGHDh36Qm8OAACMD7HhLB4cHFRbW5sqKysD+6Kjo2Wz2dTS0hLymJaWFjmdzqB9drs9ECOffPKJPB6PbDZb4PWkpCTl5uaqpaVFTz31VMjzDgwMaGBgIPB1b2+vJKmvry+ct3RLfANXb/s5I20012k8XgeJa/EZrsN1XIfrRvvPUq7FdVyH8M/r9/tHXBdWsPT09GhoaEgWiyVov8Vi0dmzZ0Me4/F4Qq73eDyB1z/bN9yaUFwul1566aWb9s+ePfvz3wiUVB3pCczBtbiO63Ad1+E6rsMNXIvr7vR1uHz5spKSkoZ9PaxgMUllZWXQnRufz6df//rXmj59uqKioiI42ej19fVp9uzZ6urqUmJiYqTHiRiuw3Vchxu4FtdxHa7jOtwwHq6F3+/X5cuXlZqaOuK6sIIlOTlZMTEx8nq9Qfu9Xq9SUlJCHpOSkjLi+s/+1+v1aubMmUFrMjMzh50lPj5e8fHxQfvuvffeW30rRktMTLxr/493O3EdruM63MC1uI7rcB3X4Ya7/VqMdGflM2E9dBsXF6esrCy53e7APp/PJ7fbrby8vJDH5OXlBa2XpKampsD6uXPnKiUlJWhNX1+ffvKTnwx7TgAAMLGE/SMhp9OpkpISZWdnKycnR9XV1erv71dpaakkqbi4WGlpaXK5XJKkjRs3Kj8/Xzt37tTatWtVX1+vEydOqK6uTpIUFRWlTZs26e/+7u+0YMECzZ07Vy+++KJSU1NVUFBw+94pAAC4a4UdLEVFRbp06ZK2bt0qj8ejzMxMNTY2Bh6a7ezsVHT0jRs3K1eu1P79+7VlyxZt3rxZCxYs0KFDh7R48eLAmr/+679Wf3+/nnnmGf3P//yPVq9ercbGRiUkJNyGt3j3iI+PV1VV1U0/6ppouA7XcR1u4Fpcx3W4jutww0S6FlH+z/scEQAAQITxdwkBAADjESwAAMB4BAsAADAewQIAAIxHsBiipqZGVqtVCQkJys3NVWtra6RHGnMffPCBvv71rys1NVVRUVET9i+/dLlcWrFihaZNm6YZM2aooKBA586di/RYY27v3r1aunRp4Bdi5eXl6Z133on0WBG3ffv2wK+DmGj+9m//VlFRUUHbwoULIz1WRFy4cEHf/OY3NX36dE2aNElLlizRiRMnIj3WHUWwGKChoUFOp1NVVVVqb29XRkaG7Ha7uru7Iz3amOrv71dGRoZqamoiPUpEHT16VOXl5Tp+/Liampp07do1Pfroo+rv74/0aGNq1qxZ2r59u9ra2nTixAn94R/+oR5//HGdPn060qNFzIcffqjvfve7Wrp0aaRHiZivfOUr+u///u/A9l//9V+RHmnM/eY3v9GqVat0zz336J133tGZM2e0c+dO3XfffZEe7c7yI+JycnL85eXlga+Hhob8qampfpfLFcGpIkuS/+DBg5Eewwjd3d1+Sf6jR49GepSIu++++/z/9E//FOkxIuLy5cv+BQsW+Juamvz5+fn+jRs3RnqkMVdVVeXPyMiI9BgR98ILL/hXr14d6THGHHdYImxwcFBtbW2y2WyBfdHR0bLZbGppaYngZDBFb2+vJOn++++P8CSRMzQ0pPr6evX390/Yv7KjvLxca9euDfpnxUR0/vx5paamat68eVq/fr06OzsjPdKYO3z4sLKzs1VYWKgZM2Zo2bJlev311yM91h1HsERYT0+PhoaGAr8p+DMWi0UejydCU8EUPp9PmzZt0qpVq4J+O/RE8dFHH2nq1KmKj4/Xs88+q4MHDyo9PT3SY425+vp6tbe3B/7Kk4kqNzdXb775phobG7V371598skn+oM/+ANdvnw50qONqV/+8pfau3evFixYoHfffVd/9md/pueee07//M//HOnR7qiwfzU/gLFTXl6uU6dOTcif00vSgw8+qI6ODvX29urf/u3fVFJSoqNHj06oaOnq6tLGjRvV1NQ04f66kt/1ta99LfDnpUuXKjc3V3PmzNG//uu/asOGDRGcbGz5fD5lZ2fr5ZdfliQtW7ZMp06dUm1trUpKSiI83Z3DHZYIS05OVkxMjLxeb9B+r9erlJSUCE0FEzgcDr311lt6//33NWvWrEiPExFxcXGaP3++srKy5HK5lJGRod27d0d6rDHV1tam7u5uLV++XLGxsYqNjdXRo0f16quvKjY2VkNDQ5EeMWLuvfdeffnLX9bPf/7zSI8ypmbOnHlTtC9atGjc/3iMYImwuLg4ZWVlye12B/b5fD653e4J+7P6ic7v98vhcOjgwYN67733NHfu3EiPZAyfz6eBgYFIjzGmHnnkEX300Ufq6OgIbNnZ2Vq/fr06OjoUExMT6REj5sqVK/rFL36hmTNnRnqUMbVq1aqbftXBxx9/rDlz5kRoorHBj4QM4HQ6VVJSouzsbOXk5Ki6ulr9/f0qLS2N9Ghj6sqVK0H/pfTJJ5+oo6ND999/vx544IEITja2ysvLtX//fv3gBz/QtGnTAs8yJSUladKkSRGebuxUVlbqa1/7mh544AFdvnxZ+/fvV3Nzs959991Ijzampk2bdtPzS1OmTNH06dMn3HNNf/VXf6Wvf/3rmjNnji5evKiqqirFxMRo3bp1kR5tTD3//PNauXKlXn75ZT355JNqbW1VXV2d6urqIj3anRXpjynhutdee83/wAMP+OPi4vw5OTn+48ePR3qkMff+++/7Jd20lZSURHq0MRXqGkjyv/HGG5EebUw9/fTT/jlz5vjj4uL8X/rSl/yPPPKI/0c/+lGkxzLCRP1Yc1FRkX/mzJn+uLg4f1pamr+oqMj/85//PNJjRcQPf/hD/+LFi/3x8fH+hQsX+uvq6iI90h0X5ff7/RFqJQAAgFvCMywAAMB4BAsAADAewQIAAIxHsAAAAOMRLAAAwHgECwAAMB7BAgAAjEewAAAA4xEsAADAeAQLAAAwHsECAACMR7AAAADj/R+yBa26LAiDhwAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "game = ConnectFour()\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "state = game.get_initial_state()\n",
    "state = game.get_next_state(state, 2, -1)\n",
    "state = game.get_next_state(state, 4, -1)\n",
    "state = game.get_next_state(state, 6, 1)\n",
    "state = game.get_next_state(state, 6, 1)\n",
    "\n",
    "print(state)\n",
    "\n",
    "encoded_state = game.get_encoded_state(state)\n",
    "\n",
    "print(encoded_state)\n",
    "\n",
    "tensor_state = torch.tensor(encoded_state, device=device).unsqueeze(0)\n",
    "\n",
    "model = ResNet(game, 9, 128, device=device)\n",
    "model.load_state_dict(torch.load(f'../models/{game}/model_7.pt', map_location=device), strict=False)\n",
    "model.eval()\n",
    "\n",
    "policy, value = model(tensor_state)\n",
    "value = value.item()\n",
    "policy = torch.softmax(policy, axis=1).squeeze(0).detach().cpu().numpy()\n",
    "\n",
    "print(value)\n",
    "\n",
    "plt.bar(range(game.action_size), policy)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fc005069-4b4f-41cb-97fb-65bad1af58cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    ''' A Monte Carlo Tree Search node '''\n",
    "    \n",
    "    def __init__(self, game, args, state, parent=None, action_taken=None, prior=0, visit_count=0):\n",
    "        self.game = game\n",
    "        self.args = args\n",
    "        self.state = state\n",
    "        self.parent = parent\n",
    "        self.action_taken = action_taken\n",
    "        self.prior = prior\n",
    "\n",
    "        self.children = []\n",
    "\n",
    "        self.visit_count = visit_count\n",
    "        self.value_sum = 0\n",
    "\n",
    "    def is_expanded(self):\n",
    "        ''' Check if the node already has children. Since we expand all children of a node at once, we check for >0 children '''\n",
    "        return len(self.children) > 0\n",
    "\n",
    "    def select(self):\n",
    "        ''' Select a child to explore '''\n",
    "        best_child = None\n",
    "        best_ucb = -np.inf\n",
    "\n",
    "        for child in self.children:\n",
    "            ucb = self.get_ucb(child)\n",
    "            if ucb > best_ucb:\n",
    "                best_child = child\n",
    "                best_ucb = ucb\n",
    "\n",
    "        return best_child\n",
    "\n",
    "    def get_ucb(self, child):\n",
    "        ''' Get how promising a move is from the opponent's perspective, normalized on [0,1] '''\n",
    "        if child.visit_count == 0:\n",
    "            q = 0\n",
    "        else:\n",
    "            q = 1 - ((child.value_sum / child.visit_count) + 1) / 2 \n",
    "\n",
    "        return q + self.args['C'] * (math.sqrt(self.visit_count) / (child.visit_count + 1)) * child.prior\n",
    "\n",
    "    def expand(self, policy):\n",
    "        ''' Expand a node by adding all legal child moves '''\n",
    "        for action, prob in enumerate(policy):\n",
    "            if prob > 0:\n",
    "                child_state = self.state.copy()\n",
    "                child_state = self.game.get_next_state(child_state, action, 1)\n",
    "                child_state = self.game.change_perspective(child_state, player=-1)\n",
    "        \n",
    "                child = Node(self.game, self.args, child_state, self, action, prob)\n",
    "                self.children.append(child)\n",
    "\n",
    "    def backpropagate(self, value):\n",
    "        ''' Propagate value sums and visit counts from children to all parents '''\n",
    "        self.value_sum += value\n",
    "        self.visit_count += 1\n",
    "\n",
    "        # flip value for opponent (parent)\n",
    "        value = self.game.get_opponent_value(value)\n",
    "\n",
    "        if self.parent is not None:\n",
    "            self.parent.backpropagate(value)\n",
    "\n",
    "class MCTS:\n",
    "    ''' A Monte Carlo Tree Search '''\n",
    "    \n",
    "    def __init__(self, game, args, model):\n",
    "        self.game = game\n",
    "        self.args = args\n",
    "        self.model = model\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def search(self, state):\n",
    "        ''' Expand and explore the MCTS and update value sums and visit counts '''\n",
    "        # DEFINE ROOT\n",
    "        root = Node(self.game, self.args, state, visit_count=1)\n",
    "\n",
    "        # add some random noise to policy to increase exploration\n",
    "        policy, _ = self.model(\n",
    "            torch.tensor(self.game.get_encoded_state(state), device=self.model.device).unsqueeze(0)\n",
    "        )\n",
    "        policy = torch.softmax(policy, axis=1).squeeze(0).cpu().numpy()\n",
    "        policy = ((1 - self.args['dirichlet_epsilon']) * \n",
    "                    policy + \n",
    "                    self.args['dirichlet_epsilon'] * \n",
    "                    np.random.dirichlet([self.args['dirichlet_alpha']] * \n",
    "                    self.game.action_size\n",
    "                ))\n",
    "        valid_moves = self.game.get_valid_moves(state)\n",
    "        policy *= valid_moves\n",
    "        policy /= np.sum(policy)\n",
    "        root.expand(policy)\n",
    "        \n",
    "        for search in range(self.args['num_searches']):\n",
    "            node = root\n",
    "            \n",
    "            # SELECTION\n",
    "            while node.is_expanded():\n",
    "                node = node.select()\n",
    "                \n",
    "            # check for end of game\n",
    "            value, is_terminal = self.game.get_value_and_terminated(node.state, node.action_taken)\n",
    "\n",
    "            # flip parent value\n",
    "            value = self.game.get_opponent_value(value)\n",
    "\n",
    "            if not is_terminal:\n",
    "                # get output from model\n",
    "                policy, value = self.model(\n",
    "                    torch.tensor(self.game.get_encoded_state(node.state), device=self.model.device).unsqueeze(0)\n",
    "                )\n",
    "\n",
    "                # change policy to proabability distribution\n",
    "                policy = torch.softmax(policy, axis=1).squeeze(0).cpu().numpy()\n",
    "\n",
    "                # mask out illegal moves\n",
    "                valid_moves = self.game.get_valid_moves(node.state)\n",
    "                policy *= valid_moves\n",
    "\n",
    "                # readjust back to probability distribution\n",
    "                policy /= np.sum(policy)\n",
    "\n",
    "                # get the value as a number from singleton tensor\n",
    "                value = value.item()\n",
    "                \n",
    "                # EXPANSION\n",
    "                node.expand(policy)\n",
    "    \n",
    "            # BACKPROP\n",
    "            node.backpropagate(value)\n",
    "\n",
    "        # probabilities of action being good\n",
    "        action_probs = np.zeros(self.game.action_size)\n",
    "        for child in root.children:\n",
    "            action_probs[child.action_taken] = child.visit_count\n",
    "\n",
    "        action_probs /= np.sum(action_probs)\n",
    "        return action_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "919a7ac9-28a6-4c49-b069-004b79290181",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlphaZero:\n",
    "    ''' AlphaZero class for self-play and training '''\n",
    "    \n",
    "    def __init__(self, model, optimizer, game, args):\n",
    "        ''' Initialize the AlphaZero instance '''\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.game = game\n",
    "        self.args = args\n",
    "        self.mcts = MCTS(game, args, model)\n",
    "\n",
    "    def self_play(self):\n",
    "        ''' Run a single self-play game until completion and generate outcome-appended training data '''\n",
    "        \n",
    "        memory = []\n",
    "        player = 1\n",
    "        state = self.game.get_initial_state()\n",
    "\n",
    "        while True:\n",
    "            # get the current state and action probabilities from MCTS\n",
    "            neutral_state = self.game.change_perspective(state, player)\n",
    "            action_probs = self.mcts.search(neutral_state)\n",
    "\n",
    "            # record a game snapshot\n",
    "            memory.append((neutral_state, action_probs, player))\n",
    "\n",
    "            # randomly sample an action from the distribution\n",
    "            adjusted_action_probs = action_probs ** (1 / self.args['temperature']) # add flexibility for exploration / exploitation\n",
    "            adjusted_action_probs /= np.sum(adjusted_action_probs)\n",
    "            action = np.random.choice(self.game.action_size, p=adjusted_action_probs)\n",
    "\n",
    "            # get the next state given the chosen action\n",
    "            state = self.game.get_next_state(state, action, player)\n",
    "\n",
    "            # check for game completion\n",
    "            value, is_terminal = self.game.get_value_and_terminated(state, action)\n",
    "\n",
    "            if is_terminal:\n",
    "                # get all states and policies from the game and append the outcome\n",
    "                return [(\n",
    "                    self.game.get_encoded_state(h_state),\n",
    "                    h_action_probs,\n",
    "                    value if h_player == player else self.game.get_opponent_value(value)\n",
    "                ) for h_state, h_action_probs, h_player in memory]\n",
    "\n",
    "            # swap the player and loop\n",
    "            player = self.game.get_opponent(player)\n",
    "\n",
    "    def train(self, memory):\n",
    "        ''' Train the model '''\n",
    "\n",
    "        # randomize training data\n",
    "        random.shuffle(memory)\n",
    "        \n",
    "        for batch_i in range(0, len(memory), self.args['batch_size']):\n",
    "            # sample a batch from training data\n",
    "            sample = memory[batch_i : min(len(memory) - 1, batch_i + self.args['batch_size'])]\n",
    "\n",
    "            # transpose list of tuples to independent lists\n",
    "            state, policy_targets, value_targets = zip(*sample)\n",
    "\n",
    "            # convert to numpy arrays\n",
    "            state = np.array(state)\n",
    "            policy_targets = np.array(policy_targets)\n",
    "            value_targets = np.array(value_targets).reshape(-1, 1) # wrap each value in its own array for simplicity later\n",
    "\n",
    "            # convert to tensors\n",
    "            state = torch.tensor(state, dtype=torch.float32, device=self.model.device)\n",
    "            policy_targets = torch.tensor(policy_targets, dtype=torch.float32, device=self.model.device)\n",
    "            value_targets = torch.tensor(value_targets, dtype=torch.float32, device=self.model.device)\n",
    "\n",
    "            # get model outputs\n",
    "            out_policy, out_value = self.model(state)\n",
    "\n",
    "            # get loss\n",
    "            policy_loss = F.cross_entropy(out_policy, policy_targets)\n",
    "            value_loss = F.mse_loss(out_value, value_targets)\n",
    "            loss = policy_loss + value_loss\n",
    "\n",
    "            # minimize loss via backpropagation\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    def learn(self):\n",
    "        ''' Generate self-play training data and train the model on it '''\n",
    "        \n",
    "        for iter in range(self.args['num_iters']):\n",
    "            memory = []\n",
    "            \n",
    "            self.model.eval()\n",
    "            for self_play_iter in trange(self.args['num_self_play_iters']):\n",
    "                memory += self.self_play()\n",
    "\n",
    "            self.model.train()\n",
    "            for epoch in trange(self.args['num_epochs']):\n",
    "                self.train(memory)\n",
    "\n",
    "            torch.save(self.model.state_dict(), f'../models/{self.game}/model_{iter}.pt')\n",
    "            torch.save(self.optimizer.state_dict(), f'../models/{self.game}/optimizer_{iter}.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b77aafbd-5988-42c8-a8c0-fae0fbee464c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MCTSParallel:\n",
    "    ''' A parallelized Monte Carlo Tree Search '''\n",
    "    \n",
    "    def __init__(self, game, args, model):\n",
    "        self.game = game\n",
    "        self.args = args\n",
    "        self.model = model\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def search(self, states, sp_games):\n",
    "        ''' Expand and explore the MCTS and update value sums and visit counts '''\n",
    "\n",
    "        # add some random noise to policy to increase exploration\n",
    "        policy, _ = self.model(torch.tensor(self.game.get_encoded_state(states), device=self.model.device))\n",
    "        policy = torch.softmax(policy, axis=1).cpu().numpy()\n",
    "        policy = ((1 - self.args['dirichlet_epsilon']) * \n",
    "                    policy + \n",
    "                    self.args['dirichlet_epsilon'] * \n",
    "                    np.random.dirichlet([self.args['dirichlet_alpha']] * \n",
    "                    self.game.action_size,\n",
    "                    size=policy.shape[0]\n",
    "                ))\n",
    "\n",
    "        for i, spg in enumerate(sp_games):\n",
    "            # get policy for this self-play game\n",
    "            spg_policy = policy[i]\n",
    "\n",
    "            # mask illegal moves out of policy\n",
    "            valid_moves = self.game.get_valid_moves(states[i])\n",
    "            spg_policy *= valid_moves\n",
    "            spg_policy /= np.sum(spg_policy)\n",
    "    \n",
    "            spg.root = Node(self.game, self.args, states[i], visit_count=1)\n",
    "            \n",
    "            spg.root.expand(spg_policy)\n",
    "        \n",
    "        for search in range(self.args['num_searches']):\n",
    "            for spg in sp_games:\n",
    "                spg.node = None\n",
    "                node = spg.root\n",
    "                \n",
    "                # SELECTION\n",
    "                while node.is_expanded():\n",
    "                    node = node.select()\n",
    "                    \n",
    "                # check for end of game\n",
    "                value, is_terminal = self.game.get_value_and_terminated(node.state, node.action_taken)\n",
    "    \n",
    "                # flip parent value\n",
    "                value = self.game.get_opponent_value(value)\n",
    "\n",
    "                # backpropagate value sums and visit counts if game ended\n",
    "                if is_terminal:\n",
    "                    node.backpropagate(value)\n",
    "\n",
    "                # otherwise, store the current node\n",
    "                else:\n",
    "                    spg.node = node\n",
    "\n",
    "            expandable_sp_games = [map_i for map_i in range(len(sp_games)) if sp_games[map_i].node is not None]\n",
    "\n",
    "            if len(expandable_sp_games) > 0:\n",
    "                states = np.stack([sp_games[map_i].node.state for map_i in expandable_sp_games])\n",
    "\n",
    "                # get output from model\n",
    "                policy, value = self.model(\n",
    "                    torch.tensor(self.game.get_encoded_state(states), device=self.model.device)\n",
    "                )\n",
    "\n",
    "                # change policy to proabability distribution\n",
    "                policy = torch.softmax(policy, axis=1).cpu().numpy()\n",
    "\n",
    "                # get value as numpy\n",
    "                value = value.cpu().numpy()\n",
    "\n",
    "            for i, map_i in enumerate(expandable_sp_games):\n",
    "                # get the current node for the self-play game\n",
    "                node = sp_games[map_i].node\n",
    "                \n",
    "                # get policy and value from self-play game\n",
    "                spg_policy, spg_value = policy[i], value[i]\n",
    "                \n",
    "                 # mask out illegal moves\n",
    "                valid_moves = self.game.get_valid_moves(node.state)\n",
    "                spg_policy *= valid_moves\n",
    "\n",
    "                # readjust back to probability distribution\n",
    "                spg_policy /= np.sum(spg_policy)\n",
    "                \n",
    "                # expand the node\n",
    "                node.expand(spg_policy)\n",
    "\n",
    "                # backpropagate values and visit counts\n",
    "                node.backpropagate(spg_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "721d5076-3088-441a-9871-2795d7d3e99c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlphaZeroParallel:\n",
    "    ''' Parallelized AlphaZero class for self-play and training '''\n",
    "    \n",
    "    def __init__(self, model, optimizer, game, args):\n",
    "        ''' Initialize the AlphaZero instance '''\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.game = game\n",
    "        self.args = args\n",
    "        self.mcts = MCTSParallel(game, args, model)\n",
    "\n",
    "    def self_play(self):\n",
    "        ''' Run a single self-play game until completion and generate outcome-appended training data '''\n",
    "        \n",
    "        return_memory = []\n",
    "        player = 1\n",
    "        sp_games = [SPG(self.game) for _ in range(self.args['num_parallel_games'])]\n",
    "\n",
    "        while len(sp_games) > 0:\n",
    "            # get states from all games\n",
    "            states = np.stack([spg.state for spg in sp_games])\n",
    "            \n",
    "            # get neutral states from all games\n",
    "            neutral_states = self.game.change_perspective(states, player)\n",
    "\n",
    "            # MCTS call\n",
    "            self.mcts.search(neutral_states, sp_games)\n",
    "\n",
    "            # loop games in reverse to avoid issues when removing terminated games from list\n",
    "            for i in range(len(sp_games))[::-1]:\n",
    "                spg = sp_games[i]\n",
    "                \n",
    "                # probabilities of action being good\n",
    "                action_probs = np.zeros(self.game.action_size)\n",
    "                for child in spg.root.children:\n",
    "                    action_probs[child.action_taken] = child.visit_count\n",
    "                action_probs /= np.sum(action_probs)\n",
    "    \n",
    "                # record a game snapshot\n",
    "                spg.memory.append((spg.root.state, action_probs, player))\n",
    "    \n",
    "                # randomly sample an action from the distribution\n",
    "                adjusted_action_probs = action_probs ** (1 / self.args['temperature']) # add flexibility for exploration / exploitation\n",
    "                adjusted_action_probs /= np.sum(adjusted_action_probs)\n",
    "                action = np.random.choice(self.game.action_size, p=adjusted_action_probs)\n",
    "    \n",
    "                # get the next state given the chosen action\n",
    "                spg.state = self.game.get_next_state(spg.state, action, player)\n",
    "    \n",
    "                # check for game completion\n",
    "                value, is_terminal = self.game.get_value_and_terminated(spg.state, action)\n",
    "    \n",
    "                if is_terminal:\n",
    "                    # get all states and policies from the games and append the outcomes\n",
    "                    for h_neutral_state, h_action_probs, h_player in spg.memory:\n",
    "                        h_outcome = value if h_player == player else self.game.get_opponent_value(value)\n",
    "                        return_memory.append((\n",
    "                            self.game.get_encoded_state(h_neutral_state),\n",
    "                            h_action_probs,\n",
    "                            h_outcome\n",
    "                        ))\n",
    "    \n",
    "                    del sp_games[i]\n",
    "\n",
    "            # swap the player and loop\n",
    "            player = self.game.get_opponent(player)\n",
    "\n",
    "        return return_memory\n",
    "\n",
    "    def train(self, memory):\n",
    "        ''' Train the model '''\n",
    "\n",
    "        # randomize training data\n",
    "        random.shuffle(memory)\n",
    "        \n",
    "        for batch_i in range(0, len(memory), self.args['batch_size']):\n",
    "            # sample a batch from training data\n",
    "            sample = memory[batch_i : min(len(memory) - 1, batch_i + self.args['batch_size'])]\n",
    "\n",
    "            # transpose list of tuples to independent lists\n",
    "            state, policy_targets, value_targets = zip(*sample)\n",
    "\n",
    "            # convert to numpy arrays\n",
    "            state = np.array(state)\n",
    "            policy_targets = np.array(policy_targets)\n",
    "            value_targets = np.array(value_targets).reshape(-1, 1) # wrap each value in its own array for simplicity later\n",
    "\n",
    "            # convert to tensors\n",
    "            state = torch.tensor(state, dtype=torch.float32, device=self.model.device)\n",
    "            policy_targets = torch.tensor(policy_targets, dtype=torch.float32, device=self.model.device)\n",
    "            value_targets = torch.tensor(value_targets, dtype=torch.float32, device=self.model.device)\n",
    "\n",
    "            # get model outputs\n",
    "            out_policy, out_value = self.model(state)\n",
    "\n",
    "            # get loss\n",
    "            policy_loss = F.cross_entropy(out_policy, policy_targets)\n",
    "            value_loss = F.mse_loss(out_value, value_targets)\n",
    "            loss = policy_loss + value_loss\n",
    "\n",
    "            # minimize loss via backpropagation\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    def learn(self):\n",
    "        ''' Generate self-play training data and train the model on it '''\n",
    "        \n",
    "        for iter in range(self.args['num_iters']):\n",
    "            memory = []\n",
    "            \n",
    "            self.model.eval()\n",
    "            for self_play_iter in trange(self.args['num_self_play_iters'] // self.args['num_parallel_games']):\n",
    "                memory += self.self_play()\n",
    "\n",
    "            self.model.train()\n",
    "            for epoch in trange(self.args['num_epochs']):\n",
    "                self.train(memory)\n",
    "\n",
    "            torch.save(self.model.state_dict(), f'../models/{self.game}/model_{iter}.pt')\n",
    "            torch.save(self.optimizer.state_dict(), f'../models/{self.game}/optimizer_{iter}.pt')\n",
    "\n",
    "class SPG:\n",
    "    ''' A self-play game '''\n",
    "    def __init__(self, game):\n",
    "        self.state = game.get_initial_state()\n",
    "        self.memory = []\n",
    "        self.root = None\n",
    "        self.node = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5f989da5-eba5-4beb-8edd-5eeff772d7ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ff2e8694f604ddd8c2e85c75b66a2a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "NameError",
     "evalue": "name 'Node' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 22\u001b[0m\n\u001b[1;32m      8\u001b[0m args \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m2\u001b[39m,\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_searches\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m600\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdirichlet_alpha\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0.3\u001b[39m\n\u001b[1;32m     19\u001b[0m }\n\u001b[1;32m     21\u001b[0m alphazero \u001b[38;5;241m=\u001b[39m AlphaZeroParallel(model, optimizer, game, args)\n\u001b[0;32m---> 22\u001b[0m \u001b[43malphazero\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[11], line 114\u001b[0m, in \u001b[0;36mAlphaZeroParallel.learn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m self_play_iter \u001b[38;5;129;01min\u001b[39;00m trange(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_self_play_iters\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_parallel_games\u001b[39m\u001b[38;5;124m'\u001b[39m]):\n\u001b[0;32m--> 114\u001b[0m     memory \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_play\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m trange(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_epochs\u001b[39m\u001b[38;5;124m'\u001b[39m]):\n",
      "Cell \u001b[0;32mIn[11], line 27\u001b[0m, in \u001b[0;36mAlphaZeroParallel.self_play\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     24\u001b[0m neutral_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgame\u001b[38;5;241m.\u001b[39mchange_perspective(states, player)\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# MCTS call\u001b[39;00m\n\u001b[0;32m---> 27\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmcts\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msearch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mneutral_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msp_games\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# loop games in reverse to avoid issues when removing terminated games from list\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(sp_games))[::\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]:\n",
      "File \u001b[0;32m~/Desktop/Pomona/CS158/alphazero/venv/lib/python3.8/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[10], line 33\u001b[0m, in \u001b[0;36mMCTSParallel.search\u001b[0;34m(self, states, sp_games)\u001b[0m\n\u001b[1;32m     30\u001b[0m     spg_policy \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m valid_moves\n\u001b[1;32m     31\u001b[0m     spg_policy \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msum(spg_policy)\n\u001b[0;32m---> 33\u001b[0m     spg\u001b[38;5;241m.\u001b[39mroot \u001b[38;5;241m=\u001b[39m \u001b[43mNode\u001b[49m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgame, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs, states[i], visit_count\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     35\u001b[0m     spg\u001b[38;5;241m.\u001b[39mroot\u001b[38;5;241m.\u001b[39mexpand(spg_policy)\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m search \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_searches\u001b[39m\u001b[38;5;124m'\u001b[39m]):\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Node' is not defined"
     ]
    }
   ],
   "source": [
    "game = ConnectFour()\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = ResNet(game, 9, 128, device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=0.0001)\n",
    "\n",
    "args = {\n",
    "    'C': 2,\n",
    "    'num_searches': 600,\n",
    "    'num_iters': 8,\n",
    "    'num_self_play_iters': 500,\n",
    "    'num_parallel_games': 100,\n",
    "    'num_epochs': 4,\n",
    "    'batch_size': 128,\n",
    "    'temperature': 1.25,\n",
    "    'dirichlet_epsilon': 0.25,\n",
    "    'dirichlet_alpha': 0.3\n",
    "}\n",
    "\n",
    "alphazero = AlphaZeroParallel(model, optimizer, game, args)\n",
    "alphazero.learn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5dcd3a3-ac48-478b-a375-c3bb3a887472",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]]\n",
      "Valid Moves: [0, 1, 2, 3, 4, 5, 6]\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "1:  3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0.]]\n",
      "[0.14833333 0.15       0.13333333 0.13833333 0.14666667 0.13166667\n",
      " 0.15166667]\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  1.  0.  0. -1.]]\n",
      "Valid Moves: [0, 1, 2, 3, 4, 5, 6]\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "1:  2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  1.  1.  0.  0. -1.]]\n",
      "[0.165      0.18666667 0.12333333 0.13333333 0.13333333 0.12833333\n",
      " 0.13      ]\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0. -1.  1.  1.  0.  0. -1.]]\n",
      "Valid Moves: [0, 1, 2, 3, 4, 5, 6]\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "1:  4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0. -1.  1.  1.  1.  0. -1.]]\n"
     ]
    }
   ],
   "source": [
    "game = ConnectFour()\n",
    "player = 1\n",
    "\n",
    "args = {\n",
    "    'C': 2,\n",
    "    'num_searches': 600,\n",
    "    'dirichlet_epsilon': 0.0,\n",
    "    'dirichlet_alpha': 0.3\n",
    "}\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = ResNet(game, 9, 128, device)\n",
    "model.load_state_dict(torch.load(f'../models/{game}/model_7.pt', map_location=device), strict=False)\n",
    "model.eval()\n",
    "\n",
    "mcts = MCTS(game, args, model)\n",
    "\n",
    "state = game.get_initial_state()\n",
    "\n",
    "while True:\n",
    "    print(state)\n",
    "\n",
    "    if player == 1:\n",
    "        valid_moves = game.get_valid_moves(state)\n",
    "        print('Valid Moves:' , [i for i in range(game.action_size) if valid_moves[i] == 1])\n",
    "        action = int(input(f'{player}: '))\n",
    "    \n",
    "        if valid_moves[action] == 0:\n",
    "            print('Invalid move')\n",
    "            continue\n",
    "    else:\n",
    "        neutral_state = game.change_perspective(state, player)\n",
    "        mcts_probs = mcts.search(neutral_state)\n",
    "        print(mcts_probs)\n",
    "        action = np.argmax(mcts_probs)\n",
    "\n",
    "    state = game.get_next_state(state, action, player)\n",
    "\n",
    "    value, is_terminal = game.get_value_and_terminated(state, action)\n",
    "\n",
    "    if is_terminal:\n",
    "        print(state)\n",
    "        if value == 1:\n",
    "            print(player, 'won')\n",
    "        else:\n",
    "            print('Draw')\n",
    "        break\n",
    "\n",
    "    player = game.get_opponent(player)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e4eb825-e431-4ea0-998f-d1da412b5daf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8251b500-c17c-49af-899a-2d3c5f6420fe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
