{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7620efee-0dba-4939-ad3d-880af53233b5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-24T03:24:22.152294Z",
     "start_time": "2023-11-24T03:24:20.805377Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.24.4\n",
      "2.1.1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fa810dae710>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "import random\n",
    "from tqdm.notebook import trange\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "print(np.__version__)\n",
    "\n",
    "import torch\n",
    "print(torch.__version__)\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "2d894942-1528-4af0-8993-7d0d1e30b243",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TicTacToe:\n",
    "    ''' Game definition for TicTacToe '''\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.row_count = 3\n",
    "        self.col_count = 3\n",
    "        self.action_size = self.row_count * self.col_count\n",
    "\n",
    "    def __repr__(self):\n",
    "        return 'TicTacToe'\n",
    "\n",
    "    def get_initial_state(self):\n",
    "        ''' Get board with all zeros '''\n",
    "        return np.zeros((self.row_count, self.col_count))\n",
    "\n",
    "    def get_next_state(self, state, action, player):\n",
    "        ''' Get the next state given the given action by the given player '''\n",
    "        row = action // self.col_count\n",
    "        col = action % self.col_count\n",
    "        state[row, col] = player\n",
    "        return state\n",
    "\n",
    "    def get_valid_moves(self, state):\n",
    "        ''' Get all the legal moves in the position '''\n",
    "        return (state.reshape(-1) == 0).astype(np.uint8)\n",
    "\n",
    "    def check_win(self, state, action):\n",
    "        ''' Check if the given action has led to a win '''\n",
    "        if action == None:\n",
    "            return False\n",
    "        \n",
    "        row = action // self.col_count\n",
    "        col = action % self.col_count\n",
    "        player = state[row, col]\n",
    "\n",
    "        return (\n",
    "            np.sum(state[row, :]) == player * self.col_count or # rows\n",
    "            np.sum(state[:, col]) == player * self.row_count or # columns\n",
    "            np.sum(np.diag(state)) == player * self.row_count or # tl->br diag\n",
    "            np.sum(np.diag(np.flip(state, axis=0))) == player * self.row_count # tr->bl diag\n",
    "        )\n",
    "\n",
    "    def get_value_and_terminated(self, state, action):\n",
    "        ''' Get the value (win/tie) and if the game has terminated '''\n",
    "        if self.check_win(state, action):\n",
    "            return 1, True\n",
    "        if np.sum(self.get_valid_moves(state)) == 0:\n",
    "            return 0, True\n",
    "        return 0, False\n",
    "\n",
    "    def get_opponent(self, player):\n",
    "        ''' Get the opponent of the player '''\n",
    "        return -player\n",
    "\n",
    "    def get_opponent_value(self, value):\n",
    "        ''' Get the value from the opponent's perspective '''\n",
    "        return -value\n",
    "\n",
    "    def change_perspective(self, state, player):\n",
    "        ''' Transform the state to be from the opponent's perspective '''\n",
    "        return state * player\n",
    "\n",
    "    def get_encoded_state(self, state):\n",
    "        ''' Get the network-ready encoded state of the game '''\n",
    "        encoded_state = np.stack((state == -1, state == 0, state == 1)).astype(np.float32)\n",
    "\n",
    "        # check for batched states\n",
    "        if len(state.shape) == 3:\n",
    "            encoded_state = np.swapaxes(encoded_state, 0, 1)\n",
    "        \n",
    "        return encoded_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3bd0cd66-2da5-4e7e-97eb-becc878292d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    "    ''' The residual neural network implementation for AlphaZero '''\n",
    "    \n",
    "    def __init__(self, game, num_resblocks, num_hidden, device):\n",
    "        super().__init__()\n",
    "\n",
    "        # store the specified device for computation\n",
    "        self.device = device\n",
    "\n",
    "        # initial NN block\n",
    "        self.start_block = nn.Sequential(\n",
    "            nn.Conv2d(3, num_hidden, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(num_hidden),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        # shared portion of network between policy and value heads\n",
    "        self.backbone = nn.ModuleList(\n",
    "            [ResBlock(num_hidden) for _ in range(num_resblocks)]\n",
    "        )\n",
    "\n",
    "        # the portion of the network responsible for outputting policies\n",
    "        self.policy_head = nn.Sequential(\n",
    "            nn.Conv2d(num_hidden, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(32 * game.row_count * game.col_count, game.action_size)\n",
    "        )\n",
    "\n",
    "        # the portion of the network responsible for outputting values\n",
    "        self.value_head = nn.Sequential(\n",
    "            nn.Conv2d(num_hidden, 3, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(3),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(3 * game.row_count * game.col_count, 1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "        # send computation to the device\n",
    "        self.to(device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        ''' Feed forward implementation for this network '''\n",
    "        x = self.start_block(x)\n",
    "        for resblock in self.backbone:\n",
    "            x = resblock(x)\n",
    "        policy = self.policy_head(x)\n",
    "        value = self.value_head(x)\n",
    "\n",
    "        return policy, value\n",
    "\n",
    "class ResBlock(nn.Module):\n",
    "    ''' ResNet block '''\n",
    "    \n",
    "    def __init__(self, num_hidden):\n",
    "        ''' Initialize the block '''\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(num_hidden, num_hidden, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(num_hidden)\n",
    "        self.conv2 = nn.Conv2d(num_hidden, num_hidden, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(num_hidden)\n",
    "\n",
    "    def forward(self, x):\n",
    "        ''' Feed forward implementation for this block '''\n",
    "        residual = x\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.bn2(self.conv2(x))\n",
    "        x += residual\n",
    "        x = F.relu(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e888f91a-00c3-46bf-a9f5-d0685368479e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  0. -1.]\n",
      " [ 0. -1.  0.]\n",
      " [ 1.  0.  1.]]\n",
      "[[[0. 0. 1.]\n",
      "  [0. 1. 0.]\n",
      "  [0. 0. 0.]]\n",
      "\n",
      " [[1. 1. 0.]\n",
      "  [1. 0. 1.]\n",
      "  [0. 1. 0.]]\n",
      "\n",
      " [[0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [1. 0. 1.]]]\n",
      "0.953662633895874\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiMAAAGdCAYAAADAAnMpAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8WgzjOAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAda0lEQVR4nO3df0xd9f3H8RfQcmltoVXGpcWraHWrWAsVhNFG67KrbOvcurgNjRvkzvGH0g29mRF0wvzV21olLC0rtpNtUZuyOX9tdZjubtV1Yqggmz/bONOCunuBqNxKEzD33u8ffr0dFtrelvrmx/ORnGQ9/Zx738cz5ZnDuZAQjUajAgAAMJJoPQAAAJjeiBEAAGCKGAEAAKaIEQAAYIoYAQAApogRAABgihgBAACmiBEAAGBqhvUAxyMSiei9997T3LlzlZCQYD0OAAA4DtFoVAcPHtTChQuVmDj2/Y9JESPvvfeeXC6X9RgAAOAE9PT06Mwzzxzz7ydFjMydO1fSJyeTmppqPA0AADgeoVBILpcr9nV8LJMiRj791kxqaioxAgDAJHOsRyx4gBUAAJgiRgAAgCliBAAAmCJGAACAKWIEAACYIkYAAIApYgQAAJgiRgAAgCliBAAAmCJGAACAKWIEAACYIkYAAIApYgQAAJgiRgAAgKkZ1gMAAPBZ2dU7rEc4pv3rVlmPMGVwZwQAAJgiRgAAgCliBAAAmCJGAACAKWIEAACYIkYAAIApYgQAAJgiRgAAgCliBAAAmCJGAACAKWIEAACYIkYAAIApYgQAAJg6oRhpbGxUdna2UlJSVFRUpPb29qOu//DDD1VZWakFCxbI4XDoi1/8op555pkTGhgAAEwtM+I9oKWlRV6vV01NTSoqKlJDQ4NKSkq0d+9eZWRkHLF+eHhYV1xxhTIyMvTYY48pKytLBw4c0Lx588ZjfgAAMMnFHSP19fWqqKiQx+ORJDU1NWnHjh1qbm5WdXX1Eeubm5v1/vvv64UXXtDMmTMlSdnZ2Sc3NQAAmDLi+jbN8PCwOjo65Ha7D79AYqLcbrfa2tpGPebpp59WcXGxKisr5XQ6tWTJEq1du1bhcHjM9xkaGlIoFBqxAQCAqSmuGOnv71c4HJbT6Ryx3+l0KhAIjHrM22+/rccee0zhcFjPPPOM7rjjDj3wwAO65557xnwfn8+ntLS02OZyueIZEwAATCKn/NM0kUhEGRkZ2rJli/Lz81VaWqrbb79dTU1NYx5TU1OjgYGB2NbT03OqxwQAAEbiemYkPT1dSUlJCgaDI/YHg0FlZmaOesyCBQs0c+ZMJSUlxfZdcMEFCgQCGh4eVnJy8hHHOBwOORyOeEYDAACTVFx3RpKTk5Wfny+/3x/bF4lE5Pf7VVxcPOoxK1as0FtvvaVIJBLbt2/fPi1YsGDUEAEAANNL3N+m8Xq92rp1q373u9/pjTfe0A033KDBwcHYp2vKyspUU1MTW3/DDTfo/fffV1VVlfbt26cdO3Zo7dq1qqysHL+zAAAAk1bcH+0tLS1VX1+famtrFQgElJeXp9bW1thDrd3d3UpMPNw4LpdLzz77rG6++WYtXbpUWVlZqqqq0q233jp+ZwEAACathGg0GrUe4lhCoZDS0tI0MDCg1NRU63EAAKdYdvUO6xGOaf+6VdYjTHjH+/Wb300DAABMESMAAMAUMQIAAEwRIwAAwBQxAgAATBEjAADAFDECAABMESMAAMAUMQIAAEwRIwAAwBQxAgAATBEjAADAFDECAABMESMAAMAUMQIAAEwRIwAAwBQxAgAATBEjAADAFDECAABMESMAAMAUMQIAAEwRIwAAwBQxAgAATBEjAADAFDECAABMESMAAMAUMQIAAEwRIwAAwBQxAgAATBEjAADAFDECAABMESMAAMAUMQIAAEwRIwAAwBQxAgAATBEjAADAFDECAABMESMAAMAUMQIAAEwRIwAAwBQxAgAATBEjAADAFDECAABMESMAAMAUMQIAAEydUIw0NjYqOztbKSkpKioqUnt7+5hrf/vb3yohIWHElpKScsIDAwCAqSXuGGlpaZHX61VdXZ06OzuVm5urkpIS9fb2jnlMamqq/vvf/8a2AwcOnNTQAABg6og7Rurr61VRUSGPx6OcnBw1NTVp9uzZam5uHvOYhIQEZWZmxjan03lSQwMAgKkjrhgZHh5WR0eH3G734RdITJTb7VZbW9uYx3300Uc6++yz5XK59O1vf1uvvfbaUd9naGhIoVBoxAYAAKamuGKkv79f4XD4iDsbTqdTgUBg1GO+9KUvqbm5WU899ZQeeeQRRSIRLV++XO+8886Y7+Pz+ZSWlhbbXC5XPGMCAIBJ5JR/mqa4uFhlZWXKy8vTypUr9fjjj+sLX/iCHnzwwTGPqamp0cDAQGzr6ek51WMCAAAjM+JZnJ6erqSkJAWDwRH7g8GgMjMzj+s1Zs6cqWXLlumtt94ac43D4ZDD4YhnNAAAMEnFdWckOTlZ+fn58vv9sX2RSER+v1/FxcXH9RrhcFivvPKKFixYEN+kAABgSorrzogkeb1elZeXq6CgQIWFhWpoaNDg4KA8Ho8kqaysTFlZWfL5fJKku+66S1/+8pd13nnn6cMPP9SGDRt04MAB/fjHPx7fMwEAAJNS3DFSWlqqvr4+1dbWKhAIKC8vT62trbGHWru7u5WYePiGywcffKCKigoFAgHNnz9f+fn5euGFF5STkzN+ZwEAACathGg0GrUe4lhCoZDS0tI0MDCg1NRU63EAAKdYdvUO6xGOaf+6VdYjTHjH+/Wb300DAABMESMAAMAUMQIAAEwRIwAAwBQxAgAATBEjAADAFDECAABMESMAAMAUMQIAAEwRIwAAwBQxAgAATBEjAADAFDECAABMESMAAMAUMQIAAEwRIwAAwBQxAgAATBEjAADAFDECAABMESMAAMAUMQIAAEwRIwAAwBQxAgAATBEjAADAFDECAABMESMAAMAUMQIAAEwRIwAAwBQxAgAATBEjAADAFDECAABMESMAAMAUMQIAAEwRIwAAwBQxAgAATBEjAADAFDECAABMESMAAMAUMQIAAEwRIwAAwBQxAgAATBEjAADAFDECAABMESMAAMDUCcVIY2OjsrOzlZKSoqKiIrW3tx/Xcdu3b1dCQoJWr159Im8LAACmoLhjpKWlRV6vV3V1ders7FRubq5KSkrU29t71OP279+vn/3sZ7r00ktPeFgAADD1xB0j9fX1qqiokMfjUU5OjpqamjR79mw1NzePeUw4HNZ1112nO++8U+eee+5JDQwAAKaWuGJkeHhYHR0dcrvdh18gMVFut1ttbW1jHnfXXXcpIyND119//XG9z9DQkEKh0IgNAABMTXHFSH9/v8LhsJxO54j9TqdTgUBg1GN2796thx56SFu3bj3u9/H5fEpLS4ttLpcrnjEBAMAkcko/TXPw4EH98Ic/1NatW5Wenn7cx9XU1GhgYCC29fT0nMIpAQCApRnxLE5PT1dSUpKCweCI/cFgUJmZmUes/89//qP9+/frqquuiu2LRCKfvPGMGdq7d68WLVp0xHEOh0MOhyOe0QAAwCQV152R5ORk5efny+/3x/ZFIhH5/X4VFxcfsX7x4sV65ZVX1NXVFdu+9a1v6Stf+Yq6urr49gsAAIjvzogkeb1elZeXq6CgQIWFhWpoaNDg4KA8Ho8kqaysTFlZWfL5fEpJSdGSJUtGHD9v3jxJOmI/AACYnuKOkdLSUvX19am2tlaBQEB5eXlqbW2NPdTa3d2txER+sCsAADg+CdFoNGo9xLGEQiGlpaVpYGBAqamp1uMAAE6x7Ood1iMc0/51q6xHmPCO9+s3tzAAAIApYgQAAJgiRgAAgCliBAAAmCJGAACAKWIEAACYIkYAAIApYgQAAJgiRgAAgCliBAAAmCJGAACAKWIEAACYIkYAAIApYgQAAJgiRgAAgCliBAAAmCJGAACAKWIEAACYIkYAAIApYgQAAJgiRgAAgCliBAAAmCJGAACAKWIEAACYIkYAAIApYgQAAJgiRgAAgCliBAAAmCJGAACAKWIEAACYIkYAAIApYgQAAJgiRgAAgCliBAAAmCJGAACAqRnWA1jLrt5hPcIx7V+3ynoEAABOGe6MAAAAU8QIAAAwRYwAAABTxAgAADBFjAAAAFPECAAAMEWMAAAAU8QIAAAwRYwAAABTJxQjjY2Nys7OVkpKioqKitTe3j7m2scff1wFBQWaN2+eTjvtNOXl5enhhx8+4YEBAMDUEneMtLS0yOv1qq6uTp2dncrNzVVJSYl6e3tHXX/66afr9ttvV1tbm/7973/L4/HI4/Ho2WefPenhAQDA5Bd3jNTX16uiokIej0c5OTlqamrS7Nmz1dzcPOr6yy+/XN/5znd0wQUXaNGiRaqqqtLSpUu1e/fukx4eAABMfnHFyPDwsDo6OuR2uw+/QGKi3G632trajnl8NBqV3+/X3r17ddlll425bmhoSKFQaMQGAACmprhipL+/X+FwWE6nc8R+p9OpQCAw5nEDAwOaM2eOkpOTtWrVKm3cuFFXXHHFmOt9Pp/S0tJim8vlimdMAAAwiXwun6aZO3euurq6tGfPHt17773yer3atWvXmOtramo0MDAQ23p6ej6PMQEAgIEZ8SxOT09XUlKSgsHgiP3BYFCZmZljHpeYmKjzzjtPkpSXl6c33nhDPp9Pl19++ajrHQ6HHA5HPKMBAIBJKq47I8nJycrPz5ff74/ti0Qi8vv9Ki4uPu7XiUQiGhoaiuetAQDAFBXXnRFJ8nq9Ki8vV0FBgQoLC9XQ0KDBwUF5PB5JUllZmbKysuTz+SR98vxHQUGBFi1apKGhIT3zzDN6+OGHtXnz5vE9EwAAMCnFHSOlpaXq6+tTbW2tAoGA8vLy1NraGnuotbu7W4mJh2+4DA4O6sYbb9Q777yjWbNmafHixXrkkUdUWlo6fmcBAAAmrYRoNBq1HuJYQqGQ0tLSNDAwoNTU1HF97ezqHeP6eqfC/nWrrEcAgM8V/22eGo736ze/mwYAAJgiRgAAgCliBAAAmCJGAACAKWIEAACYIkYAAIApYgQAAJgiRgAAgCliBAAAmCJGAACAKWIEAACYIkYAAIApYgQAAJgiRgAAgCliBAAAmCJGAACAKWIEAACYIkYAAIApYgQAAJgiRgAAgCliBAAAmCJGAACAKWIEAACYIkYAAIApYgQAAJgiRgAAgCliBAAAmCJGAACAKWIEAACYIkYAAIApYgQAAJgiRgAAgCliBAAAmCJGAACAKWIEAACYIkYAAIApYgQAAJgiRgAAgCliBAAAmCJGAACAKWIEAACYIkYAAIApYgQAAJgiRgAAgCliBAAAmDqhGGlsbFR2drZSUlJUVFSk9vb2Mddu3bpVl156qebPn6/58+fL7XYfdT0AAJhe4o6RlpYWeb1e1dXVqbOzU7m5uSopKVFvb++o63ft2qVrr71Wf//739XW1iaXy6Urr7xS77777kkPDwAAJr+4Y6S+vl4VFRXyeDzKyclRU1OTZs+erebm5lHXP/roo7rxxhuVl5enxYsX69e//rUikYj8fv9JDw8AACa/uGJkeHhYHR0dcrvdh18gMVFut1ttbW3H9RqHDh3Sxx9/rNNPP33MNUNDQwqFQiM2AAAwNcUVI/39/QqHw3I6nSP2O51OBQKB43qNW2+9VQsXLhwRNJ/l8/mUlpYW21wuVzxjAgCASeRz/TTNunXrtH37dj3xxBNKSUkZc11NTY0GBgZiW09Pz+c4JQAA+DzNiGdxenq6kpKSFAwGR+wPBoPKzMw86rH333+/1q1bp7/+9a9aunTpUdc6HA45HI54RgMAAJNUXHdGkpOTlZ+fP+Lh008fRi0uLh7zuPvuu0933323WltbVVBQcOLTAgCAKSeuOyOS5PV6VV5eroKCAhUWFqqhoUGDg4PyeDySpLKyMmVlZcnn80mS1q9fr9raWm3btk3Z2dmxZ0vmzJmjOXPmjOOpAACAySjuGCktLVVfX59qa2sVCASUl5en1tbW2EOt3d3dSkw8fMNl8+bNGh4e1ne/+90Rr1NXV6df/OIXJzc9AACY9OKOEUlas2aN1qxZM+rf7dq1a8Sf9+/ffyJvAQAApgl+Nw0AADBFjAAAAFPECAAAMEWMAAAAU8QIAAAwRYwAAABTxAgAADBFjAAAAFPECAAAMEWMAAAAU8QIAAAwRYwAAABTxAgAADBFjAAAAFPECAAAMEWMAAAAU8QIAAAwRYwAAABTxAgAADBFjAAAAFPECAAAMEWMAAAAU8QIAAAwRYwAAABTxAgAADBFjAAAAFPECAAAMEWMAAAAU8QIAAAwRYwAAABTxAgAADBFjAAAAFPECAAAMEWMAAAAU8QIAAAwRYwAAABTxAgAADBFjAAAAFPECAAAMEWMAAAAU8QIAAAwRYwAAABTxAgAADBFjAAAAFMnFCONjY3Kzs5WSkqKioqK1N7ePuba1157TVdffbWys7OVkJCghoaGE50VAABMQXHHSEtLi7xer+rq6tTZ2anc3FyVlJSot7d31PWHDh3Sueeeq3Xr1ikzM/OkBwYAAFNL3DFSX1+viooKeTwe5eTkqKmpSbNnz1Zzc/Oo6y+55BJt2LBB11xzjRwOx0kPDAAAppa4YmR4eFgdHR1yu92HXyAxUW63W21tbeM+HAAAmPpmxLO4v79f4XBYTqdzxH6n06k333xz3IYaGhrS0NBQ7M+hUGjcXhsAAEwsE/LTND6fT2lpabHN5XJZjwQAAE6RuGIkPT1dSUlJCgaDI/YHg8FxfTi1pqZGAwMDsa2np2fcXhsAAEwsccVIcnKy8vPz5ff7Y/sikYj8fr+Ki4vHbSiHw6HU1NQRGwAAmJriemZEkrxer8rLy1VQUKDCwkI1NDRocHBQHo9HklRWVqasrCz5fD5Jnzz0+vrrr8f+97vvvquuri7NmTNH55133jieCgAAmIzijpHS0lL19fWptrZWgUBAeXl5am1tjT3U2t3drcTEwzdc3nvvPS1btiz25/vvv1/333+/Vq5cqV27dp38GQAAgEkt7hiRpDVr1mjNmjWj/t1nAyM7O1vRaPRE3gYAAEwDE/LTNAAAYPogRgAAgCliBAAAmCJGAACAKWIEAACYIkYAAIApYgQAAJgiRgAAgCliBAAAmCJGAACAKWIEAACYIkYAAIApYgQAAJgiRgAAgCliBAAAmCJGAACAKWIEAACYIkYAAIApYgQAAJgiRgAAgCliBAAAmCJGAACAKWIEAACYIkYAAIApYgQAAJgiRgAAgCliBAAAmCJGAACAKWIEAACYIkYAAIApYgQAAJgiRgAAgCliBAAAmCJGAACAKWIEAACYIkYAAIApYgQAAJgiRgAAgKkZ1gNgfGVX77Ae4aj2r1tlPQIAYILhzggAADBFjAAAAFPECAAAMEWMAAAAU8QIAAAwRYwAAABTJ/TR3sbGRm3YsEGBQEC5ubnauHGjCgsLx1z/hz/8QXfccYf279+v888/X+vXr9c3vvGNEx4amGwm+keupen5sWuuCzAxxH1npKWlRV6vV3V1ders7FRubq5KSkrU29s76voXXnhB1157ra6//nq9/PLLWr16tVavXq1XX331pIcHAACTX9x3Rurr61VRUSGPxyNJampq0o4dO9Tc3Kzq6uoj1v/yl7/U1772Nd1yyy2SpLvvvls7d+7Upk2b1NTUdJLjAwA+xZ0eTFZxxcjw8LA6OjpUU1MT25eYmCi32622trZRj2lra5PX6x2xr6SkRE8++eSY7zM0NKShoaHYnwcGBiRJoVAonnGPS2To0Li/5niL57wn+vmcims4GUz06yJNz2sz1a7LVDqfqXQu09mn/4yi0ehR18UVI/39/QqHw3I6nSP2O51Ovfnmm6MeEwgERl0fCATGfB+fz6c777zziP0ulyuecaeMtAbrCcbPVDqXqYZrMzFNtesylc5nKp3LqXbw4EGlpaWN+fcT8nfT1NTUjLibEolE9P777+uMM85QQkKC4WTHFgqF5HK51NPTo9TUVOtx8P+4LhMX12Zi4rpMXJPp2kSjUR08eFALFy486rq4YiQ9PV1JSUkKBoMj9geDQWVmZo56TGZmZlzrJcnhcMjhcIzYN2/evHhGNZeamjrh/08yHXFdJi6uzcTEdZm4Jsu1OdodkU/F9Wma5ORk5efny+/3x/ZFIhH5/X4VFxePekxxcfGI9ZK0c+fOMdcDAIDpJe5v03i9XpWXl6ugoECFhYVqaGjQ4OBg7NM1ZWVlysrKks/nkyRVVVVp5cqVeuCBB7Rq1Spt375dL730krZs2TK+ZwIAACaluGOktLRUfX19qq2tVSAQUF5enlpbW2MPqXZ3dysx8fANl+XLl2vbtm36+c9/rttuu03nn3++nnzySS1ZsmT8zmICcTgcqqurO+LbTLDFdZm4uDYTE9dl4pqK1yYheqzP2wAAAJxC/G4aAABgihgBAACmiBEAAGCKGAEAAKaIkXHU2Nio7OxspaSkqKioSO3t7dYjTXs+n0+XXHKJ5s6dq4yMDK1evVp79+61HgufsW7dOiUkJOimm26yHgWS3n33Xf3gBz/QGWecoVmzZumiiy7SSy+9ZD3WtBYOh3XHHXfonHPO0axZs7Ro0SLdfffdx/ydL5MFMTJOWlpa5PV6VVdXp87OTuXm5qqkpES9vb3Wo01rzz33nCorK/Xiiy9q586d+vjjj3XllVdqcHDQejT8vz179ujBBx/U0qVLrUeBpA8++EArVqzQzJkz9Ze//EWvv/66HnjgAc2fP996tGlt/fr12rx5szZt2qQ33nhD69ev13333aeNGzdajzYu+GjvOCkqKtIll1yiTZs2SfrkJ9O6XC795Cc/UXV1tfF0+FRfX58yMjL03HPP6bLLLrMeZ9r76KOPdPHFF+tXv/qV7rnnHuXl5amhocF6rGmturpa//znP/WPf/zDehT8j29+85tyOp166KGHYvuuvvpqzZo1S4888ojhZOODOyPjYHh4WB0dHXK73bF9iYmJcrvdamtrM5wMnzUwMCBJOv30040ngSRVVlZq1apVI/7dga2nn35aBQUF+t73vqeMjAwtW7ZMW7dutR5r2lu+fLn8fr/27dsnSfrXv/6l3bt36+tf/7rxZONjQv7W3smmv79f4XA49lNoP+V0OvXmm28aTYXPikQiuummm7RixYop+xOAJ5Pt27ers7NTe/bssR4F/+Ptt9/W5s2b5fV6ddttt2nPnj366U9/quTkZJWXl1uPN21VV1crFApp8eLFSkpKUjgc1r333qvrrrvOerRxQYxg2qisrNSrr76q3bt3W48y7fX09Kiqqko7d+5USkqK9Tj4H5FIRAUFBVq7dq0kadmyZXr11VfV1NREjBj6/e9/r0cffVTbtm3ThRdeqK6uLt10001auHDhlLguxMg4SE9PV1JSkoLB4Ij9wWBQmZmZRlPhf61Zs0Z//vOf9fzzz+vMM8+0Hmfa6+joUG9vry6++OLYvnA4rOeff16bNm3S0NCQkpKSDCecvhYsWKCcnJwR+y644AL98Y9/NJoIknTLLbeourpa11xzjSTpoosu0oEDB+Tz+aZEjPDMyDhITk5Wfn6+/H5/bF8kEpHf71dxcbHhZIhGo1qzZo2eeOIJ/e1vf9M555xjPRIkffWrX9Urr7yirq6u2FZQUKDrrrtOXV1dhIihFStWHPHx93379unss882mgiSdOjQoRG/hFaSkpKSFIlEjCYaX9wZGSder1fl5eUqKChQYWGhGhoaNDg4KI/HYz3atFZZWalt27bpqaee0ty5cxUIBCRJaWlpmjVrlvF009fcuXOPeG7ntNNO0xlnnMHzPMZuvvlmLV++XGvXrtX3v/99tbe3a8uWLdqyZYv1aNPaVVddpXvvvVdnnXWWLrzwQr388suqr6/Xj370I+vRxkcU42bjxo3Rs846K5qcnBwtLCyMvvjii9YjTXuSRt1+85vfWI+Gz1i5cmW0qqrKegxEo9E//elP0SVLlkQdDkd08eLF0S1btliPNO2FQqFoVVVV9KyzzoqmpKREzz333Ojtt98eHRoash5tXPBzRgAAgCmeGQEAAKaIEQAAYIoYAQAApogRAABgihgBAACmiBEAAGCKGAEAAKaIEQAAYIoYAQAApogRAABgihgBAACmiBEAAGDq/wCsAqzIqOx61gAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "ttt = TicTacToe()\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "state = ttt.get_initial_state()\n",
    "state = ttt.get_next_state(state, 2, -1)\n",
    "state = ttt.get_next_state(state, 4, -1)\n",
    "state = ttt.get_next_state(state, 6, 1)\n",
    "state = ttt.get_next_state(state, 8, 1)\n",
    "\n",
    "print(state)\n",
    "\n",
    "encoded_state = ttt.get_encoded_state(state)\n",
    "\n",
    "print(encoded_state)\n",
    "\n",
    "tensor_state = torch.tensor(encoded_state, device=device).unsqueeze(0)\n",
    "\n",
    "model = ResNet(ttt, 4, 64, device=device)\n",
    "model.load_state_dict(torch.load(f'models/{ttt}/model_2.pt', map_location=device))\n",
    "model.eval()\n",
    "\n",
    "policy, value = model(tensor_state)\n",
    "value = value.item()\n",
    "policy = torch.softmax(policy, axis=1).squeeze(0).detach().cpu().numpy()\n",
    "\n",
    "print(value)\n",
    "\n",
    "plt.bar(range(ttt.action_size), policy)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "fc005069-4b4f-41cb-97fb-65bad1af58cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    ''' A Monte Carlo Tree Search node '''\n",
    "    \n",
    "    def __init__(self, game, args, state, parent=None, action_taken=None, prior=0, visit_count=0):\n",
    "        self.game = game\n",
    "        self.args = args\n",
    "        self.state = state\n",
    "        self.parent = parent\n",
    "        self.action_taken = action_taken\n",
    "        self.prior = prior\n",
    "\n",
    "        self.children = []\n",
    "\n",
    "        self.visit_count = visit_count\n",
    "        self.value_sum = 0\n",
    "\n",
    "    def is_expanded(self):\n",
    "        ''' Check if the node already has children. Since we expand all children of a node at once, we check for >0 children '''\n",
    "        return len(self.children) > 0\n",
    "\n",
    "    def select(self):\n",
    "        ''' Select a child to explore '''\n",
    "        best_child = None\n",
    "        best_ucb = -np.inf\n",
    "\n",
    "        for child in self.children:\n",
    "            ucb = self.get_ucb(child)\n",
    "            if ucb > best_ucb:\n",
    "                best_child = child\n",
    "                best_ucb = ucb\n",
    "\n",
    "        return best_child\n",
    "\n",
    "    def get_ucb(self, child):\n",
    "        ''' Get how promising a move is from the opponent's perspective, normalized on [0,1] '''\n",
    "        if child.visit_count == 0:\n",
    "            q = 0\n",
    "        else:\n",
    "            q = 1 - ((child.value_sum / child.visit_count) + 1) / 2 \n",
    "\n",
    "        return q + self.args['C'] * (math.sqrt(self.visit_count) / (child.visit_count + 1)) * child.prior\n",
    "\n",
    "    def expand(self, policy):\n",
    "        ''' Expand a node by adding all legal child moves '''\n",
    "        for action, prob in enumerate(policy):\n",
    "            if prob > 0:\n",
    "                child_state = self.state.copy()\n",
    "                child_state = self.game.get_next_state(child_state, action, 1)\n",
    "                child_state = self.game.change_perspective(child_state, player=-1)\n",
    "        \n",
    "                child = Node(self.game, self.args, child_state, self, action, prob)\n",
    "                self.children.append(child)\n",
    "\n",
    "    def backpropagate(self, value):\n",
    "        ''' Propagate value sums and visit counts from children to all parents '''\n",
    "        self.value_sum += value\n",
    "        self.visit_count += 1\n",
    "\n",
    "        # flip value for opponent (parent)\n",
    "        value = self.game.get_opponent_value(value)\n",
    "\n",
    "        if self.parent is not None:\n",
    "            self.parent.backpropagate(value)\n",
    "\n",
    "class MCTS:\n",
    "    ''' A Monte Carlo Tree Search '''\n",
    "    \n",
    "    def __init__(self, game, args, model):\n",
    "        self.game = game\n",
    "        self.args = args\n",
    "        self.model = model\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def search(self, state):\n",
    "        ''' Expand and explore the MCTS and update value sums and visit counts '''\n",
    "        # DEFINE ROOT\n",
    "        root = Node(self.game, self.args, state, visit_count=1)\n",
    "\n",
    "        # add some random noise to policy to increase exploration\n",
    "        policy, _ = self.model(\n",
    "            torch.tensor(self.game.get_encoded_state(state), device=self.model.device).unsqueeze(0)\n",
    "        )\n",
    "        policy = torch.softmax(policy, axis=1).squeeze(0).cpu().numpy()\n",
    "        policy = ((1 - self.args['dirichlet_epsilon']) * \n",
    "                    policy + \n",
    "                    self.args['dirichlet_epsilon'] * \n",
    "                    np.random.dirichlet([self.args['dirichlet_alpha']] * \n",
    "                    self.game.action_size\n",
    "                ))\n",
    "        valid_moves = self.game.get_valid_moves(state)\n",
    "        policy *= valid_moves\n",
    "        policy /= np.sum(policy)\n",
    "        root.expand(policy)\n",
    "        \n",
    "        for search in range(self.args['num_searches']):\n",
    "            node = root\n",
    "            \n",
    "            # SELECTION\n",
    "            while node.is_expanded():\n",
    "                node = node.select()\n",
    "                \n",
    "            # check for end of game\n",
    "            value, is_terminal = self.game.get_value_and_terminated(node.state, node.action_taken)\n",
    "\n",
    "            # flip parent value\n",
    "            value = self.game.get_opponent_value(value)\n",
    "\n",
    "            if not is_terminal:\n",
    "                # get output from model\n",
    "                policy, value = self.model(\n",
    "                    torch.tensor(self.game.get_encoded_state(node.state), device=self.model.device).unsqueeze(0)\n",
    "                )\n",
    "\n",
    "                # change policy to proabability distribution\n",
    "                policy = torch.softmax(policy, axis=1).squeeze(0).cpu().numpy()\n",
    "\n",
    "                # mask out illegal moves\n",
    "                valid_moves = self.game.get_valid_moves(node.state)\n",
    "                policy *= valid_moves\n",
    "\n",
    "                # readjust back to probability distribution\n",
    "                policy /= np.sum(policy)\n",
    "\n",
    "                # get the value as a number from singleton tensor\n",
    "                value = value.item()\n",
    "                \n",
    "                # EXPANSION\n",
    "                node.expand(policy)\n",
    "    \n",
    "            # BACKPROP\n",
    "            node.backpropagate(value)\n",
    "\n",
    "        # probabilities of action being good\n",
    "        action_probs = np.zeros(self.game.action_size)\n",
    "        for child in root.children:\n",
    "            action_probs[child.action_taken] = child.visit_count\n",
    "\n",
    "        action_probs /= np.sum(action_probs)\n",
    "        return action_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "919a7ac9-28a6-4c49-b069-004b79290181",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlphaZero:\n",
    "    ''' AlphaZero class for self-play and training '''\n",
    "    \n",
    "    def __init__(self, model, optimizer, game, args):\n",
    "        ''' Initialize the AlphaZero instance '''\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.game = game\n",
    "        self.args = args\n",
    "        self.mcts = MCTS(game, args, model)\n",
    "\n",
    "    def self_play(self):\n",
    "        ''' Run a single self-play game until completion and generate outcome-appended training data '''\n",
    "        \n",
    "        memory = []\n",
    "        player = 1\n",
    "        state = self.game.get_initial_state()\n",
    "\n",
    "        while True:\n",
    "            # get the current state and action probabilities from MCTS\n",
    "            neutral_state = self.game.change_perspective(state, player)\n",
    "            action_probs = self.mcts.search(neutral_state)\n",
    "\n",
    "            # record a game snapshot\n",
    "            memory.append((neutral_state, action_probs, player))\n",
    "\n",
    "            # randomly sample an action from the distribution\n",
    "            adjusted_action_probs = action_probs ** (1 / self.args['temperature']) # add flexibility for exploration / exploitation\n",
    "            adjusted_action_probs /= np.sum(adjusted_action_probs)\n",
    "            action = np.random.choice(self.game.action_size, p=adjusted_action_probs)\n",
    "\n",
    "            # get the next state given the chosen action\n",
    "            state = self.game.get_next_state(state, action, player)\n",
    "\n",
    "            # check for game completion\n",
    "            value, is_terminal = self.game.get_value_and_terminated(state, action)\n",
    "\n",
    "            if is_terminal:\n",
    "                # get all states and policies from the game and append the outcome\n",
    "                return [(\n",
    "                    self.game.get_encoded_state(h_state),\n",
    "                    h_action_probs,\n",
    "                    value if h_player == player else self.game.get_opponent_value(value)\n",
    "                ) for h_state, h_action_probs, h_player in memory]\n",
    "\n",
    "            # swap the player and loop\n",
    "            player = self.game.get_opponent(player)\n",
    "\n",
    "    def train(self, memory):\n",
    "        ''' Train the model '''\n",
    "\n",
    "        # randomize training data\n",
    "        random.shuffle(memory)\n",
    "        \n",
    "        for batch_i in range(0, len(memory), self.args['batch_size']):\n",
    "            # sample a batch from training data\n",
    "            sample = memory[batch_i : min(len(memory) - 1, batch_i + self.args['batch_size'])]\n",
    "\n",
    "            # transpose list of tuples to independent lists\n",
    "            state, policy_targets, value_targets = zip(*sample)\n",
    "\n",
    "            # convert to numpy arrays\n",
    "            state = np.array(state)\n",
    "            policy_targets = np.array(policy_targets)\n",
    "            value_targets = np.array(value_targets).reshape(-1, 1) # wrap each value in its own array for simplicity later\n",
    "\n",
    "            # convert to tensors\n",
    "            state = torch.tensor(state, dtype=torch.float32, device=self.model.device)\n",
    "            policy_targets = torch.tensor(policy_targets, dtype=torch.float32, device=self.model.device)\n",
    "            value_targets = torch.tensor(value_targets, dtype=torch.float32, device=self.model.device)\n",
    "\n",
    "            # get model outputs\n",
    "            out_policy, out_value = self.model(state)\n",
    "\n",
    "            # get loss\n",
    "            policy_loss = F.cross_entropy(out_policy, policy_targets)\n",
    "            value_loss = F.mse_loss(out_value, value_targets)\n",
    "            loss = policy_loss + value_loss\n",
    "\n",
    "            # minimize loss via backpropagation\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    def learn(self):\n",
    "        ''' Generate self-play training data and train the model on it '''\n",
    "        \n",
    "        for iter in range(self.args['num_iters']):\n",
    "            memory = []\n",
    "            \n",
    "            self.model.eval()\n",
    "            for self_play_iter in trange(self.args['num_self_play_iters']):\n",
    "                memory += self.self_play()\n",
    "\n",
    "            self.model.train()\n",
    "            for epoch in trange(self.args['num_epochs']):\n",
    "                self.train(memory)\n",
    "\n",
    "            Path(f'models/{self.game}').mkdir(parents=True, exist_ok=True)\n",
    "            \n",
    "            torch.save(self.model.state_dict(), f'models/{self.game}/model_{iter}.pt')\n",
    "            torch.save(self.optimizer.state_dict(), f'models/{self.game}/optimizer_{iter}.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "5f989da5-eba5-4beb-8edd-5eeff772d7ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e1537c7181843b69e65dd990e53a5e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aaff8d1eeb5741e6a4ba896a5e5ec1f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "912cc71ddfb04ed195f49cbfe57d90fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f950e0d2c8e44e698157b37d7b896b36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0125b63164474a668447959da99d6c7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3dad006c63c64bf1ae290c3b651e6603",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ttt = TicTacToe()\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = ResNet(ttt, 4, 64, device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=0.0001)\n",
    "\n",
    "args = {\n",
    "    'C': 2,\n",
    "    'num_searches': 60,\n",
    "    'num_iters': 3,\n",
    "    'num_self_play_iters': 500,\n",
    "    'num_epochs': 4,\n",
    "    'batch_size': 64,\n",
    "    'temperature': 1.25,\n",
    "    'dirichlet_epsilon': 0.25,\n",
    "    'dirichlet_alpha': 0.3\n",
    "}\n",
    "\n",
    "alphazero = AlphaZero(model, optimizer, ttt, args)\n",
    "alphazero.learn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "703608a9-b8be-4b03-b7da-2ba45921465c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]]\n",
      "Valid Moves: [0, 1, 2, 3, 4, 5, 6, 7, 8]\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "1:  0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]]\n",
      "[[ 1.  0.  0.]\n",
      " [ 0. -1.  0.]\n",
      " [ 0.  0.  0.]]\n",
      "Valid Moves: [1, 2, 3, 5, 6, 7, 8]\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "1:  1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.  1.  0.]\n",
      " [ 0. -1.  0.]\n",
      " [ 0.  0.  0.]]\n",
      "[[ 1.  1. -1.]\n",
      " [ 0. -1.  0.]\n",
      " [ 0.  0.  0.]]\n",
      "Valid Moves: [3, 5, 6, 7, 8]\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "1:  8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.  1. -1.]\n",
      " [ 0. -1.  0.]\n",
      " [ 0.  0.  1.]]\n",
      "[[ 1.  1. -1.]\n",
      " [ 0. -1.  0.]\n",
      " [-1.  0.  1.]]\n",
      "-1 won\n"
     ]
    }
   ],
   "source": [
    "ttt = TicTacToe()\n",
    "player = 1\n",
    "\n",
    "args = {\n",
    "    'C': 2,\n",
    "    'num_searches': 1000\n",
    "}\n",
    "\n",
    "model = ResNet(ttt, 4, 64)\n",
    "model.eval()\n",
    "\n",
    "mcts = MCTS(ttt, args, model)\n",
    "\n",
    "state = ttt.get_initial_state()\n",
    "\n",
    "while True:\n",
    "    print(state)\n",
    "\n",
    "    if player == 1:\n",
    "        valid_moves = ttt.get_valid_moves(state)\n",
    "        print('Valid Moves:' , [i for i in range(ttt.action_size) if valid_moves[i] == 1])\n",
    "        action = int(input(f'{player}: '))\n",
    "    \n",
    "        if valid_moves[action] == 0:\n",
    "            print('Invalid move')\n",
    "            continue\n",
    "    else:\n",
    "        neutral_state = ttt.change_perspective(state, player)\n",
    "        mcts_probs = mcts.search(neutral_state)\n",
    "        action = np.argmax(mcts_probs)\n",
    "\n",
    "    state = ttt.get_next_state(state, action, player)\n",
    "\n",
    "    value, is_terminal = ttt.get_value_and_terminated(state, action)\n",
    "\n",
    "    if is_terminal:\n",
    "        print(state)\n",
    "        if value == 1:\n",
    "            print(player, 'won')\n",
    "        else:\n",
    "            print('Draw')\n",
    "        break\n",
    "\n",
    "    player = ttt.get_opponent(player)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e4eb825-e431-4ea0-998f-d1da412b5daf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8251b500-c17c-49af-899a-2d3c5f6420fe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
