{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7620efee-0dba-4939-ad3d-880af53233b5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-24T03:24:22.152294Z",
     "start_time": "2023-11-24T03:24:20.805377Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.24.4\n",
      "2.1.1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fa810dae710>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "import random\n",
    "from tqdm.notebook import trange\n",
    "\n",
    "import numpy as np\n",
    "print(np.__version__)\n",
    "\n",
    "import torch\n",
    "print(torch.__version__)\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d894942-1528-4af0-8993-7d0d1e30b243",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TicTacToe:\n",
    "    def __init__(self):\n",
    "        self.row_count = 3\n",
    "        self.col_count = 3\n",
    "        self.action_size = self.row_count * self.col_count\n",
    "\n",
    "    def get_initial_state(self):\n",
    "        ''' Get board with all zeros '''\n",
    "        return np.zeros((self.row_count, self.col_count))\n",
    "\n",
    "    def get_next_state(self, state, action, player):\n",
    "        ''' Get the next state given the given action by the given player '''\n",
    "        row = action // self.col_count\n",
    "        col = action % self.col_count\n",
    "        state[row, col] = player\n",
    "        return state\n",
    "\n",
    "    def get_valid_moves(self, state):\n",
    "        ''' Get all the legal moves in the position '''\n",
    "        return (state.reshape(-1) == 0).astype(np.uint8)\n",
    "\n",
    "    def check_win(self, state, action):\n",
    "        ''' Check if the given action has led to a win '''\n",
    "        if action == None:\n",
    "            return False\n",
    "        \n",
    "        row = action // self.col_count\n",
    "        col = action % self.col_count\n",
    "        player = state[row, col]\n",
    "\n",
    "        return (\n",
    "            np.sum(state[row, :]) == player * self.col_count or # rows\n",
    "            np.sum(state[:, col]) == player * self.row_count or # columns\n",
    "            np.sum(np.diag(state)) == player * self.row_count or # tl->br diag\n",
    "            np.sum(np.diag(np.flip(state, axis=0))) == player * self.row_count # tr->bl diag\n",
    "        )\n",
    "\n",
    "    def get_value_and_terminated(self, state, action):\n",
    "        ''' Get the value (win/tie) and if the game has terminated '''\n",
    "        if self.check_win(state, action):\n",
    "            return 1, True\n",
    "        if np.sum(self.get_valid_moves(state)) == 0:\n",
    "            return 0, True\n",
    "        return 0, False\n",
    "\n",
    "    def get_opponent(self, player):\n",
    "        return -player\n",
    "\n",
    "    def get_opponent_value(self, value):\n",
    "        return -value\n",
    "\n",
    "    def change_perspective(self, state, player):\n",
    "        return state * player\n",
    "\n",
    "    def get_encoded_state(self, state):\n",
    "        return np.stack(\n",
    "            (state == -1, state == 0, state == 1)\n",
    "        ).astype(np.float32)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3bd0cd66-2da5-4e7e-97eb-becc878292d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    "    def __init__(self, game, num_resblocks, num_hidden):\n",
    "        super().__init__()\n",
    "        self.start_block = nn.Sequential(\n",
    "            nn.Conv2d(3, num_hidden, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(num_hidden),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.backbone = nn.ModuleList(\n",
    "            [ResBlock(num_hidden) for _ in range(num_resblocks)]\n",
    "        )\n",
    "\n",
    "        self.policy_head = nn.Sequential(\n",
    "            nn.Conv2d(num_hidden, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(32 * game.row_count * game.col_count, game.action_size)\n",
    "        )\n",
    "\n",
    "        self.value_head = nn.Sequential(\n",
    "            nn.Conv2d(num_hidden, 3, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(3),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(3 * game.row_count * game.col_count, 1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.start_block(x)\n",
    "        for resblock in self.backbone:\n",
    "            x = resblock(x)\n",
    "        policy = self.policy_head(x)\n",
    "        value = self.value_head(x)\n",
    "\n",
    "        return policy, value\n",
    "\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, num_hidden):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(num_hidden, num_hidden, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(num_hidden)\n",
    "        self.conv2 = nn.Conv2d(num_hidden, num_hidden, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(num_hidden)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.bn2(self.conv2(x))\n",
    "        x += residual\n",
    "        x = F.relu(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e888f91a-00c3-46bf-a9f5-d0685368479e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  0. -1.]\n",
      " [ 0. -1.  0.]\n",
      " [ 1.  0.  1.]]\n",
      "[[[0. 0. 1.]\n",
      "  [0. 1. 0.]\n",
      "  [0. 0. 0.]]\n",
      "\n",
      " [[1. 1. 0.]\n",
      "  [1. 0. 1.]\n",
      "  [0. 1. 0.]]\n",
      "\n",
      " [[0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [1. 0. 1.]]]\n",
      "0.9559563994407654\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "ttt = TicTacToe()\n",
    "\n",
    "state = ttt.get_initial_state()\n",
    "state = ttt.get_next_state(state, 2, -1)\n",
    "state = ttt.get_next_state(state, 4, -1)\n",
    "state = ttt.get_next_state(state, 6, 1)\n",
    "state = ttt.get_next_state(state, 8, 1)\n",
    "\n",
    "print(state)\n",
    "\n",
    "encoded_state = ttt.get_encoded_state(state)\n",
    "\n",
    "print(encoded_state)\n",
    "\n",
    "tensor_state = torch.tensor(encoded_state).unsqueeze(0)\n",
    "\n",
    "model = ResNet(ttt, 4, 64)\n",
    "model.load_state_dict(torch.load('model_2.pt'))\n",
    "model.eval()\n",
    "\n",
    "policy, value = model(tensor_state)\n",
    "value = value.item()\n",
    "policy = torch.softmax(policy, axis=1).squeeze(0).detach().cpu().numpy()\n",
    "\n",
    "print(value)\n",
    "\n",
    "plt.bar(range(ttt.action_size), policy)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fc005069-4b4f-41cb-97fb-65bad1af58cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, game, args, state, parent=None, action_taken=None, prior=0):\n",
    "        self.game = game\n",
    "        self.args = args\n",
    "        self.state = state\n",
    "        self.parent = parent\n",
    "        self.action_taken = action_taken\n",
    "        self.prior = prior\n",
    "\n",
    "        self.children = []\n",
    "\n",
    "        self.visit_count = 0\n",
    "        self.value_sum = 0\n",
    "\n",
    "    def is_expanded(self):\n",
    "        return len(self.children) > 0\n",
    "\n",
    "    def select(self):\n",
    "        best_child = None\n",
    "        best_ucb = -np.inf\n",
    "\n",
    "        for child in self.children:\n",
    "            ucb = self.get_ucb(child)\n",
    "            if ucb > best_ucb:\n",
    "                best_child = child\n",
    "                best_ucb = ucb\n",
    "\n",
    "        return best_child\n",
    "\n",
    "    def get_ucb(self, child):\n",
    "        ''' Get how promising a move is from the opponent's perspective, normalized on [0,1] '''\n",
    "        if child.visit_count == 0:\n",
    "            q = 0\n",
    "        else:\n",
    "            q = 1 - ((child.value_sum / child.visit_count) + 1) / 2 \n",
    "\n",
    "        return q + self.args['C'] * (math.sqrt(self.visit_count) / (child.visit_count + 1)) * child.prior\n",
    "\n",
    "    def expand(self, policy):\n",
    "        for action, prob in enumerate(policy):\n",
    "            if prob > 0:\n",
    "                child_state = self.state.copy()\n",
    "                child_state = self.game.get_next_state(child_state, action, 1)\n",
    "                child_state = self.game.change_perspective(child_state, player=-1)\n",
    "        \n",
    "                child = Node(self.game, self.args, child_state, self, action, prob)\n",
    "                self.children.append(child)\n",
    "\n",
    "    def backpropagate(self, value):\n",
    "        self.value_sum += value\n",
    "        self.visit_count += 1\n",
    "\n",
    "        # flip value for opponent (parent)\n",
    "        value = self.game.get_opponent_value(value)\n",
    "\n",
    "        if self.parent is not None:\n",
    "            self.parent.backpropagate(value)\n",
    "\n",
    "class MCTS:\n",
    "    def __init__(self, game, args, model):\n",
    "        self.game = game\n",
    "        self.args = args\n",
    "        self.model = model\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def search(self, state):\n",
    "        # DEFINE ROOT\n",
    "        root = Node(self.game, self.args, state)\n",
    "        \n",
    "        for search in range(self.args['num_searches']):\n",
    "            node = root\n",
    "            \n",
    "            # SELECTION\n",
    "            while node.is_expanded():\n",
    "                node = node.select()\n",
    "                \n",
    "            # check for end of game\n",
    "            value, is_terminal = self.game.get_value_and_terminated(node.state, node.action_taken)\n",
    "\n",
    "            # flip parent value\n",
    "            value = self.game.get_opponent_value(value)\n",
    "\n",
    "            if not is_terminal:\n",
    "                # get output from model\n",
    "                policy, value = self.model(torch.tensor(self.game.get_encoded_state(node.state)).unsqueeze(0))\n",
    "\n",
    "                # change policy to proabability distribution\n",
    "                policy = torch.softmax(policy, axis=1).squeeze(0).cpu().numpy()\n",
    "\n",
    "                # mask out illegal moves\n",
    "                valid_moves = self.game.get_valid_moves(node.state)\n",
    "                policy *= valid_moves\n",
    "\n",
    "                # readjust back to probability distribution\n",
    "                policy /= np.sum(policy)\n",
    "\n",
    "                # get the value as a number from singleton tensor\n",
    "                value = value.item()\n",
    "                \n",
    "                # EXPANSION\n",
    "                node.expand(policy)\n",
    "    \n",
    "            # BACKPROP\n",
    "            node.backpropagate(value)\n",
    "\n",
    "        # probabilities of action being good\n",
    "        action_probs = np.zeros(self.game.action_size)\n",
    "        for child in root.children:\n",
    "            action_probs[child.action_taken] = child.visit_count\n",
    "\n",
    "        action_probs /= np.sum(action_probs)\n",
    "        return action_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "919a7ac9-28a6-4c49-b069-004b79290181",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlphaZero:\n",
    "    ''' AlphaZero class for self-play and training '''\n",
    "    \n",
    "    def __init__(self, model, optimizer, game, args):\n",
    "        ''' Initialize the AlphaZero instance '''\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.game = game\n",
    "        self.args = args\n",
    "        self.mcts = MCTS(game, args, model)\n",
    "\n",
    "    def self_play(self):\n",
    "        ''' Run a single self-play game until completion and generate outcome-appended training data '''\n",
    "        \n",
    "        memory = []\n",
    "        player = 1\n",
    "        state = self.game.get_initial_state()\n",
    "\n",
    "        while True:\n",
    "            # get the current state and action probabilities from MCTS\n",
    "            neutral_state = self.game.change_perspective(state, player)\n",
    "            action_probs = self.mcts.search(neutral_state)\n",
    "\n",
    "            # record a game snapshot\n",
    "            memory.append((neutral_state, action_probs, player))\n",
    "\n",
    "            # randomly sample an action from the distribution\n",
    "            action = np.random.choice(self.game.action_size, p=action_probs)\n",
    "\n",
    "            # get the next state given the chosen action\n",
    "            state = self.game.get_next_state(state, action, player)\n",
    "\n",
    "            # check for game completion\n",
    "            value, is_terminal = self.game.get_value_and_terminated(state, action)\n",
    "\n",
    "            if is_terminal:\n",
    "                # get all states and policies from the game and append the outcome\n",
    "                return [(\n",
    "                    self.game.get_encoded_state(h_state),\n",
    "                    h_action_probs,\n",
    "                    value if h_player == player else self.game.get_opponent_value(value)\n",
    "                ) for h_state, h_action_probs, h_player in memory]\n",
    "\n",
    "            # swap the player and loop\n",
    "            player = self.game.get_opponent(player)\n",
    "\n",
    "    def train(self, memory):\n",
    "        ''' Train the model '''\n",
    "\n",
    "        # randomize training data\n",
    "        random.shuffle(memory)\n",
    "        \n",
    "        for batch_i in range(0, len(memory), self.args['batch_size']):\n",
    "            # sample a batch from training data\n",
    "            sample = memory[batch_i : min(len(memory) - 1, batch_i + self.args['batch_size'])]\n",
    "\n",
    "            # transpose list of tuples to independent lists\n",
    "            state, policy_targets, value_targets = zip(*sample)\n",
    "\n",
    "            # convert to numpy arrays\n",
    "            state = np.array(state)\n",
    "            policy_targets = np.array(policy_targets)\n",
    "            value_targets = np.array(value_targets).reshape(-1, 1) # wrap each value in its own array for simplicity later\n",
    "\n",
    "            # convert to tensors\n",
    "            state = torch.tensor(state, dtype=torch.float32)\n",
    "            policy_targets = torch.tensor(policy_targets, dtype=torch.float32)\n",
    "            value_targets = torch.tensor(value_targets, dtype=torch.float32)\n",
    "\n",
    "            # get model outputs\n",
    "            out_policy, out_value = self.model(state)\n",
    "\n",
    "            # get loss\n",
    "            policy_loss = F.cross_entropy(out_policy, policy_targets)\n",
    "            value_loss = F.mse_loss(out_value, value_targets)\n",
    "            loss = policy_loss + value_loss\n",
    "\n",
    "            # minimize loss via backpropagation\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    def learn(self):\n",
    "        ''' Generate self-play training data and train the model on it '''\n",
    "        \n",
    "        for iter in range(self.args['num_iters']):\n",
    "            memory = []\n",
    "            \n",
    "            self.model.eval()\n",
    "            for self_play_iter in trange(self.args['num_self_play_iters']):\n",
    "                memory += self.self_play()\n",
    "\n",
    "            self.model.train()\n",
    "            for epoch in trange(self.args['num_epochs']):\n",
    "                self.train(memory)\n",
    "\n",
    "            torch.save(self.model.state_dict(), f'model_{iter}.pt')\n",
    "            torch.save(self.optimizer.state_dict(), f'optimizer_{iter}.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5f989da5-eba5-4beb-8edd-5eeff772d7ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82363ab1b66b41068502466f3b6ccaf2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5afa8a8d5f8745f7986cc86fdf719133",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21daff565f2b4bac903aaabafb7d7309",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b1baea2b20e4584a40d8cc821b08844",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63cd2f9dce4e48c3b33ba16014b50f0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5681e9a7ba74b53872857e00a6fab5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ttt = TicTacToe()\n",
    "\n",
    "model = ResNet(ttt, 4, 64)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "args = {\n",
    "    'C': 2,\n",
    "    'num_searches': 60,\n",
    "    'num_iters': 3,\n",
    "    'num_self_play_iters': 500,\n",
    "    'num_epochs': 4,\n",
    "    'batch_size': 64\n",
    "}\n",
    "\n",
    "alphazero = AlphaZero(model, optimizer, ttt, args)\n",
    "alphazero.learn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "703608a9-b8be-4b03-b7da-2ba45921465c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]]\n",
      "Valid Moves: [0, 1, 2, 3, 4, 5, 6, 7, 8]\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "1:  0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]]\n",
      "[[ 1.  0.  0.]\n",
      " [ 0. -1.  0.]\n",
      " [ 0.  0.  0.]]\n",
      "Valid Moves: [1, 2, 3, 5, 6, 7, 8]\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "1:  1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.  1.  0.]\n",
      " [ 0. -1.  0.]\n",
      " [ 0.  0.  0.]]\n",
      "[[ 1.  1. -1.]\n",
      " [ 0. -1.  0.]\n",
      " [ 0.  0.  0.]]\n",
      "Valid Moves: [3, 5, 6, 7, 8]\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "1:  8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.  1. -1.]\n",
      " [ 0. -1.  0.]\n",
      " [ 0.  0.  1.]]\n",
      "[[ 1.  1. -1.]\n",
      " [ 0. -1.  0.]\n",
      " [-1.  0.  1.]]\n",
      "-1 won\n"
     ]
    }
   ],
   "source": [
    "ttt = TicTacToe()\n",
    "player = 1\n",
    "\n",
    "args = {\n",
    "    'C': 2,\n",
    "    'num_searches': 1000\n",
    "}\n",
    "\n",
    "model = ResNet(ttt, 4, 64)\n",
    "model.eval()\n",
    "\n",
    "mcts = MCTS(ttt, args, model)\n",
    "\n",
    "state = ttt.get_initial_state()\n",
    "\n",
    "while True:\n",
    "    print(state)\n",
    "\n",
    "    if player == 1:\n",
    "        valid_moves = ttt.get_valid_moves(state)\n",
    "        print('Valid Moves:' , [i for i in range(ttt.action_size) if valid_moves[i] == 1])\n",
    "        action = int(input(f'{player}: '))\n",
    "    \n",
    "        if valid_moves[action] == 0:\n",
    "            print('Invalid move')\n",
    "            continue\n",
    "    else:\n",
    "        neutral_state = ttt.change_perspective(state, player)\n",
    "        mcts_probs = mcts.search(neutral_state)\n",
    "        action = np.argmax(mcts_probs)\n",
    "\n",
    "    state = ttt.get_next_state(state, action, player)\n",
    "\n",
    "    value, is_terminal = ttt.get_value_and_terminated(state, action)\n",
    "\n",
    "    if is_terminal:\n",
    "        print(state)\n",
    "        if value == 1:\n",
    "            print(player, 'won')\n",
    "        else:\n",
    "            print('Draw')\n",
    "        break\n",
    "\n",
    "    player = ttt.get_opponent(player)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e4eb825-e431-4ea0-998f-d1da412b5daf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8251b500-c17c-49af-899a-2d3c5f6420fe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
