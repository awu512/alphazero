{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7620efee-0dba-4939-ad3d-880af53233b5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-24T03:24:22.152294Z",
     "start_time": "2023-11-24T03:24:20.805377Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.24.4\n",
      "2.1.1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fa810dae710>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "import random\n",
    "from tqdm.notebook import trange\n",
    "\n",
    "import numpy as np\n",
    "print(np.__version__)\n",
    "\n",
    "import torch\n",
    "print(torch.__version__)\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2d894942-1528-4af0-8993-7d0d1e30b243",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TicTacToe:\n",
    "    def __init__(self):\n",
    "        self.row_count = 3\n",
    "        self.col_count = 3\n",
    "        self.action_size = self.row_count * self.col_count\n",
    "\n",
    "    def __repr___(self):\n",
    "        return 'TicTacToe'\n",
    "\n",
    "    def get_initial_state(self):\n",
    "        ''' Get board with all zeros '''\n",
    "        return np.zeros((self.row_count, self.col_count))\n",
    "\n",
    "    def get_next_state(self, state, action, player):\n",
    "        ''' Get the next state given the given action by the given player '''\n",
    "        row = action // self.col_count\n",
    "        col = action % self.col_count\n",
    "        state[row, col] = player\n",
    "        return state\n",
    "\n",
    "    def get_valid_moves(self, state):\n",
    "        ''' Get all the legal moves in the position '''\n",
    "        return (state.reshape(-1) == 0).astype(np.uint8)\n",
    "\n",
    "    def check_win(self, state, action):\n",
    "        ''' Check if the given action has led to a win '''\n",
    "        if action == None:\n",
    "            return False\n",
    "        \n",
    "        row = action // self.col_count\n",
    "        col = action % self.col_count\n",
    "        player = state[row, col]\n",
    "\n",
    "        return (\n",
    "            np.sum(state[row, :]) == player * self.col_count or # rows\n",
    "            np.sum(state[:, col]) == player * self.row_count or # columns\n",
    "            np.sum(np.diag(state)) == player * self.row_count or # tl->br diag\n",
    "            np.sum(np.diag(np.flip(state, axis=0))) == player * self.row_count # tr->bl diag\n",
    "        )\n",
    "\n",
    "    def get_value_and_terminated(self, state, action):\n",
    "        ''' Get the value (win/tie) and if the game has terminated '''\n",
    "        if self.check_win(state, action):\n",
    "            return 1, True\n",
    "        if np.sum(self.get_valid_moves(state)) == 0:\n",
    "            return 0, True\n",
    "        return 0, False\n",
    "\n",
    "    def get_opponent(self, player):\n",
    "        return -player\n",
    "\n",
    "    def get_opponent_value(self, value):\n",
    "        return -value\n",
    "\n",
    "    def change_perspective(self, state, player):\n",
    "        return state * player\n",
    "\n",
    "    def get_encoded_state(self, state):\n",
    "        return np.stack(\n",
    "            (state == -1, state == 0, state == 1)\n",
    "        ).astype(np.float32)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3bd0cd66-2da5-4e7e-97eb-becc878292d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    "    def __init__(self, game, num_resblocks, num_hidden, device):\n",
    "        super().__init__()\n",
    "\n",
    "        self.device = device\n",
    "        \n",
    "        self.start_block = nn.Sequential(\n",
    "            nn.Conv2d(3, num_hidden, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(num_hidden),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.backbone = nn.ModuleList(\n",
    "            [ResBlock(num_hidden) for _ in range(num_resblocks)]\n",
    "        )\n",
    "\n",
    "        self.policy_head = nn.Sequential(\n",
    "            nn.Conv2d(num_hidden, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(32 * game.row_count * game.col_count, game.action_size)\n",
    "        )\n",
    "\n",
    "        self.value_head = nn.Sequential(\n",
    "            nn.Conv2d(num_hidden, 3, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(3),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(3 * game.row_count * game.col_count, 1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "        self.to(device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.start_block(x)\n",
    "        for resblock in self.backbone:\n",
    "            x = resblock(x)\n",
    "        policy = self.policy_head(x)\n",
    "        value = self.value_head(x)\n",
    "\n",
    "        return policy, value\n",
    "\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, num_hidden):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(num_hidden, num_hidden, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(num_hidden)\n",
    "        self.conv2 = nn.Conv2d(num_hidden, num_hidden, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(num_hidden)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.bn2(self.conv2(x))\n",
    "        x += residual\n",
    "        x = F.relu(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e888f91a-00c3-46bf-a9f5-d0685368479e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  0. -1.]\n",
      " [ 0. -1.  0.]\n",
      " [ 1.  0.  1.]]\n",
      "[[[0. 0. 1.]\n",
      "  [0. 1. 0.]\n",
      "  [0. 0. 0.]]\n",
      "\n",
      " [[1. 1. 0.]\n",
      "  [1. 0. 1.]\n",
      "  [0. 1. 0.]]\n",
      "\n",
      " [[0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [1. 0. 1.]]]\n",
      "0.897591233253479\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "ttt = TicTacToe()\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "state = ttt.get_initial_state()\n",
    "state = ttt.get_next_state(state, 2, -1)\n",
    "state = ttt.get_next_state(state, 4, -1)\n",
    "state = ttt.get_next_state(state, 6, 1)\n",
    "state = ttt.get_next_state(state, 8, 1)\n",
    "\n",
    "print(state)\n",
    "\n",
    "encoded_state = ttt.get_encoded_state(state)\n",
    "\n",
    "print(encoded_state)\n",
    "\n",
    "tensor_state = torch.tensor(encoded_state, device=device).unsqueeze(0)\n",
    "\n",
    "model = ResNet(ttt, 4, 64, device=device)\n",
    "model.load_state_dict(torch.load('model_2.pt', map_location=device))\n",
    "model.eval()\n",
    "\n",
    "policy, value = model(tensor_state)\n",
    "value = value.item()\n",
    "policy = torch.softmax(policy, axis=1).squeeze(0).detach().cpu().numpy()\n",
    "\n",
    "print(value)\n",
    "\n",
    "plt.bar(range(ttt.action_size), policy)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fc005069-4b4f-41cb-97fb-65bad1af58cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, game, args, state, parent=None, action_taken=None, prior=0, visit_count=0):\n",
    "        self.game = game\n",
    "        self.args = args\n",
    "        self.state = state\n",
    "        self.parent = parent\n",
    "        self.action_taken = action_taken\n",
    "        self.prior = prior\n",
    "\n",
    "        self.children = []\n",
    "\n",
    "        self.visit_count = visit_count\n",
    "        self.value_sum = 0\n",
    "\n",
    "    def is_expanded(self):\n",
    "        return len(self.children) > 0\n",
    "\n",
    "    def select(self):\n",
    "        best_child = None\n",
    "        best_ucb = -np.inf\n",
    "\n",
    "        for child in self.children:\n",
    "            ucb = self.get_ucb(child)\n",
    "            if ucb > best_ucb:\n",
    "                best_child = child\n",
    "                best_ucb = ucb\n",
    "\n",
    "        return best_child\n",
    "\n",
    "    def get_ucb(self, child):\n",
    "        ''' Get how promising a move is from the opponent's perspective, normalized on [0,1] '''\n",
    "        if child.visit_count == 0:\n",
    "            q = 0\n",
    "        else:\n",
    "            q = 1 - ((child.value_sum / child.visit_count) + 1) / 2 \n",
    "\n",
    "        return q + self.args['C'] * (math.sqrt(self.visit_count) / (child.visit_count + 1)) * child.prior\n",
    "\n",
    "    def expand(self, policy):\n",
    "        for action, prob in enumerate(policy):\n",
    "            if prob > 0:\n",
    "                child_state = self.state.copy()\n",
    "                child_state = self.game.get_next_state(child_state, action, 1)\n",
    "                child_state = self.game.change_perspective(child_state, player=-1)\n",
    "        \n",
    "                child = Node(self.game, self.args, child_state, self, action, prob)\n",
    "                self.children.append(child)\n",
    "\n",
    "    def backpropagate(self, value):\n",
    "        self.value_sum += value\n",
    "        self.visit_count += 1\n",
    "\n",
    "        # flip value for opponent (parent)\n",
    "        value = self.game.get_opponent_value(value)\n",
    "\n",
    "        if self.parent is not None:\n",
    "            self.parent.backpropagate(value)\n",
    "\n",
    "class MCTS:\n",
    "    def __init__(self, game, args, model):\n",
    "        self.game = game\n",
    "        self.args = args\n",
    "        self.model = model\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def search(self, state):\n",
    "        # DEFINE ROOT\n",
    "        root = Node(self.game, self.args, state, visit_count=1)\n",
    "\n",
    "        # add some random noise to policy to increase exploration\n",
    "        policy, _ = self.model(\n",
    "            torch.tensor(self.game.get_encoded_state(state), device=self.model.device).unsqueeze(0)\n",
    "        )\n",
    "        policy = torch.softmax(policy, axis=1).squeeze(0).cpu().numpy()\n",
    "        policy = ((1 - self.args['dirichlet_epsilon']) * \n",
    "                    policy + \n",
    "                    self.args['dirichlet_epsilon'] * \n",
    "                    np.random.dirichlet([self.args['dirichlet_alpha']] * \n",
    "                    self.game.action_size\n",
    "                ))\n",
    "        valid_moves = self.game.get_valid_moves(state)\n",
    "        policy *= valid_moves\n",
    "        policy /= np.sum(policy)\n",
    "        root.expand(policy)\n",
    "        \n",
    "        for search in range(self.args['num_searches']):\n",
    "            node = root\n",
    "            \n",
    "            # SELECTION\n",
    "            while node.is_expanded():\n",
    "                node = node.select()\n",
    "                \n",
    "            # check for end of game\n",
    "            value, is_terminal = self.game.get_value_and_terminated(node.state, node.action_taken)\n",
    "\n",
    "            # flip parent value\n",
    "            value = self.game.get_opponent_value(value)\n",
    "\n",
    "            if not is_terminal:\n",
    "                # get output from model\n",
    "                policy, value = self.model(\n",
    "                    torch.tensor(self.game.get_encoded_state(node.state), device=self.model.device).unsqueeze(0)\n",
    "                )\n",
    "\n",
    "                # change policy to proabability distribution\n",
    "                policy = torch.softmax(policy, axis=1).squeeze(0).cpu().numpy()\n",
    "\n",
    "                # mask out illegal moves\n",
    "                valid_moves = self.game.get_valid_moves(node.state)\n",
    "                policy *= valid_moves\n",
    "\n",
    "                # readjust back to probability distribution\n",
    "                policy /= np.sum(policy)\n",
    "\n",
    "                # get the value as a number from singleton tensor\n",
    "                value = value.item()\n",
    "                \n",
    "                # EXPANSION\n",
    "                node.expand(policy)\n",
    "    \n",
    "            # BACKPROP\n",
    "            node.backpropagate(value)\n",
    "\n",
    "        # probabilities of action being good\n",
    "        action_probs = np.zeros(self.game.action_size)\n",
    "        for child in root.children:\n",
    "            action_probs[child.action_taken] = child.visit_count\n",
    "\n",
    "        action_probs /= np.sum(action_probs)\n",
    "        return action_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "919a7ac9-28a6-4c49-b069-004b79290181",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlphaZero:\n",
    "    ''' AlphaZero class for self-play and training '''\n",
    "    \n",
    "    def __init__(self, model, optimizer, game, args):\n",
    "        ''' Initialize the AlphaZero instance '''\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.game = game\n",
    "        self.args = args\n",
    "        self.mcts = MCTS(game, args, model)\n",
    "\n",
    "    def self_play(self):\n",
    "        ''' Run a single self-play game until completion and generate outcome-appended training data '''\n",
    "        \n",
    "        memory = []\n",
    "        player = 1\n",
    "        state = self.game.get_initial_state()\n",
    "\n",
    "        while True:\n",
    "            # get the current state and action probabilities from MCTS\n",
    "            neutral_state = self.game.change_perspective(state, player)\n",
    "            action_probs = self.mcts.search(neutral_state)\n",
    "\n",
    "            # record a game snapshot\n",
    "            memory.append((neutral_state, action_probs, player))\n",
    "\n",
    "            # randomly sample an action from the distribution\n",
    "            adjusted_action_probs = action_probs ** (1 / self.args['temperature']) # add flexibility for exploration / exploitation\n",
    "            adjusted_action_probs /= np.sum(adjusted_action_probs)\n",
    "            action = np.random.choice(self.game.action_size, p=adjusted_action_probs)\n",
    "\n",
    "            # get the next state given the chosen action\n",
    "            state = self.game.get_next_state(state, action, player)\n",
    "\n",
    "            # check for game completion\n",
    "            value, is_terminal = self.game.get_value_and_terminated(state, action)\n",
    "\n",
    "            if is_terminal:\n",
    "                # get all states and policies from the game and append the outcome\n",
    "                return [(\n",
    "                    self.game.get_encoded_state(h_state),\n",
    "                    h_action_probs,\n",
    "                    value if h_player == player else self.game.get_opponent_value(value)\n",
    "                ) for h_state, h_action_probs, h_player in memory]\n",
    "\n",
    "            # swap the player and loop\n",
    "            player = self.game.get_opponent(player)\n",
    "\n",
    "    def train(self, memory):\n",
    "        ''' Train the model '''\n",
    "\n",
    "        # randomize training data\n",
    "        random.shuffle(memory)\n",
    "        \n",
    "        for batch_i in range(0, len(memory), self.args['batch_size']):\n",
    "            # sample a batch from training data\n",
    "            sample = memory[batch_i : min(len(memory) - 1, batch_i + self.args['batch_size'])]\n",
    "\n",
    "            # transpose list of tuples to independent lists\n",
    "            state, policy_targets, value_targets = zip(*sample)\n",
    "\n",
    "            # convert to numpy arrays\n",
    "            state = np.array(state)\n",
    "            policy_targets = np.array(policy_targets)\n",
    "            value_targets = np.array(value_targets).reshape(-1, 1) # wrap each value in its own array for simplicity later\n",
    "\n",
    "            # convert to tensors\n",
    "            state = torch.tensor(state, dtype=torch.float32, device=self.model.device)\n",
    "            policy_targets = torch.tensor(policy_targets, dtype=torch.float32, device=self.model.device)\n",
    "            value_targets = torch.tensor(value_targets, dtype=torch.float32, device=self.model.device)\n",
    "\n",
    "            # get model outputs\n",
    "            out_policy, out_value = self.model(state)\n",
    "\n",
    "            # get loss\n",
    "            policy_loss = F.cross_entropy(out_policy, policy_targets)\n",
    "            value_loss = F.mse_loss(out_value, value_targets)\n",
    "            loss = policy_loss + value_loss\n",
    "\n",
    "            # minimize loss via backpropagation\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    def learn(self):\n",
    "        ''' Generate self-play training data and train the model on it '''\n",
    "        \n",
    "        for iter in range(self.args['num_iters']):\n",
    "            memory = []\n",
    "            \n",
    "            self.model.eval()\n",
    "            for self_play_iter in trange(self.args['num_self_play_iters']):\n",
    "                memory += self.self_play()\n",
    "\n",
    "            self.model.train()\n",
    "            for epoch in trange(self.args['num_epochs']):\n",
    "                self.train(memory)\n",
    "\n",
    "            torch.save(self.model.state_dict(), f'models/{self.game}/model_{iter}.pt')\n",
    "            torch.save(self.optimizer.state_dict(), f'models/{self.game}/optimizer_{iter}.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5f989da5-eba5-4beb-8edd-5eeff772d7ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6194884865a4fe6abdc48be96ffe32b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ec4becc496b4e29811b6b8f436b8f99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "256f15b934e64e4789f61fde42ef35af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a247a049a1ab425381d3f3d3aa847bf9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3af3d379065945f0ab234c15ca070340",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef998047b5364e81a313bb10a2736863",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ttt = TicTacToe()\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = ResNet(ttt, 4, 64, device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=0.0001)\n",
    "\n",
    "args = {\n",
    "    'C': 2,\n",
    "    'num_searches': 60,\n",
    "    'num_iters': 3,\n",
    "    'num_self_play_iters': 500,\n",
    "    'num_epochs': 4,\n",
    "    'batch_size': 64,\n",
    "    'temperature': 1.25,\n",
    "    'dirichlet_epsilon': 0.25,\n",
    "    'dirichlet_alpha': 0.3\n",
    "}\n",
    "\n",
    "alphazero = AlphaZero(model, optimizer, ttt, args)\n",
    "alphazero.learn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "703608a9-b8be-4b03-b7da-2ba45921465c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]]\n",
      "Valid Moves: [0, 1, 2, 3, 4, 5, 6, 7, 8]\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "1:  0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]]\n",
      "[[ 1.  0.  0.]\n",
      " [ 0. -1.  0.]\n",
      " [ 0.  0.  0.]]\n",
      "Valid Moves: [1, 2, 3, 5, 6, 7, 8]\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "1:  1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.  1.  0.]\n",
      " [ 0. -1.  0.]\n",
      " [ 0.  0.  0.]]\n",
      "[[ 1.  1. -1.]\n",
      " [ 0. -1.  0.]\n",
      " [ 0.  0.  0.]]\n",
      "Valid Moves: [3, 5, 6, 7, 8]\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "1:  8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.  1. -1.]\n",
      " [ 0. -1.  0.]\n",
      " [ 0.  0.  1.]]\n",
      "[[ 1.  1. -1.]\n",
      " [ 0. -1.  0.]\n",
      " [-1.  0.  1.]]\n",
      "-1 won\n"
     ]
    }
   ],
   "source": [
    "ttt = TicTacToe()\n",
    "player = 1\n",
    "\n",
    "args = {\n",
    "    'C': 2,\n",
    "    'num_searches': 1000\n",
    "}\n",
    "\n",
    "model = ResNet(ttt, 4, 64)\n",
    "model.eval()\n",
    "\n",
    "mcts = MCTS(ttt, args, model)\n",
    "\n",
    "state = ttt.get_initial_state()\n",
    "\n",
    "while True:\n",
    "    print(state)\n",
    "\n",
    "    if player == 1:\n",
    "        valid_moves = ttt.get_valid_moves(state)\n",
    "        print('Valid Moves:' , [i for i in range(ttt.action_size) if valid_moves[i] == 1])\n",
    "        action = int(input(f'{player}: '))\n",
    "    \n",
    "        if valid_moves[action] == 0:\n",
    "            print('Invalid move')\n",
    "            continue\n",
    "    else:\n",
    "        neutral_state = ttt.change_perspective(state, player)\n",
    "        mcts_probs = mcts.search(neutral_state)\n",
    "        action = np.argmax(mcts_probs)\n",
    "\n",
    "    state = ttt.get_next_state(state, action, player)\n",
    "\n",
    "    value, is_terminal = ttt.get_value_and_terminated(state, action)\n",
    "\n",
    "    if is_terminal:\n",
    "        print(state)\n",
    "        if value == 1:\n",
    "            print(player, 'won')\n",
    "        else:\n",
    "            print('Draw')\n",
    "        break\n",
    "\n",
    "    player = ttt.get_opponent(player)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e4eb825-e431-4ea0-998f-d1da412b5daf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8251b500-c17c-49af-899a-2d3c5f6420fe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
